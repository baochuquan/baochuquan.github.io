<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/chu.icon">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/chu.icon">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chuquan.me","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.18.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="前面，我们介绍了适用于图像和视觉领域的 卷积神经网络。本文，我们来介绍适用于时序数据处理场景的 循环神经网络（Recurrent Neural Network，RNN）。">
<meta property="og:type" content="article">
<meta property="og:title" content="循环神经网络">
<meta property="og:url" content="http://chuquan.me/2024/10/26/rnn-introduction/index.html">
<meta property="og:site_name" content="楚权的世界">
<meta property="og:description" content="前面，我们介绍了适用于图像和视觉领域的 卷积神经网络。本文，我们来介绍适用于时序数据处理场景的 循环神经网络（Recurrent Neural Network，RNN）。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-10-26T11:02:22.000Z">
<meta property="article:modified_time" content="2024-10-26T11:08:23.146Z">
<meta property="article:author" content="Bao Chuquan">
<meta property="article:tag" content="RNN">
<meta property="article:tag" content="LSTM">
<meta property="article:tag" content="Gated RNN">
<meta property="article:tag" content="GRU">
<meta property="article:tag" content="Time RNN">
<meta property="article:tag" content="BPTT">
<meta property="article:tag" content="Truncated BPTT">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://chuquan.me/2024/10/26/rnn-introduction/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://chuquan.me/2024/10/26/rnn-introduction/","path":"2024/10/26/rnn-introduction/","title":"循环神经网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>循环神经网络 | 楚权的世界</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?030cf08d5b27d756fa9aeecb1130fe13"></script>






<link rel="dns-prefetch" href="https://vercel.chuquan.me">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="楚权的世界" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="headband"></div>
  <a href="https://github.com/baochuquan"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_orange_ff7600.png?resize=149%2C149" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">楚权的世界</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Seek the wonder of life.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-number">1.</span> <span class="nav-text">前馈神经网络的局限性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">循环神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rnn-%E4%B8%8E-time-rnn"><span class="nav-number">3.</span> <span class="nav-text">RNN 与 Time RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">3.1.</span> <span class="nav-text">正向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">3.2.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bptt"><span class="nav-number">3.2.1.</span> <span class="nav-text">BPTT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#truncated-bptt"><span class="nav-number">3.2.2.</span> <span class="nav-text">Truncated BPTT</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rnn-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.</span> <span class="nav-text">RNN 语言模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">5.</span> <span class="nav-text">梯度爆炸和梯度消失</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E4%BC%98%E5%8C%96"><span class="nav-number">6.</span> <span class="nav-text">梯度爆炸优化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%BC%98%E5%8C%96"><span class="nav-number">7.</span> <span class="nav-text">梯度消失优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#lstm"><span class="nav-number">7.1.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="nav-number">7.1.1.</span> <span class="nav-text">基本结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E9%97%A8"><span class="nav-number">7.1.2.</span> <span class="nav-text">输出门</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%97%E5%BF%98%E9%97%A8"><span class="nav-number">7.1.3.</span> <span class="nav-text">遗忘门</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%B0%E5%BF%86%E5%8D%95%E5%85%83%E4%BC%98%E5%8C%96"><span class="nav-number">7.1.4.</span> <span class="nav-text">记忆单元优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E9%97%A8"><span class="nav-number">7.1.5.</span> <span class="nav-text">输入门</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%BB%93%E6%9E%84"><span class="nav-number">7.1.6.</span> <span class="nav-text">完整结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-1"><span class="nav-number">7.1.7.</span> <span class="nav-text">反向传播</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gru"><span class="nav-number">7.2.</span> <span class="nav-text">GRU</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">8.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">9.</span> <span class="nav-text">参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Bao Chuquan"
      src="/images/SlamDunk.png">
  <p class="site-author-name" itemprop="name">Bao Chuquan</p>
  <div class="site-description" itemprop="description">积累，沉淀，吸收，转换</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">153</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">382</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/baochuquan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;baochuquan" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:baochuquan@gmail.com" title="E-Mail → mailto:baochuquan@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/baochuquan" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;baochuquan" rel="noopener me" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Baochuquan" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Baochuquan" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://chuquan.me/2024/10/26/rnn-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/SlamDunk.png">
      <meta itemprop="name" content="Bao Chuquan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="楚权的世界">
      <meta itemprop="description" content="积累，沉淀，吸收，转换">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="循环神经网络 | 楚权的世界">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          循环神经网络
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-10-26 19:02:22 / 修改时间：19:08:23" itemprop="dateCreated datePublished" datetime="2024-10-26T19:02:22+08:00">2024-10-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2024/10/26/rnn-introduction/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2024/10/26/rnn-introduction/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>前面，我们介绍了适用于图像和视觉领域的 <a
href="https://chuquan.me/2024/09/28/cnn-introduction/">卷积神经网络</a>。本文，我们来介绍适用于时序数据处理场景的
<strong>循环神经网络</strong>（Recurrent Neural Network，RNN）。</p>
<span id="more"></span>
<h1 id="前馈神经网络的局限性">前馈神经网络的局限性</h1>
<p>我们首先来介绍一下前馈神经网络的局限性。什么是前馈神经网络？其特征是数据在神经网络中单向流动，从输入层经过隐藏层达到输出层，中间不存在反馈连接。前面我们介绍的入门级神经网络、卷积神经网络都属于前馈神经网络。</p>
<p>前馈神经网络的一个缺点是无法处理时序数据，通常只适用于一次输入，一次输出的场景。根本原因在于其不存在记忆能力，类似函数式编程中的纯函数，其内部不存在状态，当输入相同时，则输出也相同。</p>
<h1 id="循环神经网络">循环神经网络</h1>
<p>循环神经网络解决了前馈神经网络无法解决的问题，经过训练后可以处理时序数据。时序数据是指具有顺序或时间关联性的数据，比如：视频流、音频流、构成文章或句子的单词序列等。因此，RNN
非常适合语言翻译、语音识别、图像字幕、自然语言处理等场景。</p>
<p>类似于 CNN 包含卷积层、池化层，RNN 的特征是包含一个循环层（Recurrent
Layer），不过大多数情况下称为 “RNN 层”。下面，我们来重点介绍一下 RNN
层的工作原理。</p>
<h1 id="rnn-与-time-rnn">RNN 与 Time RNN</h1>
<p>下图所示，是 RNN
层的两种数据流向示意图。经典的表示法采用水平流向表示，为了方便分析，后续我们将使用垂直流向表示。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-01.png?x-oss-process=image/resize,w_800" /></p>
<p>RNN 层内部具有环路，数据在层内循环。下标 <span
class="math inline">\(t\)</span> 表示时间，当时序数据 <span
class="math inline">\((x_0, x_1, ..., x_t, ...)\)</span> 输入到 RNN
层后，将输出 <span class="math inline">\((h_0, h_1, ..., h_t,
...)\)</span> 等 <strong>隐藏状态</strong>（Hidden State）。</p>
<p>为了进一步分析环路的数据走向，我们对 RNN
层进行循环展开，如下所示。可以看出，各个时刻的 RNN
层接收两个输入，分别是：</p>
<ul>
<li><strong>当前时刻的时序数据输入</strong></li>
<li><strong>前一时刻的隐藏状态输出</strong></li>
</ul>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-02.png?x-oss-process=image/resize,w_800" /></p>
<p>很显然，循环展开后包含了各个时刻的 RNN 层。为了区分，我们重新定义 RNN
的不同结构。</p>
<ul>
<li><strong>RNN 层</strong>：表示单一时刻的 RNN 层</li>
<li><strong>Time RNN 层</strong>：表示由多个单一时刻 RNN 层所构成的 RNN
层</li>
</ul>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-08.png?x-oss-process=image/resize,w_800" /></p>
<h2 id="正向传播">正向传播</h2>
<p>我们先来看下 RNN
层的正向传播计算图，如下所示。其内部包含三种运算，分别是：</p>
<ul>
<li>矩阵乘法：MatMul 节点</li>
<li>矩阵加法：Sum 节点</li>
<li>激活函数：tanh 节点，即双曲正切函数</li>
</ul>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-03.png?x-oss-process=image/resize,w_800" /></p>
<p>可以看出，其内部包含两个权重，分别是将输入 <span
class="math inline">\(x\)</span> 转化为输出 <span
class="math inline">\(h\)</span> 的权重 <span
class="math inline">\(W_x\)</span>
和将前一时刻的输出转换为当前时刻的输出的权重 <span
class="math inline">\(W_h\)</span>。此外，还有偏置 <span
class="math inline">\(b\)</span>。这里的 <span
class="math inline">\(h_{t-1}\)</span> 和 <span
class="math inline">\(x_t\)</span>
都是行向量。根据计算图，我们最终可以得出如下的计算公式。</p>
<span class="math display">\[\begin{aligned}
h_t = tanh(h_{t-1}W_h + x_t W_x + b)
\end{aligned}\]</span>
<p>由此，我们可以认为 RNN 具有 “状态” <span
class="math inline">\(h\)</span>，并基于此进行运算，这也是 RNN 层是
“具有状态的层” 或 “具有记忆的层” 的原因。</p>
<p>相比 RNN 层，Time RNN 层的正向传播在输入和输出侧有所不同。Time RNN 将
<span class="math inline">\((x_0, x_1, ..., x_{T-1})\)</span> 捆绑为
<span class="math inline">\(xs\)</span> 作为输入，将 <span
class="math inline">\((h_0, h_1, ..., h_{T-1})\)</span> 捆绑为 <span
class="math inline">\(hs\)</span> 作为输出，内部则是由多个 RNN
层连接而成。</p>
<h2 id="反向传播">反向传播</h2>
<p>下图所示为 RNN 层的反向传播计算图。结合 <a
href="https://chuquan.me/2024/09/21/deep-learning-layer/">《神经网络的分层设计原理》</a>
中基于计算图计算反向传播梯度的方法，我们可以很容易求解 RNN
层的反向传播梯度。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-07.png?x-oss-process=image/resize,w_800" /></p>
<p>下图所示为 Time RNN 层的反向传播计算图。上游的梯度记为 <span
class="math inline">\(dhs\)</span>，下游的梯度记为 <span
class="math inline">\(dxs\)</span>。这是 Time RNN
层作为整体在外部的反向传播路径。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-09.png?x-oss-process=image/resize,w_800" /></p>
<p>关于 Time RNN 层内部各个时刻的 RNN
层的反向传播路径，具体方法有两种，分别是：</p>
<ul>
<li><strong>基于时间的反向传播</strong>（Backpropagation Through
Time，BPTT），简称 BPTT</li>
<li>基于 BPTT 进行分段截断，简称 Truncated BPTT</li>
</ul>
<h3 id="bptt">BPTT</h3>
<p>BPTT 的核心思想很简单，对 Time RNN
层进行循环展开，由此求各个时刻的目标梯度，即按时间顺序展开的神经网络的误差传播法。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-04.png?x-oss-process=image/resize,w_800" /></p>
<p>随着时序数据的时间跨度增大，BPTT 反向传播会出现以下几个问题：</p>
<ul>
<li>内存成比例增大，因为各个时刻的 RNN 层会存储中间数据。</li>
<li>时间成比例变长，因为误差（梯度）会按顺序反向传播。</li>
<li>梯度逐渐不稳定，比如：梯度爆炸、梯度消失。</li>
</ul>
<h3 id="truncated-bptt">Truncated BPTT</h3>
<p>为了解决反向传播的长时序数据问题，一种自然而然的方法是 <strong>将
Time RNN 层循环展开后的反向传播路径进行分段截断</strong>，这就是
Truncated BPTT
的核心思想。这里要注意的是，我们只截断反向传播，而不截断正向传播，如下所示。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-05.png?x-oss-process=image/resize,w_800" /></p>
<p>下图所示是 Truncated BPTT
的训练示意图。我们来看一下它是如何对时序数据过大产生的问题进行优化的。</p>
<ul>
<li>对于时间问题，由于反向传播依赖正向传播的中间数据，即使是 Truncated
BPTT，也需要按序的正向传播。只不过对于反向传播，每个分段可以并行处理。从而在一定程度上提高训练速度。</li>
<li>对于梯度问题，截断后也能够在一定程度上缓解梯度不稳定的情况，前提是要选择合适的截断长度。</li>
<li>对于内存问题，此时我们无需积累完整路径的所有中间数据后才进行反向传播，因此也有了一定程度的内存优化。</li>
</ul>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-06.png?x-oss-process=image/resize,w_800" /></p>
<h1 id="rnn-语言模型">RNN 语言模型</h1>
<p>RNN 语言模型（Language Model），简称 RNNLM，是一个基于 RNN
层实现的神经网络，用于理解和生成自然语言文本。</p>
<p>我们考虑一个问题：当输入的内容为 “Tom was watching TV in his room.
Mary came into the room. Mary said hi to _”
时，如何预测最后一个单词是什么？很显然，最后一个单词应该是
“Tom”。但是，由于 “Tom”
单词相距预测点太远，常规的神经网络无法有效地进行预测。对此，基于 RNN
的语言模型 RNNLM 可以有效解决这个问题。</p>
<p>下图所示是一个简单的 RNNLM 的神经网络模型。每一个 RNN
层的上下游都有对应的神经网络层，比如：Embedding、Affine、Softmax
等，用于处理单一时刻的时序数据。类似于 RNN 和 Time RNN
的关系，这里我们将整体处理含有 <span class="math inline">\(T\)</span>
个时序数据的层称为 “Time XX 层”。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-10.png?x-oss-process=image/resize,w_800" /></p>
<p>对于学习阶段，RNNLM 中也有对应的 Time Softmax with Loss
层，如下所示。<span class="math inline">\(x_0\)</span>、<span
class="math inline">\(x_1\)</span> 等数据是推理输出的得分，<span
class="math inline">\(t_0\)</span>、<span
class="math inline">\(t_1\)</span> 等数据是正确解标签。<span
class="math inline">\(T\)</span> 个 Softmax with Loss
层各自计算损失，求和取平均值，作为最终的损失。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-11.png?x-oss-process=image/resize,w_800" /></p>
<h1 id="梯度爆炸和梯度消失">梯度爆炸和梯度消失</h1>
<p>前面我们提到 RNN
在学习时序数据的长期依赖时，会存在梯度爆炸和梯度消失的问题。这里我们来介绍一下深层次的原因。</p>
<p>如下图所示，我们仅关注 Time RNN 层中的梯度传播。这里考虑长度为 <span
class="math inline">\(T\)</span>
的时序数据，在反向传播过程中，梯度会反复经历多次 tanh、sum、MatMul
的运算：</p>
<ul>
<li>sum：加法的导数是常数 1</li>
<li>tanh：当 <span class="math inline">\(y = tanh(x)\)</span>
时，其导数为 <span class="math inline">\(\frac{dy}{dx} = 1 -
y^2\)</span></li>
<li>MatMul：矩阵乘法的导数计算可以参考 <a
href="https://chuquan.me/2024/09/21/deep-learning-layer/#matmul-%E8%8A%82%E7%82%B9">《神经网络的分层设计原理》</a></li>
</ul>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-12.png?x-oss-process=image/resize,w_800" /></p>
<p>首先，我们来看 tanh 对于反向传播梯度的影响。下图所示为 tanh
及其导数的曲线图，导数值的范围为 [0, 1]。当多次经过 tanh
节点时，梯度值会不断进行乘积，会逐渐减小，趋近于
0。这正是梯度消失的原因之一。这里主要是因为激活函数 tanh
导致的，如果将其改为 ReLU，可以有效抑制梯度消失的问题。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-13.png?x-oss-process=image/resize,w_800" /></p>
<p>然后，我们考虑 MatMul 对于反向传播梯度的影响。简单起见，我们忽略 tanh
节点的影响。下图所示为 MatMul 的反向传播计算图，其中 <span
class="math inline">\(W_h\)</span> 保持不变，当时序数据越长，<span
class="math inline">\(W_h^T\)</span> 的乘积次数越多。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-14.png?x-oss-process=image/resize,w_800" /></p>
<p>由此可以看出梯度爆炸或者梯度消失，取决于矩阵，更专业的术语是矩阵奇异值。矩阵奇异值本质上表示数据的离散程度，根据奇异值（更准确地说是多个奇异值中的最大值）是否大于
1。如果奇异值的最大值大于
1，可以预测梯度很有可能会呈指数级增加，即梯度爆炸；如果奇异值的最大值小于
1，可以预测梯度会呈指数级减少，即梯度消失。当然，并不是说奇异值比 1
大就一定会出现梯度爆炸。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-15.png?x-oss-process=image/resize,w_800" /></p>
<h1 id="梯度爆炸优化">梯度爆炸优化</h1>
<p><strong>梯度裁剪</strong>（Gradient
Clipping）可以用于解决梯度爆炸，其基本思想可以用如下所示的伪代码表示。</p>
<span class="math display">\[\begin{aligned}
if \space \space \space || \hat g || \geq &amp; \space threshold
\\
\hat g = &amp; \space \frac{threshold}{|| \hat g ||} \hat g
\end{aligned}\]</span>
<p>这里将神经网络所有参数的梯度整合成一个，用符合 <span
class="math inline">\(\hat g\)</span> 表示。<span
class="math inline">\(threshold\)</span> 表示阈值。当 <span
class="math inline">\(\hat g\)</span> 大于或等于阈值，则对 <span
class="math inline">\(\hat g\)</span> 进行裁剪。</p>
<h1 id="梯度消失优化">梯度消失优化</h1>
<p><strong>Gated RNN</strong> 可以用于解决梯度消失，其从根本上改变了 RNN
结构。目前业界已经出现了诸多 Gated RNN 网络结构，其中具有代表性的有 LSTM
和 GRU。下面，我们分别来进行介绍。</p>
<h2 id="lstm">LSTM</h2>
<p>LSTM（Long Short-Term Memory），长短期记忆网络。类似于 RNN 和 Time
RNN，这里也存在 LSTM 和 Time LSTM 的区别。这里，我们主要介绍单个时刻的
LSTM 结构。</p>
<p>如下所示，LSTM 与 RNN 的区别在于 LSTM 具有额外的记忆单元 <span
class="math inline">\(c\)</span>，其只存在于 Time LSTM
内部，不会其它层进行传输。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-16.png?x-oss-process=image/resize,w_800" /></p>
<p>LSTM
内部包含多种结构，包括：输出门、遗忘门、记忆单元、输入门等，下面我们先从
LSTM 的基本结构进行介绍。</p>
<h3 id="基本结构">基本结构</h3>
<p>LSTM 中的记忆单元 <span class="math inline">\(c_t\)</span>
存储了过去到时刻 <span class="math inline">\(t\)</span>
的所有记忆，并基于此向外部层和下一时刻的 LSTM 输出隐藏状态 <span
class="math inline">\(h_t\)</span>。</p>
<p>下图所示，LSTM 中当前记忆单元 <span
class="math inline">\(c_t\)</span> 基于 3 个输入 <span
class="math inline">\(c_{t-1}\)</span>、<span
class="math inline">\(h_{t-1}\)</span>、<span
class="math inline">\(x_t\)</span>
经过“某种计算”得出。最后输出的隐藏状态 <span
class="math inline">\(h_t\)</span> 是在 <span
class="math inline">\(c_t\)</span> 的基础上应用 tanh
函数计算得到的。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-17.png?x-oss-process=image/resize,w_800" /></p>
<p>这里提到的“某种计算”是下面要介绍的各种运算，它们都会涉及到一个概念——<strong>门</strong>（Gate）。门的本质就是阀门，可以控制流过数据的大小。在神经网络中，激活函数的概念与它非常相似。因此，LSTM
使用经典的激活函数 <code>sigmoid</code> 作为各种门结构，后续将使用 <span
class="math inline">\(\sigma\)</span> 符号进行表示。</p>
<h3 id="输出门">输出门</h3>
<p>首先，LSTM 增加了 <strong>输出门</strong>（Output Gate）。输入 <span
class="math inline">\(x_t\)</span> 有权重 <span
class="math inline">\(W_x\)</span>，输入隐藏状态 <span
class="math inline">\(h_{t-1}\)</span> 有权重 <span
class="math inline">\(W_h\)</span>。将它们的矩阵乘积和偏置 <span
class="math inline">\(b\)</span> 之和传入 <span
class="math inline">\(sigmoid\)</span> 函数，结果就是输门的输出值 <span
class="math inline">\(o\)</span>。</p>
<span class="math display">\[\begin{aligned}
o = \sigma (x_t W_x + h_{t-1} W_h + b)
\end{aligned}\]</span>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-18.png?x-oss-process=image/resize,w_800" /></p>
<p>输出门的输出值 <span class="math inline">\(o\)</span> 进一步与 <span
class="math inline">\(tanh(c_t)\)</span>
进行乘积。注意，这里的乘积并不是矩阵乘积，而是
<strong>阿达玛乘积</strong>，即矩阵对应元素之间的乘积。如果使用 <span
class="math inline">\(\circ\)</span> 表示阿达玛乘积，其计算式如下。</p>
<span class="math display">\[\begin{aligned}
h_t = o \circ tanh(c_t)
\end{aligned}\]</span>
<h3 id="遗忘门">遗忘门</h3>
<p>为了模拟更真实的记忆效果，LSTM 加入了 <strong>遗忘门</strong>（Forget
Gate），其计算图如下所示。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-19.png?x-oss-process=image/resize,w_800" /></p>
<p>遗忘门的计算与输出门类似，区别在于两者在计算图中的位置不同，其计算式如下。</p>
<span class="math display">\[\begin{aligned}
f = \sigma (x_t W_x + h_{t-1} W_h + b)
\end{aligned}\]</span>
<h3 id="记忆单元优化">记忆单元优化</h3>
<p>如果针对记忆单元，我们只增加遗忘门，那么会导致记忆逐步遗忘。因此，我们还要增加强化记忆的结构，由此对记忆单元进行优化。这里，我们增加
tanh 节点，如下所示。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-20.png?x-oss-process=image/resize,w_800" /></p>
<p>tanh 节点计算与其他门的计算类似，如下所示。</p>
<span class="math display">\[\begin{aligned}
g = tanh(x_t W_x + h_{t-1} W_h + b)
\end{aligned}\]</span>
<p>基于 tanh 节点计算得到的结果 <span class="math inline">\(g\)</span>
最终与加到上上一时刻的记忆单元 <span
class="math inline">\(c_{t-1}\)</span> 中，从而形成新的记忆。</p>
<h3 id="输入门">输入门</h3>
<p>输入门判断新增信息 <span class="math inline">\(g\)</span>
的各个元素的价值，会对待添加的信息进行取舍，其计算图如下所示。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-21.png?x-oss-process=image/resize,w_800" /></p>
<p>输入门的输出值计算如下所示。</p>
<span class="math display">\[\begin{aligned}
i = \sigma (x_t W_x + h_{t-1} W_h + b)
\end{aligned}\]</span>
<h3 id="完整结构">完整结构</h3>
<p>最后，我们可以得到如下所示的 LSTM 完整结构，可以看出 LSTM 是在 RNN
的基础上增加了一系列门结构以及相关运算实现的。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-22.png?x-oss-process=image/resize,w_800" /></p>
<h3 id="反向传播-1">反向传播</h3>
<p>接下来，我们来看一下为什么 LSTM
能够解决梯度消失的问题。其原因可以通过观察记忆单元 <span
class="math inline">\(c\)</span> 的反向传播来了解，如下所示。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-23.png?x-oss-process=image/resize,w_800" /></p>
<p>记忆单元的反向传播会反复经过 “+” 和 “×” 节点。“+” 节点的导数是
1，会直接传递上游梯度至下游。“×”
节点并不是矩阵乘积，而是阿达玛乘积。阿达玛乘积是矩阵中对应元素的乘积运算，而且每次都会基于不同的门值进行乘积运算，因此不会发生梯度消失或梯度爆炸。</p>
<p>在反向传播过程中，遗忘门认为应该忘记的记忆单元元素，其梯度会变小；不能忘记的记忆单元元素，其梯度并不会退化。因此，可以期待记忆单元能够保存学习长期的依赖关系。</p>
<h2 id="gru">GRU</h2>
<p>GRU（Gated Recurrent Unit），门控循环单元，其继承了 LSTM
的思路，但是减少了参数，缩短了计算时间。</p>
<p>GRU 的计算图和计算式如下所示。</p>
<p><img
src="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-24.png?x-oss-process=image/resize,w_800" /></p>
<span class="math display">\[\begin{aligned}
z = &amp; \space \sigma (x_t W_x + h_{t-1} W_h + b)
\\
r = &amp; \space \sigma (x_t W_x + h_{t-1} W_h + b)
\\
\widetilde{h} = &amp; \space tanh (x_t W_x + h_{t-1} W_h + b)
\\
h_t = &amp; \space (1 - z) \circ h_{t-1} + z \circ \widetilde{h}
\end{aligned}\]</span>
<p>GRU 没有记忆单元，只有一个隐藏状态 <span
class="math inline">\(h\)</span>
在时间方向上传播。其使用了两个门结构：<span
class="math inline">\(r\)</span>（reset 门）、<span
class="math inline">\(z\)</span>（update 门）。</p>
<p><span class="math inline">\(r\)</span>
决定在多大程度上“忽略”过去的隐藏状态。如果 <span
class="math inline">\(r\)</span> 为 0，则新的隐藏状态 <span
class="math inline">\(\widetiled{h}\)</span> 只取决于 <span
class="math inline">\(x_t\)</span>，即过去的隐藏状态被忽略。</p>
<p><span class="math inline">\(z\)</span>是更新隐藏状态的门，其包含了
LSTM 中遗忘门和输入门的作用。<span
class="math inline">\(\widetilde{h}\)</span> 中的 <span
class="math inline">\((1-z)\circ h_{t-1}\)</span>
表示遗忘门的功能；<span class="math inline">\(z \circ
\widetilde{h}\)</span> 表示输入门的功能。</p>
<p>整体而言，GRU 是简化版的 LSTM 架构，相比之下，GRU
的参数和计算成本更少。关于 GRU 和 LSTM
的选择，根据不同的任务和超参数的设置，结论可能不同。由于 GRU
超参数少、计算量小，因此比较适合于数据集较小，设计模型需要反复实验的场景。</p>
<h1 id="总结">总结</h1>
<p>本文我们主要介绍了循环神经网络的结构，其内部根据时序数据可以展开成多个循环结构。为了区分，我们将整体称为
Time RNN，将内部的单个循环结构称为 RNN。</p>
<p>Time RNN 的反向传播主要有两种方法，分别是：BPTT 和 Truncated
BPTT。后者是在前者的基础上对反向传播路径进行分段截断。从而在一定程度上缓解时序数据过长带来的时间问题、梯度问题、内存问题。</p>
<p>之后，我们进一步介绍了梯度问题中，梯度爆炸和梯度消失出现的原因。对于梯度爆炸，我们介绍了梯度裁剪的方法。对于梯度消失，我们介绍了
Gate RNN 的两种结构：LSTM 和 GRU。</p>
<h1 id="参考">参考</h1>
<ol type="1">
<li>《深度学习进阶：自然语言处理》</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>请我一杯咖啡吧！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpeg" alt="Bao Chuquan 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.png" alt="Bao Chuquan 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/RNN/" rel="tag"># RNN</a>
              <a href="/tags/LSTM/" rel="tag"># LSTM</a>
              <a href="/tags/Gated-RNN/" rel="tag"># Gated RNN</a>
              <a href="/tags/GRU/" rel="tag"># GRU</a>
              <a href="/tags/Time-RNN/" rel="tag"># Time RNN</a>
              <a href="/tags/BPTT/" rel="tag"># BPTT</a>
              <a href="/tags/Truncated-BPTT/" rel="tag"># Truncated BPTT</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/10/15/nlp-word-representation/" rel="prev" title="自然语言处理中的单词含义表示">
                  <i class="fa fa-angle-left"></i> 自然语言处理中的单词含义表示
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/11/02/attention-introduction/" rel="next" title="语言模型中的注意力机制">
                  语言模型中的注意力机制 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2017 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">true</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>
  <div>
     <a href="https://beian.miit.gov.cn/" target="_blank">浙ICP备16031766号-1</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"https://vercel.chuquan.me","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"placeholder":"(发表评论)","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"dark":"body.darkmode--activated","visitor":true,"comment_count":true,"requiredFields":["nick"],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","el":"#waline","comment":true,"path":"/2024/10/26/rnn-introduction/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>
<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: false,
  label: '🌓',
  autoMatchOsTheme: false
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
if (window.darkmode && !window.darkmode.isActivated()) {
  window.darkmode.toggle();
  var toggleButtons = document.getElementsByClassName("darkmode-toggle");
  if (toggleButtons && toggleButtons.length > 0) {
    for (i = 0; i < toggleButtons.length; i++) {
      toggleButtons[i].classList.add("darkmode-toggle--white");
    }
  }
}
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

<script src="/live2dwlib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dwassets/tororo.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
