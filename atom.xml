<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>楚权的世界</title>
  
  <subtitle>Seek the wonder of life.</subtitle>
  <link href="http://chuquan.me/atom.xml" rel="self"/>
  
  <link href="http://chuquan.me/"/>
  <updated>2024-10-26T11:08:23.146Z</updated>
  <id>http://chuquan.me/</id>
  
  <author>
    <name>Bao Chuquan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>循环神经网络</title>
    <link href="http://chuquan.me/2024/10/26/rnn-introduction/"/>
    <id>http://chuquan.me/2024/10/26/rnn-introduction/</id>
    <published>2024-10-26T11:02:22.000Z</published>
    <updated>2024-10-26T11:08:23.146Z</updated>
    
    <content type="html"><![CDATA[<p>前面，我们介绍了适用于图像和视觉领域的 <ahref="https://chuquan.me/2024/09/28/cnn-introduction/">卷积神经网络</a>。本文，我们来介绍适用于时序数据处理场景的<strong>循环神经网络</strong>（Recurrent Neural Network，RNN）。</p><span id="more"></span><h1 id="前馈神经网络的局限性">前馈神经网络的局限性</h1><p>我们首先来介绍一下前馈神经网络的局限性。什么是前馈神经网络？其特征是数据在神经网络中单向流动，从输入层经过隐藏层达到输出层，中间不存在反馈连接。前面我们介绍的入门级神经网络、卷积神经网络都属于前馈神经网络。</p><p>前馈神经网络的一个缺点是无法处理时序数据，通常只适用于一次输入，一次输出的场景。根本原因在于其不存在记忆能力，类似函数式编程中的纯函数，其内部不存在状态，当输入相同时，则输出也相同。</p><h1 id="循环神经网络">循环神经网络</h1><p>循环神经网络解决了前馈神经网络无法解决的问题，经过训练后可以处理时序数据。时序数据是指具有顺序或时间关联性的数据，比如：视频流、音频流、构成文章或句子的单词序列等。因此，RNN非常适合语言翻译、语音识别、图像字幕、自然语言处理等场景。</p><p>类似于 CNN 包含卷积层、池化层，RNN 的特征是包含一个循环层（RecurrentLayer），不过大多数情况下称为 “RNN 层”。下面，我们来重点介绍一下 RNN层的工作原理。</p><h1 id="rnn-与-time-rnn">RNN 与 Time RNN</h1><p>下图所示，是 RNN层的两种数据流向示意图。经典的表示法采用水平流向表示，为了方便分析，后续我们将使用垂直流向表示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-01.png?x-oss-process=image/resize,w_800" /></p><p>RNN 层内部具有环路，数据在层内循环。下标 <spanclass="math inline">\(t\)</span> 表示时间，当时序数据 <spanclass="math inline">\((x_0, x_1, ..., x_t, ...)\)</span> 输入到 RNN层后，将输出 <span class="math inline">\((h_0, h_1, ..., h_t,...)\)</span> 等 <strong>隐藏状态</strong>（Hidden State）。</p><p>为了进一步分析环路的数据走向，我们对 RNN层进行循环展开，如下所示。可以看出，各个时刻的 RNN层接收两个输入，分别是：</p><ul><li><strong>当前时刻的时序数据输入</strong></li><li><strong>前一时刻的隐藏状态输出</strong></li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-02.png?x-oss-process=image/resize,w_800" /></p><p>很显然，循环展开后包含了各个时刻的 RNN 层。为了区分，我们重新定义 RNN的不同结构。</p><ul><li><strong>RNN 层</strong>：表示单一时刻的 RNN 层</li><li><strong>Time RNN 层</strong>：表示由多个单一时刻 RNN 层所构成的 RNN层</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-08.png?x-oss-process=image/resize,w_800" /></p><h2 id="正向传播">正向传播</h2><p>我们先来看下 RNN层的正向传播计算图，如下所示。其内部包含三种运算，分别是：</p><ul><li>矩阵乘法：MatMul 节点</li><li>矩阵加法：Sum 节点</li><li>激活函数：tanh 节点，即双曲正切函数</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-03.png?x-oss-process=image/resize,w_800" /></p><p>可以看出，其内部包含两个权重，分别是将输入 <spanclass="math inline">\(x\)</span> 转化为输出 <spanclass="math inline">\(h\)</span> 的权重 <spanclass="math inline">\(W_x\)</span>和将前一时刻的输出转换为当前时刻的输出的权重 <spanclass="math inline">\(W_h\)</span>。此外，还有偏置 <spanclass="math inline">\(b\)</span>。这里的 <spanclass="math inline">\(h_{t-1}\)</span> 和 <spanclass="math inline">\(x_t\)</span>都是行向量。根据计算图，我们最终可以得出如下的计算公式。</p><span class="math display">\[\begin{aligned}h_t = tanh(h_{t-1}W_h + x_t W_x + b)\end{aligned}\]</span><p>由此，我们可以认为 RNN 具有 “状态” <spanclass="math inline">\(h\)</span>，并基于此进行运算，这也是 RNN 层是“具有状态的层” 或 “具有记忆的层” 的原因。</p><p>相比 RNN 层，Time RNN 层的正向传播在输入和输出侧有所不同。Time RNN 将<span class="math inline">\((x_0, x_1, ..., x_{T-1})\)</span> 捆绑为<span class="math inline">\(xs\)</span> 作为输入，将 <spanclass="math inline">\((h_0, h_1, ..., h_{T-1})\)</span> 捆绑为 <spanclass="math inline">\(hs\)</span> 作为输出，内部则是由多个 RNN层连接而成。</p><h2 id="反向传播">反向传播</h2><p>下图所示为 RNN 层的反向传播计算图。结合 <ahref="https://chuquan.me/2024/09/21/deep-learning-layer/">《神经网络的分层设计原理》</a>中基于计算图计算反向传播梯度的方法，我们可以很容易求解 RNN层的反向传播梯度。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-07.png?x-oss-process=image/resize,w_800" /></p><p>下图所示为 Time RNN 层的反向传播计算图。上游的梯度记为 <spanclass="math inline">\(dhs\)</span>，下游的梯度记为 <spanclass="math inline">\(dxs\)</span>。这是 Time RNN层作为整体在外部的反向传播路径。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-09.png?x-oss-process=image/resize,w_800" /></p><p>关于 Time RNN 层内部各个时刻的 RNN层的反向传播路径，具体方法有两种，分别是：</p><ul><li><strong>基于时间的反向传播</strong>（Backpropagation ThroughTime，BPTT），简称 BPTT</li><li>基于 BPTT 进行分段截断，简称 Truncated BPTT</li></ul><h3 id="bptt">BPTT</h3><p>BPTT 的核心思想很简单，对 Time RNN层进行循环展开，由此求各个时刻的目标梯度，即按时间顺序展开的神经网络的误差传播法。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-04.png?x-oss-process=image/resize,w_800" /></p><p>随着时序数据的时间跨度增大，BPTT 反向传播会出现以下几个问题：</p><ul><li>内存成比例增大，因为各个时刻的 RNN 层会存储中间数据。</li><li>时间成比例变长，因为误差（梯度）会按顺序反向传播。</li><li>梯度逐渐不稳定，比如：梯度爆炸、梯度消失。</li></ul><h3 id="truncated-bptt">Truncated BPTT</h3><p>为了解决反向传播的长时序数据问题，一种自然而然的方法是 <strong>将Time RNN 层循环展开后的反向传播路径进行分段截断</strong>，这就是Truncated BPTT的核心思想。这里要注意的是，我们只截断反向传播，而不截断正向传播，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-05.png?x-oss-process=image/resize,w_800" /></p><p>下图所示是 Truncated BPTT的训练示意图。我们来看一下它是如何对时序数据过大产生的问题进行优化的。</p><ul><li>对于时间问题，由于反向传播依赖正向传播的中间数据，即使是 TruncatedBPTT，也需要按序的正向传播。只不过对于反向传播，每个分段可以并行处理。从而在一定程度上提高训练速度。</li><li>对于梯度问题，截断后也能够在一定程度上缓解梯度不稳定的情况，前提是要选择合适的截断长度。</li><li>对于内存问题，此时我们无需积累完整路径的所有中间数据后才进行反向传播，因此也有了一定程度的内存优化。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-06.png?x-oss-process=image/resize,w_800" /></p><h1 id="rnn-语言模型">RNN 语言模型</h1><p>RNN 语言模型（Language Model），简称 RNNLM，是一个基于 RNN层实现的神经网络，用于理解和生成自然语言文本。</p><p>我们考虑一个问题：当输入的内容为 “Tom was watching TV in his room.Mary came into the room. Mary said hi to _”时，如何预测最后一个单词是什么？很显然，最后一个单词应该是“Tom”。但是，由于 “Tom”单词相距预测点太远，常规的神经网络无法有效地进行预测。对此，基于 RNN的语言模型 RNNLM 可以有效解决这个问题。</p><p>下图所示是一个简单的 RNNLM 的神经网络模型。每一个 RNN层的上下游都有对应的神经网络层，比如：Embedding、Affine、Softmax等，用于处理单一时刻的时序数据。类似于 RNN 和 Time RNN的关系，这里我们将整体处理含有 <span class="math inline">\(T\)</span>个时序数据的层称为 “Time XX 层”。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-10.png?x-oss-process=image/resize,w_800" /></p><p>对于学习阶段，RNNLM 中也有对应的 Time Softmax with Loss层，如下所示。<span class="math inline">\(x_0\)</span>、<spanclass="math inline">\(x_1\)</span> 等数据是推理输出的得分，<spanclass="math inline">\(t_0\)</span>、<spanclass="math inline">\(t_1\)</span> 等数据是正确解标签。<spanclass="math inline">\(T\)</span> 个 Softmax with Loss层各自计算损失，求和取平均值，作为最终的损失。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-11.png?x-oss-process=image/resize,w_800" /></p><h1 id="梯度爆炸和梯度消失">梯度爆炸和梯度消失</h1><p>前面我们提到 RNN在学习时序数据的长期依赖时，会存在梯度爆炸和梯度消失的问题。这里我们来介绍一下深层次的原因。</p><p>如下图所示，我们仅关注 Time RNN 层中的梯度传播。这里考虑长度为 <spanclass="math inline">\(T\)</span>的时序数据，在反向传播过程中，梯度会反复经历多次 tanh、sum、MatMul的运算：</p><ul><li>sum：加法的导数是常数 1</li><li>tanh：当 <span class="math inline">\(y = tanh(x)\)</span>时，其导数为 <span class="math inline">\(\frac{dy}{dx} = 1 -y^2\)</span></li><li>MatMul：矩阵乘法的导数计算可以参考 <ahref="https://chuquan.me/2024/09/21/deep-learning-layer/#matmul-%E8%8A%82%E7%82%B9">《神经网络的分层设计原理》</a></li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-12.png?x-oss-process=image/resize,w_800" /></p><p>首先，我们来看 tanh 对于反向传播梯度的影响。下图所示为 tanh及其导数的曲线图，导数值的范围为 [0, 1]。当多次经过 tanh节点时，梯度值会不断进行乘积，会逐渐减小，趋近于0。这正是梯度消失的原因之一。这里主要是因为激活函数 tanh导致的，如果将其改为 ReLU，可以有效抑制梯度消失的问题。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-13.png?x-oss-process=image/resize,w_800" /></p><p>然后，我们考虑 MatMul 对于反向传播梯度的影响。简单起见，我们忽略 tanh节点的影响。下图所示为 MatMul 的反向传播计算图，其中 <spanclass="math inline">\(W_h\)</span> 保持不变，当时序数据越长，<spanclass="math inline">\(W_h^T\)</span> 的乘积次数越多。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-14.png?x-oss-process=image/resize,w_800" /></p><p>由此可以看出梯度爆炸或者梯度消失，取决于矩阵，更专业的术语是矩阵奇异值。矩阵奇异值本质上表示数据的离散程度，根据奇异值（更准确地说是多个奇异值中的最大值）是否大于1。如果奇异值的最大值大于1，可以预测梯度很有可能会呈指数级增加，即梯度爆炸；如果奇异值的最大值小于1，可以预测梯度会呈指数级减少，即梯度消失。当然，并不是说奇异值比 1大就一定会出现梯度爆炸。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-15.png?x-oss-process=image/resize,w_800" /></p><h1 id="梯度爆炸优化">梯度爆炸优化</h1><p><strong>梯度裁剪</strong>（GradientClipping）可以用于解决梯度爆炸，其基本思想可以用如下所示的伪代码表示。</p><span class="math display">\[\begin{aligned}if \space \space \space || \hat g || \geq &amp; \space threshold\\\hat g = &amp; \space \frac{threshold}{|| \hat g ||} \hat g\end{aligned}\]</span><p>这里将神经网络所有参数的梯度整合成一个，用符合 <spanclass="math inline">\(\hat g\)</span> 表示。<spanclass="math inline">\(threshold\)</span> 表示阈值。当 <spanclass="math inline">\(\hat g\)</span> 大于或等于阈值，则对 <spanclass="math inline">\(\hat g\)</span> 进行裁剪。</p><h1 id="梯度消失优化">梯度消失优化</h1><p><strong>Gated RNN</strong> 可以用于解决梯度消失，其从根本上改变了 RNN结构。目前业界已经出现了诸多 Gated RNN 网络结构，其中具有代表性的有 LSTM和 GRU。下面，我们分别来进行介绍。</p><h2 id="lstm">LSTM</h2><p>LSTM（Long Short-Term Memory），长短期记忆网络。类似于 RNN 和 TimeRNN，这里也存在 LSTM 和 Time LSTM 的区别。这里，我们主要介绍单个时刻的LSTM 结构。</p><p>如下所示，LSTM 与 RNN 的区别在于 LSTM 具有额外的记忆单元 <spanclass="math inline">\(c\)</span>，其只存在于 Time LSTM内部，不会其它层进行传输。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-16.png?x-oss-process=image/resize,w_800" /></p><p>LSTM内部包含多种结构，包括：输出门、遗忘门、记忆单元、输入门等，下面我们先从LSTM 的基本结构进行介绍。</p><h3 id="基本结构">基本结构</h3><p>LSTM 中的记忆单元 <span class="math inline">\(c_t\)</span>存储了过去到时刻 <span class="math inline">\(t\)</span>的所有记忆，并基于此向外部层和下一时刻的 LSTM 输出隐藏状态 <spanclass="math inline">\(h_t\)</span>。</p><p>下图所示，LSTM 中当前记忆单元 <spanclass="math inline">\(c_t\)</span> 基于 3 个输入 <spanclass="math inline">\(c_{t-1}\)</span>、<spanclass="math inline">\(h_{t-1}\)</span>、<spanclass="math inline">\(x_t\)</span>经过“某种计算”得出。最后输出的隐藏状态 <spanclass="math inline">\(h_t\)</span> 是在 <spanclass="math inline">\(c_t\)</span> 的基础上应用 tanh函数计算得到的。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-17.png?x-oss-process=image/resize,w_800" /></p><p>这里提到的“某种计算”是下面要介绍的各种运算，它们都会涉及到一个概念——<strong>门</strong>（Gate）。门的本质就是阀门，可以控制流过数据的大小。在神经网络中，激活函数的概念与它非常相似。因此，LSTM使用经典的激活函数 <code>sigmoid</code> 作为各种门结构，后续将使用 <spanclass="math inline">\(\sigma\)</span> 符号进行表示。</p><h3 id="输出门">输出门</h3><p>首先，LSTM 增加了 <strong>输出门</strong>（Output Gate）。输入 <spanclass="math inline">\(x_t\)</span> 有权重 <spanclass="math inline">\(W_x\)</span>，输入隐藏状态 <spanclass="math inline">\(h_{t-1}\)</span> 有权重 <spanclass="math inline">\(W_h\)</span>。将它们的矩阵乘积和偏置 <spanclass="math inline">\(b\)</span> 之和传入 <spanclass="math inline">\(sigmoid\)</span> 函数，结果就是输门的输出值 <spanclass="math inline">\(o\)</span>。</p><span class="math display">\[\begin{aligned}o = \sigma (x_t W_x + h_{t-1} W_h + b)\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-18.png?x-oss-process=image/resize,w_800" /></p><p>输出门的输出值 <span class="math inline">\(o\)</span> 进一步与 <spanclass="math inline">\(tanh(c_t)\)</span>进行乘积。注意，这里的乘积并不是矩阵乘积，而是<strong>阿达玛乘积</strong>，即矩阵对应元素之间的乘积。如果使用 <spanclass="math inline">\(\circ\)</span> 表示阿达玛乘积，其计算式如下。</p><span class="math display">\[\begin{aligned}h_t = o \circ tanh(c_t)\end{aligned}\]</span><h3 id="遗忘门">遗忘门</h3><p>为了模拟更真实的记忆效果，LSTM 加入了 <strong>遗忘门</strong>（ForgetGate），其计算图如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-19.png?x-oss-process=image/resize,w_800" /></p><p>遗忘门的计算与输出门类似，区别在于两者在计算图中的位置不同，其计算式如下。</p><span class="math display">\[\begin{aligned}f = \sigma (x_t W_x + h_{t-1} W_h + b)\end{aligned}\]</span><h3 id="记忆单元优化">记忆单元优化</h3><p>如果针对记忆单元，我们只增加遗忘门，那么会导致记忆逐步遗忘。因此，我们还要增加强化记忆的结构，由此对记忆单元进行优化。这里，我们增加tanh 节点，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-20.png?x-oss-process=image/resize,w_800" /></p><p>tanh 节点计算与其他门的计算类似，如下所示。</p><span class="math display">\[\begin{aligned}g = tanh(x_t W_x + h_{t-1} W_h + b)\end{aligned}\]</span><p>基于 tanh 节点计算得到的结果 <span class="math inline">\(g\)</span>最终与加到上上一时刻的记忆单元 <spanclass="math inline">\(c_{t-1}\)</span> 中，从而形成新的记忆。</p><h3 id="输入门">输入门</h3><p>输入门判断新增信息 <span class="math inline">\(g\)</span>的各个元素的价值，会对待添加的信息进行取舍，其计算图如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-21.png?x-oss-process=image/resize,w_800" /></p><p>输入门的输出值计算如下所示。</p><span class="math display">\[\begin{aligned}i = \sigma (x_t W_x + h_{t-1} W_h + b)\end{aligned}\]</span><h3 id="完整结构">完整结构</h3><p>最后，我们可以得到如下所示的 LSTM 完整结构，可以看出 LSTM 是在 RNN的基础上增加了一系列门结构以及相关运算实现的。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-22.png?x-oss-process=image/resize,w_800" /></p><h3 id="反向传播-1">反向传播</h3><p>接下来，我们来看一下为什么 LSTM能够解决梯度消失的问题。其原因可以通过观察记忆单元 <spanclass="math inline">\(c\)</span> 的反向传播来了解，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-23.png?x-oss-process=image/resize,w_800" /></p><p>记忆单元的反向传播会反复经过 “+” 和 “×” 节点。“+” 节点的导数是1，会直接传递上游梯度至下游。“×”节点并不是矩阵乘积，而是阿达玛乘积。阿达玛乘积是矩阵中对应元素的乘积运算，而且每次都会基于不同的门值进行乘积运算，因此不会发生梯度消失或梯度爆炸。</p><p>在反向传播过程中，遗忘门认为应该忘记的记忆单元元素，其梯度会变小；不能忘记的记忆单元元素，其梯度并不会退化。因此，可以期待记忆单元能够保存学习长期的依赖关系。</p><h2 id="gru">GRU</h2><p>GRU（Gated Recurrent Unit），门控循环单元，其继承了 LSTM的思路，但是减少了参数，缩短了计算时间。</p><p>GRU 的计算图和计算式如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rnn-introduction-24.png?x-oss-process=image/resize,w_800" /></p><span class="math display">\[\begin{aligned}z = &amp; \space \sigma (x_t W_x + h_{t-1} W_h + b)\\r = &amp; \space \sigma (x_t W_x + h_{t-1} W_h + b)\\\widetilde{h} = &amp; \space tanh (x_t W_x + h_{t-1} W_h + b)\\h_t = &amp; \space (1 - z) \circ h_{t-1} + z \circ \widetilde{h}\end{aligned}\]</span><p>GRU 没有记忆单元，只有一个隐藏状态 <spanclass="math inline">\(h\)</span>在时间方向上传播。其使用了两个门结构：<spanclass="math inline">\(r\)</span>（reset 门）、<spanclass="math inline">\(z\)</span>（update 门）。</p><p><span class="math inline">\(r\)</span>决定在多大程度上“忽略”过去的隐藏状态。如果 <spanclass="math inline">\(r\)</span> 为 0，则新的隐藏状态 <spanclass="math inline">\(\widetiled{h}\)</span> 只取决于 <spanclass="math inline">\(x_t\)</span>，即过去的隐藏状态被忽略。</p><p><span class="math inline">\(z\)</span>是更新隐藏状态的门，其包含了LSTM 中遗忘门和输入门的作用。<spanclass="math inline">\(\widetilde{h}\)</span> 中的 <spanclass="math inline">\((1-z)\circ h_{t-1}\)</span>表示遗忘门的功能；<span class="math inline">\(z \circ\widetilde{h}\)</span> 表示输入门的功能。</p><p>整体而言，GRU 是简化版的 LSTM 架构，相比之下，GRU的参数和计算成本更少。关于 GRU 和 LSTM的选择，根据不同的任务和超参数的设置，结论可能不同。由于 GRU超参数少、计算量小，因此比较适合于数据集较小，设计模型需要反复实验的场景。</p><h1 id="总结">总结</h1><p>本文我们主要介绍了循环神经网络的结构，其内部根据时序数据可以展开成多个循环结构。为了区分，我们将整体称为Time RNN，将内部的单个循环结构称为 RNN。</p><p>Time RNN 的反向传播主要有两种方法，分别是：BPTT 和 TruncatedBPTT。后者是在前者的基础上对反向传播路径进行分段截断。从而在一定程度上缓解时序数据过长带来的时间问题、梯度问题、内存问题。</p><p>之后，我们进一步介绍了梯度问题中，梯度爆炸和梯度消失出现的原因。对于梯度爆炸，我们介绍了梯度裁剪的方法。对于梯度消失，我们介绍了Gate RNN 的两种结构：LSTM 和 GRU。</p><h1 id="参考">参考</h1><ol type="1"><li>《深度学习进阶：自然语言处理》</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;前面，我们介绍了适用于图像和视觉领域的 &lt;a
href=&quot;https://chuquan.me/2024/09/28/cnn-introduction/&quot;&gt;卷积神经网络&lt;/a&gt;。本文，我们来介绍适用于时序数据处理场景的
&lt;strong&gt;循环神经网络&lt;/strong&gt;（Recurrent Neural Network，RNN）。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://chuquan.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="RNN" scheme="http://chuquan.me/tags/RNN/"/>
    
    <category term="LSTM" scheme="http://chuquan.me/tags/LSTM/"/>
    
    <category term="Gated RNN" scheme="http://chuquan.me/tags/Gated-RNN/"/>
    
    <category term="GRU" scheme="http://chuquan.me/tags/GRU/"/>
    
    <category term="Time RNN" scheme="http://chuquan.me/tags/Time-RNN/"/>
    
    <category term="BPTT" scheme="http://chuquan.me/tags/BPTT/"/>
    
    <category term="Truncated BPTT" scheme="http://chuquan.me/tags/Truncated-BPTT/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理中的单词含义表示</title>
    <link href="http://chuquan.me/2024/10/15/nlp-word-representation/"/>
    <id>http://chuquan.me/2024/10/15/nlp-word-representation/</id>
    <published>2024-10-15T01:31:01.000Z</published>
    <updated>2024-10-15T01:34:17.411Z</updated>
    
    <content type="html"><![CDATA[<p>我们日常使用的语言，如中文、英文，称为<strong>自然语言</strong>（natural language）。<strong>自然语言处理</strong>（Natural LanguageProcessing，NLP）则是一种让计算机理解人类语言的技术。现在很多知名的工具，比如：搜索引擎、机器翻译、ChatGPT等，都是基于自然语言处理技术的应用。</p><span id="more"></span><h1 id="文本分词">文本分词</h1><p>大多数情况下，NLP处理的是由大量单词组成的文本。因此，这里面临的第一个问题是如何进行分词。</p><p>英文的分词比较简单，每个单词都是独立的，可以通过空、标点符号、正则表达式等进行分词。相比而言，中文的分词会显得比较复杂。对此，研究人员针对不同语言开发了各种不同的分词工具，比如：</p><ul><li>英文分词可以使用 NLTK、spaCy 等工具</li><li>中文分词可以使用 jieba、THULAC 等工具</li></ul><p>一些知名的预训练模型 BERT 和 GPT各自拥有专属的分词器（Tokenizer），比如：WordPiece、Byte PairEncoding（BPE）等。</p><h1 id="单词含义">单词含义</h1><p>自然语言的灵活性非常强，同一个单词在不同的语境下，不同的上下文中具有不同的含义，这与编程语言截然不同。因此，理解单词的含义成为了NLP 的核心问题之一，也是本文将重点讨论的话题。</p><p>在 NLP 技术的发展过程中，单词含义的表示方法主要经历了一下几种：</p><ul><li>基于同义词词典的方法</li><li>基于计数的方法</li><li>基于推理的方法</li></ul><p>下面，我们分别来介绍这几种单词含义的表示方法。</p><h1 id="同义词词典法">同义词词典法</h1><p>基于 <strong>同义词词典</strong>（Thesaurus）的方法的核心思路是：</p><ul><li>将含义相同或含义类似的单词归类到同一个组中</li><li>同时定义单词之间细粒度的层级关系，比如：上位与下位的关系，整体与局部的关系。</li></ul><p>如下所示的例子是一个含义组别中，各个单词之间的关系，使用图进行表示。其中，motorvehicle 和 car 是上位和下位的关系。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-01.png?x-oss-process=image/resize,w_800" /></p><p>在 NLP 领域中，最著名的同义词词典是普林斯顿大学开发的 <ahref="https://wordnet.princeton.edu/">WordNet</a>，其至今被应用于各种自然语言处理应用中。</p><h2 id="缺陷">缺陷</h2><p>首先，同义词词典是一个人工标记的词典，制作词典会耗费巨大的人力成本。</p><p>其次，同义词必须要随着语言的发展而更新。一方面，语言会出现新的单词，比如：团购、众筹等；另一方面，语言的含义也会发生变化，比如：卧龙凤雏，其含义从往年的褒义词变成了如今的贬义词。</p><p>然而最致命的缺陷还是同义词词典<strong>无法表示单词含义的微妙差异</strong>。各种语言中都存在单词含义相同，但是用法不同的情况，而这种细微的差别在同义词词典中是无法表示出来的。</p><h1 id="计数法">计数法</h1><p>计数法，也称为统计法，其核心思想是<strong>基于语料库（Corpus）中的大量文本数据，自动且高效地提取单词的本质信息</strong>。</p><h2 id="语料库预处理">语料库预处理</h2><p>语料库本质上就是文本数据而已，比如：Wikipedia、GoogleNews、文学作品集等。</p><p>在使用语料库之前，必须先进行预处理，预处理主要有以下几个步骤：</p><ul><li>对文本进行分词</li><li>为每个单词创建唯一的 ID</li><li>基于语料库创建（ID，单词）和（单词，ID）的映射表，便于后续进行查找</li></ul><p>如下所示，我们对一个简单的语料库进行预处理，分别得到<code>id_to_word</code> 和 <code>word_to_id</code> 两个映射表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>text</span><br><span class="line"><span class="string">&#x27;you say goodbye and i say hello .&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>words</span><br><span class="line">[<span class="string">&#x27;you&#x27;</span>, <span class="string">&#x27;say&#x27;</span>, <span class="string">&#x27;goodbye&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;say&#x27;</span>, <span class="string">&#x27;hello&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>id_to_word</span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">&#x27;you&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;say&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;goodbye&#x27;</span>, <span class="number">3</span>: <span class="string">&#x27;and&#x27;</span>, <span class="number">4</span>: <span class="string">&#x27;i&#x27;</span>, <span class="number">5</span>: <span class="string">&#x27;hello&#x27;</span>, <span class="number">6</span>:<span class="string">&#x27;.&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>word_to_id</span><br><span class="line">&#123;<span class="string">&#x27;you&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;say&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;goodbye&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;and&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;hello&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">6</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="分布式表示">分布式表示</h2><p>在 NLP 中，广泛使用 <strong>分布式表示</strong>（DistributedRepresentation）来表示单词含义。分布式表示将单词表示为<strong>固定长度的向量</strong>，其采用密集向量表示，即向量的大多数元素是由非0 实数表示的。比如，一个三维分布式表示<code>[0.21, -0.45, 0.83]</code>。</p><p>为什么使用向量来表示？因为对于向量，我们可以使用<strong>余弦相似度</strong>（CosineSimilarity）来计算两个向量之间的夹角余弦值，余弦值越接近1，表示向量夹角越小，说明相似度越高。使用向量表示单词，那么就可以计算不同单词含义之间的相似度。</p><p>假设，有两个向量 <span class="math inline">\(x = (x_1, x_2, x_3, ...,x_n)\)</span> 和 <span class="math inline">\(y = (y_1, y_2, y_3, ...,y_n)\)</span>，那么余弦相似度的定义如下。为了避免分母为0，一般会给分母加上一个微小值 eps，默认其值为 0.00000001。</p><span class="math display">\[\begin{aligned}similarity(x, y) = \frac{x \cdot y}{ ||x|| \space ||y|| }=  \frac{x_1 y_1 + ... + x_n y_n}{\sqrt{x_1^2 + ... + x_n^2} \sqrt{y_1^2+ ... + y_n^2}}\end{aligned}\]</span><h2 id="分布式假设">分布式假设</h2><p>在 NLP 中，很多研究都是基于<strong>分布式假设</strong>（DistributionalHypothesis）完成的。分布式假设认为，<strong>一个单词的含义是由其周围的单词形成的</strong>。单词本身没有含义，其含义是由上下文（语境）形成的。相同的单词经常出现在相同的语境中，比如："Idrink beer." 和 "I drink wine."，drink 附近经常会出现饮料，beer 和 wine则是含义相似的单词。</p><p>这里提到的“上下文”，指的是某个单词周围的单词。我们将上下文的大小称为<strong>窗口大小</strong>（Window Size）。假如，窗口大小为2，则表示上下文包含了目标单词前后各两个单词。如下所示，是一个窗口大小为2 的示例。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-02.png?x-oss-process=image/resize,w_800" /></p><h2 id="词嵌入">词嵌入</h2><p><strong>词嵌入</strong>（Word Embedding）表示<strong>将单词映射到向量中间的过程或方法</strong>，也可以认为是<strong>基于分布式假设将单词转换成分布式表示的过程或方法</strong>。</p><p>计数法的词嵌入，可以分为以下几个步骤：</p><ul><li>首先，进行语料库预处理，得到每个单词的 ID 以及两个映射表</li><li>其次，定义每个单词的向量维度为语料库中不同单词的数量</li><li>然后，定义上下文大小，即窗口大小</li><li>最后，根据窗口大小，为每个单词统计其上下文的单词数量，并记录在向量中指定位置（各个单词ID 指定的位置）</li></ul><p>假设，我们以 “you say goodby and i say hello.”作为语料库，定义窗口大小为 1。由于语料库中总共有 7个不同的单词，包括句号，因此单词的向量维度等于 7。</p><p>首先，我们来看 "you" 的向量表示，其上下文只有一个单词"say"，那么我们可以将单词 "say" 的 ID 对应在向量中的位置进行计数，值为1，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-03.png?x-oss-process=image/resize,w_800" /></p><p>然后，我们来看 “say” 的向量表示。语料库中 “say”出现了两次，因此我们需要分别统计这两个 “say”的上下文，并在向量对应的位置进行计数，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-04.png?x-oss-process=image/resize,w_800" /></p><p>当对语料库中所有单词进行了词嵌入的处理后，我们可以得到所有的单词的向量表示，它们共同构成了一个矩阵，我们称之为<strong>共现矩阵</strong>（Co-ocurrence Matrix），如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-05.png?x-oss-process=image/resize,w_800" /></p><h2 id="优化">优化</h2><h3 id="高频词汇问题">高频词汇问题</h3><p>共现矩阵的元素表示每个单词与其上下文的单词共同出现的次数。这种表示方法对于高频词汇可能会出现问题。比如：the和 car 的共现次数会非常大，甚至要远远大于 car 和 drive的共现次数。对于这种情况，实际上我们不应该认为 the 和 car具有很强的相关性，因为 the 仅仅是一个常用词。</p><p>为了解决这个问题，我们可以使用 <strong>点互信息</strong>（PointwiseMutual Information，PMI）指标，其定义如下所示。</p><span class="math display">\[\begin{aligned}PMI(x, y) = log_2 \frac{P(x, y)}{P(x) P(y)}\end{aligned}\]</span><p>其中，<span class="math inline">\(P(x)\)</span> 表示 x发生的概率，<span class="math inline">\(P(y)\)</span> 表示 y发生的概率，<span class="math inline">\(P(x, y)\)</span> 表示 x 和 y同时发生的概率。PMI 的值越高，表明相关性越强。</p><p>假设语料库的单词数量为 10000，the 出现 100 次，car 出现 20 次，drive出现 10 次，the 和 car 共出现 10 次，car 和 drive 共出现 5次。那么，我们可以计算一下 PMI 值，如下所示。</p><span class="math display">\[\begin{aligned}PMI(the, car) = &amp; log_2 \frac{10 \cdot 10000}{1000 \cdot 20} \approx2.32\\PMI(car, drive) = &amp; log_2 \frac{5 \cdot 10000}{20 \cdot 10} \approx7.97\end{aligned}\]</span><p>通过PMI，我们可以解决高频词汇在共现矩阵中对于单词关联性的影响。不过，PMI也有一个问题，即当两个单词共现次数为0 时，<span class="math inline">\(log_2 0 =-\infty\)</span>。为了解决这问题，在实际中使用<strong>正的点互信息</strong>（PositivePMI，PPMI）来处理，其本质就是使用最小值 0来进行兜底，其定义如下所示。</p><span class="math display">\[\begin{aligned}PPMI(x, y) = max(0, PMI(x, y))\end{aligned}\]</span><p>经过 PPMI 的处理，我们可以将共现矩阵转换成 PPMI 矩阵，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-06.png?x-oss-process=image/resize,w_800" /></p><h3 id="向量维度问题">向量维度问题</h3><p>计数法还存在一个问题，那就是随着语料库词汇量的增加，各个单词向量的维数也会增加。如果语料库的词汇量达到10 万，那么单词向量的维度也会达到 10万。维度过高会导致处理计算量极大增加，而且向量中绝大多数元素都是0，会造成存储空间浪费。对此，我们要对向量进行<strong>降维</strong>（Dimensionality Reduction）。</p><p>降维的核心思想是<strong>从稀疏矩阵中找到重要的轴，用更少的维度重新表示</strong>。比如下图所示，我们发现二维向量的分布具有某种特点，而这种特点可以使用一维向量来表示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-07.png?x-oss-process=image/resize,w_800" /></p><p>具体而言，我们可以使用 <strong>奇异值分解</strong>（Singular ValueDecomposition，SVD）来进行降维。关于 SVD的工作原理，要解释明白的话，涉及的篇幅会很大，这里不作具体阐述，有兴趣的朋友可以自行去了解。</p><h1 id="推理法">推理法</h1><p>基于推理的方法，其本质上是基于神经网络来学习单词的内在含义，最终使用权重来表示单词的含义。推理法使用部分学习数据逐步进行学习；计数法则使用整个语料库进行数据统计，一次性处理得到单词的分布式表示。两者之间的差异如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-08.png?x-oss-process=image/resize,w_800" /></p><h2 id="语料库预处理-1">语料库预处理</h2><p>类似于计数法，推理法也会对语料库进行预处理，主要有以下几个步骤：</p><ul><li>对文本进行分词</li><li>为每个单词创建唯一的 ID</li><li>为每个单词定义 one-hot 表示</li></ul><p>什么是 one-hot 表示？one-hot表示会定义一个固定长度的向量，该向量的长度等于语料库中词汇的数量。每个单词的one-hot 表示中，只有其 ID 对应的元素为 1，其余元素均为 0。如下所示为one-hot 表示的示意图。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-09.png?x-oss-process=image/resize,w_800" /></p><h2 id="分布式表示-1">分布式表示</h2><p>推理法中单词的分布式表示是通过神经网络的权重来表示的。如下所示，输入层和中间层会构建一个全连接层，其中输入层的神经元数量等于one-hot 表示的长度，也就是语料库的词汇数量，比如7；中间层的神经元数量可以自定义，比如 3。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-10.png?x-oss-process=image/resize,w_800" /></p><p>通过学习，我们最终可以得到一个 7 x 3的权重矩阵。我们使用任意一个单词的 one-hot表示与权重矩阵进行乘积，即可提取到对应单词的分布式表示，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-11.png?x-oss-process=image/resize,w_800" /></p><h2 id="词嵌入-1">词嵌入</h2><p>推理法的词嵌入方法很多，本质而言，这些方法都是基于不同的神经网络模型实现的，通常我们会直接用神经网络模型表示不同的方法，比如：word2vec、gloVe等。这里我们主要介绍 word2vec 模型，其主要有两种架构，分别是：</p><ul><li>CBOW（Continuous Bag ofWords）模型：根据上下文的多个单词预测中间的单词</li><li>skip-gram 模型：根据中间的单词预测上下文的单词</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-12.png?x-oss-process=image/resize,w_800" /></p><p>下面，我们分别来介绍这两种结构。</p><h3 id="cbow-模型">CBOW 模型</h3><p>CBOW模型，也称为连续词袋模型，其工作原理是通过上下文的多个单词来预测中间的单词。</p><p>CBOW 模型包含了输入层、中间层、输出层。输入层接收单词的 one-hot表示，中间层通过权重矩阵将输入转换成低维度的密集向量，输出层则使用softmax 函数预测目标词的概率分布，如下所示。输出层的各个神经元经过softmax 的计算可以得到各自的概率值，概率值最大的值转换成 1，其余转换成0，由此得到的 one-hot 表示可以对应一个单词。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-13.png?x-oss-process=image/resize,w_800" /></p><p>CBOW 模型基于 softmax 函数和交叉熵误差进行学习，两者可以构建一个Softmax with Loss 层，学习阶段时完备的神经网络结构如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-14.png?x-oss-process=image/resize,w_800" /></p><h3 id="skip-gram-模型">Skip-gram 模型</h3><p>Skip-gram 模型，其工作原理是通过中间的单词预测上下文的单词。</p><p>Skip-gram模型同样包含了输入层、中间层、输出。区别在于输出层只接收中间单词的one-hot编码，中间层同样通过权重矩阵转换成密集向量，输出层则为上下文中的每个单词分配概率，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-15.png?x-oss-process=image/resize,w_800" /></p><p>Skip-gram 模型的输出层数量与上下文单词的个数相同。因此需要通过Softmax with Loss 层分别求出各个输出层的损失并求和。具体而言，skip-gram模型的函数可以表示如下所示的函数。</p><span class="math display">\[\begin{aligned}L = - \frac{1}{T} \sum_{t=1}^{T} (logP(w_{t-1}|w_t) + logP(w_{t+1}|w_t))\end{aligned}\]</span><p>其中，<span class="math inline">\(P(w_{t-1}|w_t)\)</span> 表示给定<span class="math inline">\(w_t\)</span> 时，<spanclass="math inline">\(w_{t-1}\)</span> 发生的概率；<spanclass="math inline">\(P(w_{t+1}|w_t)\)</span> 表示给定 <spanclass="math inline">\(w_t\)</span> 时，<spanclass="math inline">\(w_{t+1}\)</span> 发生的概率。</p><h3 id="cbow-vs-skip-gram">CBOW vs skip-gram</h3><p>关于 CBOW 和 skip-gram的对比，从单词的分布式表示的准确度而言，skip-gram的表现更好。从学习速度而言，CBOW 的表现更快。</p><p>在实际应用中，更多会选择skip-gram。因为一旦生成了单词的分布式表示后，我们可以重复应用到各种场景中。因此可以无需考虑学习速度，更应该注重准确度。</p><h2 id="优化-1">优化</h2><p>推理法存在一个问题，即 one-hot 表示的维度问题。由于 one-hot表示的维度等于语料库的词汇量数量，如果语料库的词汇量为 100万，那么神经网络中各个层之间的矩阵运算的数据量将非常大，极其影响性能。</p><p>我们假设语料库词汇量有 100 万个，中间层神经元有 100 个，CBOW模型如下所示。下面，我们将以 word2vec 的 CBOW 模型为例，进行优化。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-16.png?x-oss-process=image/resize,w_800" /></p><h3 id="输入层维度问题">输入层维度问题</h3><p>首先，我们考虑 one-hot 表示对于输入层维度的影响。我们使用 one-hot表示作为输入，one-hot 表示和权重矩阵 <spanclass="math inline">\(W_{in}\)</span>的乘积显然会耗费巨大的计算资源，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-17.png?x-oss-process=image/resize,w_800" /></p><p>从图中可以看出，矩阵乘法的目的是为了取出单词的 ID 在权重矩阵 <spanclass="math inline">\(W_{in}\)</span>中对应行的向量，即单词对应的向量表示。对此，我们可以创建一个层，专门用于从权重参数中提取<strong>单词 ID 对应行（向量）</strong>，称之为 <strong>Embedding层</strong>。</p><p>Embedding 层以单词 ID作为输入，输出单词的向量表示。在反向传播时，权重只会更新单词 ID对应行的权重，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-18.png?x-oss-process=image/resize,w_800" /></p><p>当引入 Embedding 层之后，我们可以得到 CBOW模型的全貌图，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-20.png?x-oss-process=image/resize,w_800" /></p><h3 id="输出层维度问题">输出层维度问题</h3><p>其次，我们考虑 one-hot 表示对于输出层维度的影响。</p><p>首先，权重矩阵 <span class="math inline">\(W_{out}\)</span> 的大小是100 x 1000000，中间层的单词向量为 100。很显然，单词向量与权重矩阵 <spanclass="math inline">\(W_{out}\)</span>的乘积页会耗费巨大的计算资源。</p><p>其次，基于如下所示的 softmax 公式，我们可以看出 softmax也存在计算量过大的问题。</p><span class="math display">\[\begin{aligned}y_k = \frac{exp(s_k)}{\sum_{i=1}^{1000000} \space exp(s_i)}\end{aligned}\]</span><p>对此，我们使用 <strong>负采样</strong>（NegativeSampling）来进行优化。负采样的核心思想是<strong>使用二分类拟合多分类</strong>。举个例子，在未优化的实现中，我们使用softmax 计算出所有单词的概率值，由此构建一个 one-hot结果，来与正确值进行对比。而负采样的实现中，我们给定一个单词，神经网络计算这个单词的概率，判断它是否是正确值。这样就能将计算复杂度从O(N) 降低至 O(1)，其示意图如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-19.png?x-oss-process=image/resize,w_800" /></p><p>另一方面，<strong>在多分类的情况下，输出层使用 softmax函数将得分转化为概率，损失函数使用交叉熵误差。在二分类的情况下，输出层使用sigmoid 函数，损失函数也使用交叉熵误差</strong>。因此，我们可以进一步将Softmax-with-Loss 替换成Sigmoid-with-Loss，得到如下所示的二分类神经网络。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-21.png?x-oss-process=image/resize,w_800" /></p><p>至此，我们已经将多分类问题转换成了二分类问题，神经网络结构也进行了优化。下面，我们来看看二分类情况下如何进行学习。</p><h4 id="负采样的样本学习">负采样的样本学习</h4><p>负采样的样本学习，核心思想是使用一个正例、采样选择若干个负例，对这些数据的损失求和，基于此来进行学习，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/nlp-word-presentation-22.png?x-oss-process=image/resize,w_800" /></p><p>关于负例的选择，负采样的做法是：基于语料库中单词使用频率，计算出各个单词的概率分布，然后基于概率分布对单词进行采样。这样可以尽可能保证单词的出现概率与真实的情况类似。此外，为了避免稀有单词出现的概率过低，这里还会对概率分布进行调整，如下所示。</p><span class="math display">\[\begin{aligned}P&#39;(w_i) = \frac{P(w_i)^{0.75}}{\sum_j^n P(w_j)^{0.75}}\end{aligned}\]</span><h1 id="计算法-vs-推理法">计算法 vs 推理法</h1><p>最后，我们来简单对比一下计数法和推理法。</p><p>首先，两种方法在学习机制上存在差异。计数法通过对整个语料库的统计数据进行一次学习来获得单词的分布式表示；推理法则反复基于语料库的部分数据进行学习。每当语料库新增词汇时，计数法需要重新计算，完成共现矩阵生成、SVD等一系列操作。推理法则可以允许参数的增量学习，可以将之前学习到的权重作为下一次学习的初始值。</p><p>其次，两种方法在单词的分布式表示的性质上存在差异。计数法主要是编码单词的相似性。推理法除了单词的相似性之外，还能理解单词之间更复杂的模式，比如能够求解"king - man + woman = queen" 这样的类推问题。</p><p>当然，如果只评判单词的相似性，两种方法则不相上下。</p><h1 id="总结">总结</h1><p>本文我们主要介绍了自然语言处理中的核心问题之一——单词含义的表示。由此，我们介绍了三种方法，分别是词典法、计数法、推理法。</p><p>词典法的核心思想是<strong>将含义相同或含义类似的单词归类到同一个组中，同时定义单词之间细粒度的层级关系，比如：上位与下位的关系，整体与局部的关系</strong>。</p><p>计数法的核心思想是<strong>基于语料库（Corpus），根据上下文的窗口大小，统计目标词周围的单词的频率</strong>，这种方法的依据是<strong>分布式假设</strong>，即一个单词的含义是由其周围的单词形成。</p><p>推理法则是采用了神经网络的方式，学习样本数据，更新权重，以权重作为单词含义的表示。文中，我们介绍了word2vec 模型的两种架构，分别是 CBOW 和 skip-gram。</p><h1 id="参考">参考</h1><ol type="1"><li>《深度学习：自然语言处理》</li><li>《图解GPT》</li><li><ahref="https://www.53ai.com/news/qianyanjishu/2024070628415.html">一文彻底搞懂Transformer- Word Embedding（词嵌入）</a></li><li>Pennington, Jeffrey, Richard Socher, Christopher D. Manning.Glove:Global Vectors for Word Representation[J]. EMNLP. Vol.14. 2014.</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;我们日常使用的语言，如中文、英文，称为
&lt;strong&gt;自然语言&lt;/strong&gt;（natural language）。
&lt;strong&gt;自然语言处理&lt;/strong&gt;（Natural Language
Processing，NLP）则是一种让计算机理解人类语言的技术。现在很多知名的工具，比如：搜索引擎、机器翻译、ChatGPT
等，都是基于自然语言处理技术的应用。&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="http://chuquan.me/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="词典法" scheme="http://chuquan.me/tags/%E8%AF%8D%E5%85%B8%E6%B3%95/"/>
    
    <category term="计数法" scheme="http://chuquan.me/tags/%E8%AE%A1%E6%95%B0%E6%B3%95/"/>
    
    <category term="推理法" scheme="http://chuquan.me/tags/%E6%8E%A8%E7%90%86%E6%B3%95/"/>
    
    <category term="word2vec" scheme="http://chuquan.me/tags/word2vec/"/>
    
    <category term="CBOW" scheme="http://chuquan.me/tags/CBOW/"/>
    
    <category term="skip-gram" scheme="http://chuquan.me/tags/skip-gram/"/>
    
    <category term="SVD" scheme="http://chuquan.me/tags/SVD/"/>
    
    <category term="负采样" scheme="http://chuquan.me/tags/%E8%B4%9F%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络</title>
    <link href="http://chuquan.me/2024/09/28/cnn-introduction/"/>
    <id>http://chuquan.me/2024/09/28/cnn-introduction/</id>
    <published>2024-09-28T12:27:29.000Z</published>
    <updated>2024-09-28T12:32:32.900Z</updated>
    
    <content type="html"><![CDATA[<p>通过前面几篇文章，我们对神经网络有了一个基本的认识。本文，我们来介绍一下神经网络在图像与视觉领域的应用——<strong>卷积神经网络</strong>（ConventionalNeural Network，简称 CNN）。</p><span id="more"></span><h1 id="视觉认知原理">视觉认知原理</h1><p>在深度学习的发展过程中，广泛应用了大脑认知原理，尤其是视觉认知原理，这里我们来简单了解一下视觉认知原理。</p><p>1981 年的诺贝尔医学奖，颁发给了 David Hubel 和 TorstenWiesel，以及RogerSperry。前两位的主要贡献，是发现了视觉系统的信息处理，可视皮层是分级的。整体而言，人类的视觉认知原理大体分为以下几个步骤：</p><ul><li>通过瞳孔摄入原始的像素信号</li><li>通过大脑皮层的细胞发现边缘和方向</li><li>对边缘和方向进行组合，从而完成形状判定</li><li>对形状进行高级视觉抽象，从而完成分类认知</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-01.png?x-oss-process=image/resize,w_800" /></p><p>人类视觉认知是一个分层递进的过程：提取边缘特征，抽象高级特征，组合整体图像，最终完成分类。下图所示是对于不同物体的视觉认知过程。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-02.png?x-oss-process=image/resize,w_800" /></p><h1 id="全连接层的局限性">全连接层的局限性</h1><p>理论上，我们可以使用一个由全连接层组成的神经网络来进行图像识别，事实上，在<ahref="https://chuquan.me/2024/07/31/neural-network-implement/">初识人工神经网络（2）——代码实现</a>一文中，我们也是这么做的。</p><p>然而，对于复杂图像，全连接神经网络的表现并不好，根本原因在于<strong>全连接层忽略了数据的形状</strong>。图像是 3 维形状（宽、高、RGB通道），空间上邻近的像素会有相似的值，RGB各通道之间也有关联性。而全连接层会忽略形状，将数据作为一维数据进行处理，因此无法利用数据的空间关联信息。</p><p>因此，为了提取图像数据的空间关联信息，于是诞生了卷积神经网络。</p><h1 id="卷积神经网络">卷积神经网络</h1><p>我们首先来看一下全连接神经网络和卷积神经网络的整体结构，如下所示。两者的主要区别在于：<strong>在输入端，CNN络使用「卷积层 + ReLU + 池化层」的结构代替了「全连接层 +ReLU」的结构</strong>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-08.png?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-09.png?x-oss-process=image/resize,w_800" /></p><p>整体而言，卷积神经网络的工作流程主要分为三个阶段，分别是：</p><ul><li>通过 <strong>卷积层</strong>（ConvolutionLayer）对图像进行特征提取，得到特征图；</li><li>通过 <strong>池化层</strong>（PoolingLayer）对特征图进行降采样，压缩数据量；</li><li>通过 <strong>全连接层</strong>（AffineLayer）对特征图进行分类，得到识别结果。</li></ul><p>下面，我们分别介绍一下卷积层和池化层。</p><h1 id="卷积层">卷积层</h1><p>在 <ahref="https://chuquan.me/2024/03/30/foundation-of-computer-graphic-04/">《计算机图形学基础（4）——光栅化》</a>一文中，我们提到过卷积（Convolution）是实现滤波的主要数学工具和底层原理。滤波器的基本原理是<strong>响应函数与输入信号进行卷积运算</strong>，因此滤波器也可以称为<strong>卷积核</strong>。</p><p>卷积神经网络的核心在于卷积层，卷积层则包含了大量滤波器，这些滤波器通过监督学习，自适应调整内部权重参数，从而能够更加准确地提取特征值。下图所示是一个图片应用不同滤波器的示例。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-05.png?x-oss-process=image/resize,w_800" /></p><h2 id="卷积运算">卷积运算</h2><p>了解了滤波器的作用后，我们再来看一下滤波器的内部原理——卷积运算。如下所示，是一个卷积运算的例子。其中，输入大小是<span class="math inline">\((4, 4)\)</span>，滤波器大小是 <spanclass="math inline">\((3, 3)\)</span>，输出大小是 <spanclass="math inline">\((2, 2)\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-06.png?x-oss-process=image/resize,w_800" /></p><p>下图所示展示了卷积运算的执行顺序。对于输入数据，卷积运算以一定步幅滑动滤波器的窗口，同时将滤波器元素和对应的输入元素相乘并求和，最终保存至对应的输出位置，即可得到卷积运算结果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-07.png?x-oss-process=image/resize,w_800" /></p><p>全连接神经网络中，除了权重参数，还存在偏置。同样，在 CNN中，也存在偏置，下图所示展示了包含偏置的卷积运算。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-10.png?x-oss-process=image/resize,w_800" /></p><h2 id="填充">填充</h2><p>前面，我们介绍了一个卷积运算的例子，其输入大小是 <spanclass="math inline">\((4, 4)\)</span>，滤波器大小是 <spanclass="math inline">\((3, 3)\)</span>，输出大小是 <spanclass="math inline">\((2,2)\)</span>。此时可以会发现输出结果的大小已经小于滤波器的大小，将无法继续执行卷积运算，那么该如何解决呢？我们可以使用<strong>填充</strong>（Padding）来解决。</p><p>如下所示，我们对输入数据进行了幅度为 1 的填充，输入大小从 <spanclass="math inline">\((4, 4)\)</span> 变成了 <spanclass="math inline">\((6, 6)\)</span>，输出大小从 <spanclass="math inline">\((2, 2)\)</span> 变成了 <spanclass="math inline">\((6,6)\)</span>。由此可见，填充可以调整卷积运算的输出大小，填充增大，输出大小增大。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-11.png?x-oss-process=image/resize,w_800" /></p><h2 id="步幅">步幅</h2><p>除了填充，我们还可以通过<strong>步幅</strong>（Stride）来控制输出大小。下图所示，我们设置步幅为2，滤波器每次滑动的间隔则为 2。整体而言，步幅增大，输出大小减小。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-12.png?x-oss-process=image/resize,w_800" /></p><h2 id="滤波器通道">滤波器通道</h2><p>上述例子中的输入是 2 维数据，但是图像是 3 维数据（宽、高、RGB通道），对此，我们也可以为滤波器增加一个维度——通道。</p><p>下图所示，纵深方向为通道方向，输入数据的通道数为3，与此对应，滤波器的通道数也为3。每个通道的输入数据与对应通道的滤波器进行卷积运算，最后对所有通道的输出进行求和，从而得到输出结果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-13.png?x-oss-process=image/resize,w_800" /></p><p>此时，我们可以将输入数据抽象为 3 个维度，分别是通道数 <spanclass="math inline">\(C\)</span>、高度 <spanclass="math inline">\(H\)</span>，宽度 <spanclass="math inline">\(W\)</span>，整体使用 <spanclass="math inline">\((C, H, W)\)</span> 表示；滤波器也样，高度 <spanclass="math inline">\(FH\)</span>（Filter Height）、宽度 <spanclass="math inline">\(W\)</span>（Filter Width），通道数为 <spanclass="math inline">\(C\)</span>，整体使用 <spanclass="math inline">\((C, FH, FW)\)</span> 表示；输出大小为 <spanclass="math inline">\((1, OH, OW)\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-14.png?x-oss-process=image/resize,w_800" /></p><h2 id="滤波器数量">滤波器数量</h2><p>上述卷积运算一次只能输出一张特征图（FeatureMap），如果希望一次输出多张特征图，我们可以增加滤波器的数量，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-15.png?x-oss-process=image/resize,w_800" /></p><h1 id="池化层">池化层</h1><p>如果卷积层的输入数据非常大，那么输出特征图的数据量也会非常大。为了降低后续处理的数据量，以及提高鲁棒性、防止过拟合，卷积神经网络会在卷积层之后增加一个池化层。</p><p>池化的本质是对输出特征图进行局部采样，也称降采样（Downsampling）。下图所示，是对一个输出特征图进行池化运算的示意图，其中使用的池化层为Max 池化层，即计算局部区域中的最大值。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-16.png?x-oss-process=image/resize,w_800" /></p><p>对于多通道的场景，池化层将同时处理多个通道的数据，并进行池化运算，如下所示展示的是多通道池化运算顺序。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-17.png?x-oss-process=image/resize,w_800" /></p><h1 id="全连接层">全连接层</h1><p>经过多层「卷积 + ReLU +池化」的运算，神经网络能够提炼出经过高度简化、高度抽象的特征，这些特征总结了图像的空间信息。最后，为了对图像进行分类识别，我们需要使用全连接层来进行处理。</p><p>如下图所示，全连接层会将池化层的输出结果进行降维输入，在监督学习的过程中，除了调整卷积层中的滤波器权重参数，也会调整全连接的权重参数，最终输出准确的分类结果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-18.png?x-oss-process=image/resize,w_800" /></p><h1 id="经典-cnn">经典 CNN</h1><p>时至今日，业界已经出现了各种卷积神经网络。这里，我们介绍其中两个经典的网络，一个是1998 年首次被提出的 CNN——LeNet；另一个是在深度学习受到关注的 2012年提出的 AlextNet。</p><h2 id="lenet">LeNet</h2><p>LeNet 在 1998年提出时是用于解决手写数字识别的问题。如下所示，它包含连续的卷积层和子采样层，最后通过全连接层输出结果。与现在的CNN 相比，LeNet 有几个不同点：</p><ul><li>LeNet 使用 sigmoid 函数作为激活函数，而现在的 CNN 主要使用 ReLU函数作为激活函数</li><li>Lext 采用子采样（Subsampling）进行数据压缩，而现在的 CNN主要使用池化层（Max 池化）进行数据压缩</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-19.png?x-oss-process=image/resize,w_800" /></p><h2 id="alexnet">AlexNet</h2><p>AlexNet 是图领奖获得者 Geoffrey Hinton 和他的学生 Alex Krizhevsky 在2012 年提出的，AlexNet 提出后引发了深度学习的浪潮，不过它的网络结构和LeNet 并没有本质的区别。</p><p>AlexNet 和 LeNet 之间的主要区别在于：</p><ul><li>AlexNet 采用了 ReLU 作为激活函数</li><li>AlexNet 采用了局部正规化的 LRN（Local ResponseNormalization）层</li><li>AlexNet 采用了 Drouput，可以有效抑制过拟合</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/cnn-intro-20.png?x-oss-process=image/resize,w_800" /></p><h1 id="总结">总结</h1><p>本文我们主要介绍了卷积神经网络的基本概念及其工作原理，相比于全连接神经网络，卷积神经网络主要增加了卷积层和池化层，其中卷积层用于提取图像数据的空间特征，池化层用于压缩数据，提高鲁棒性，最后使用全连接层进行分类结果的计算和输出。</p><h1 id="参考">参考</h1><ol type="1"><li>《深度学习入门：基于Python的理论与实现》</li><li><ahref="https://cs231n.github.io/convolutional-networks/">ConvolutionalNeural Networks (CNNs / ConvNets)</a></li><li><ahref="https://www.zhihu.com/question/29366638/answer/3383464185">卷积神经网络和深度神经网络的区别是什么？</a></li><li><ahref="https://easyai.tech/ai-definition/cnn/#google_vignette">卷积神经网络-CNN</a></li><li><ahref="https://towardsdatascience.com/convolutional-neural-network-1368ee2998d3">ConvolutionalNeural Network</a></li><li><a href="https://mlnotebook.github.io/post/CNN1/">ConvolutionalNeural Networks - Basics</a></li><li><ahref="https://chuquan.me/2024/03/30/foundation-of-computer-graphic-04/#%E5%8D%B7%E7%A7%AF">卷积</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;通过前面几篇文章，我们对神经网络有了一个基本的认识。本文，我们来介绍一下神经网络在图像与视觉领域的应用——&lt;strong&gt;卷积神经网络&lt;/strong&gt;（Conventional
Neural Network，简称 CNN）。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://chuquan.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="卷积层" scheme="http://chuquan.me/tags/%E5%8D%B7%E7%A7%AF%E5%B1%82/"/>
    
    <category term="池化层" scheme="http://chuquan.me/tags/%E6%B1%A0%E5%8C%96%E5%B1%82/"/>
    
    <category term="AlextNet" scheme="http://chuquan.me/tags/AlextNet/"/>
    
    <category term="LeNet" scheme="http://chuquan.me/tags/LeNet/"/>
    
  </entry>
  
  <entry>
    <title>神经网络的分层设计原理</title>
    <link href="http://chuquan.me/2024/09/21/deep-learning-layer/"/>
    <id>http://chuquan.me/2024/09/21/deep-learning-layer/</id>
    <published>2024-09-21T12:13:04.000Z</published>
    <updated>2024-09-21T12:18:21.985Z</updated>
    
    <content type="html"><![CDATA[<p>在深度学习中，为了便于组合任意结构、任意层次的神经网络，通常会对神经网络进行分层设计，这也是一种模块化的设计思想。</p><span id="more"></span><p>我们知道，神经网络的核心功能是 <strong>推理</strong> 和<strong>学习</strong>，也称为 <strong>正向传播</strong>（ForwardPropagation） 和 <strong>反向传播</strong>（BackwardPropagation）。为了实现结构化的分层设计，在局部层面，每个分层都必须同样具备对应的能力。</p><h1 id="计算图">计算图</h1><p>为了理解神经网络各个分层的工作原理，我们将使用<strong>计算图</strong>（ComputationalGraphic）的方式来分析推理和学习的过程。</p><p>下面，我们尝试计算图使用来分析一个简单的问题：</p><blockquote><p>张三在超市买了 2 个西瓜，其中西瓜每个 100 元，消费税为10%。对此，请计算两个问题： 1. 本次消费的总金额是多少？ 2.西瓜价格的上涨会在多大程度上影响最终的支付金额？即“支付金额关于西瓜价格的导数”是什么？</p></blockquote><h2 id="正向传播">正向传播</h2><p>对于第一个问题，我们可以得到如下所示的计算图的正向传播路径。其中，每个节点是一个数学函数，对其输入进行计算，得到对应的输出，并正向传播至下一个节点。</p><p>计算图可以将一个复杂的整体运算拆分成多个简单的局部运算。同时，将各个局部运算的结果不断地传递至其他计算节点，进而可以得到整体结果。最终可得：本次消费的总金额为220 元。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-01.png?x-oss-process=image/resize,w_800" /></p><h2 id="反向传播">反向传播</h2><p>对于第二个问题，我们可以进一步得到计算图的反向传播路径。这里从右向左依次传递导数，1，1.1，2.2。最终可得：支付金额关于西瓜的价格的导数为2.2，即西瓜价格每上涨 1 元，最终支付价格会增加 2.2 元。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-02.png?x-oss-process=image/resize,w_800" /></p><p>这里为什么使用反向传播计算导数呢？因为神经网络的学习过程就是基于损失函数的导数（更准确地说，是梯度）来进行的，所以列举了一个类似的问题。</p><h3 id="损失函数">损失函数</h3><p>神经网络的反向传播是围绕 <strong>损失函数</strong>（LossFunction，或称误差函数）完成的，其表示神经网络的性能，即当前神经网络的推理结果对比监督数据的正确值在多大程度上不拟合。</p><p>神经网络的学习过程的核心思想就是<strong>通过损失函数计算其梯度，并结合梯度下降法，根据梯度的正负值来更新权重</strong>。如果梯度值为负，通过调整权重参数向正方向改变，可以减小损失函数的值；如果梯度值为正，通过调整权重参数向负方向改变，可以减小损失函数的值。</p><p>下面，我们来介绍两个最常见的损失函数。</p><h4 id="均方误差">均方误差</h4><p><strong>均方误差</strong>（Mean SquaredError）的计算公式如下所示：</p><span class="math display">\[\begin{aligned}E = \frac{1}{2} \sum_k (y_k - t_k)^2\end{aligned}\]</span><p>其中，<span class="math inline">\(y_k\)</span>表示神经网络的输出，<span class="math inline">\(t_k\)</span>表示监督数据的正确值，<span class="math inline">\(k\)</span>表示数据的维度。</p><p>均方误差会计算神经网络的输出和监督数据的各个元素之差的平方，再求总和。我们在<ahref="https://chuquan.me/2024/07/21/neural-network-introduce/">《初识人工神经网络（1）——基本原理》</a>中介绍的损失函数就是均方误差。</p><p>根据定义，我们可以完成均方误差的代码实现，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_error</span>(<span class="params">y, t</span>): </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.<span class="built_in">sum</span>((y-t)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="交叉熵误差">交叉熵误差</h4><p><strong>交叉熵误差</strong>（Cross EntropyError）的计算公式如下所示：</p><span class="math display">\[\begin{aligned}E = - \sum_k t_k log y_k\end{aligned}\]</span><p>其中，<span class="math inline">\(log\)</span> 表示以 <spanclass="math inline">\(e\)</span> 为底数的自然对数 <spanclass="math inline">\(log_e\)</span>，<spanclass="math inline">\(y_k\)</span> 表示神经网络的输出，<spanclass="math inline">\(t_k\)</span> 为监督数据的正确值。</p><p>这里采用 one-hot 表示法，即 <span class="math inline">\(t_k\)</span>中只有正确值的位置的值为 1，其他均为0。因此，交叉熵误差实际上只计算对应正确值标签的输出的自然对数。比如：当正确值标签的索引是2，如果对应的神经网络的输出是 0.6，那么交叉熵误差为 <spanclass="math inline">\(-log0.6 =0.51\)</span>；如果对应的神经网络的输出是 0.1，那么交叉熵误差为 <spanclass="math inline">\(-log0.1 =2.30\)</span>。由此可以看出，在正确值的位置的输出值越接近1，则误差越小。</p><p>根据定义，我们可以完成交叉熵误差的代码实现，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>): </span><br><span class="line">    delta = <span class="number">1e-7</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + delta))</span><br></pre></td></tr></table></figure><h3 id="链式法则">链式法则</h3><p>神经网络的学习过程是基于损失函数的导数（梯度）来完成的，由于反向传播的路径中包含了多个分层，以及大量的权重参数，因此我们需要考虑如何对损失函数的导数进行拆分，以便在计算图中的路径中进行传播。</p><p>很幸运，<strong>链式法则</strong>（ChainRule）可以完美地解决这个问题。什么是链式法则？链式法则是关于复合函数的导数的性质，其定义如下：</p><blockquote><p>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。</p></blockquote><p>举个例子，有一个函数定义为 <span class="math inline">\(z = (x +y)^2\)</span>，此时我们可以将它拆解成两个函数，分别是：</p><span class="math display">\[\begin{aligned}z =&amp; t^2\\t =&amp; x + y\end{aligned}\]</span><p>对此，我们可以基于链式法则对复合函数 <spanclass="math inline">\(z\)</span> 关于 <spanclass="math inline">\(x\)</span> 到导数 <spanclass="math inline">\(\frac{\partial z}{\partial x}\)</span>进行求解，如下所示。</p><span class="math display">\[\begin{aligned}\frac{\partial z}{\partial x} = &amp;\frac{\partial z}{\partial t} \frac{\partial t}{\partial x}\\= &amp;2t \cdot 1\\= &amp;2(x + y)\end{aligned}\]</span><p>使用计算图进行拆解，可以得到如下所示的反向传播的计算路径。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-03.png?x-oss-process=image/resize,w_800" /></p><h2 id="运算节点">运算节点</h2><p>在计算图中，节点是决定正向传播和反向传播的关键。下面，我们来看几种典型的运算节点。</p><h3 id="加法节点">加法节点</h3><p>加法节点用于处理加法运算，比如：<span class="math inline">\(z = x +y\)</span>。<span class="math inline">\(z\)</span> 关于 <spanclass="math inline">\(x\)</span> 和 <spanclass="math inline">\(y\)</span> 的导数都是常量 <spanclass="math inline">\(1\)</span>，其正向传播和反向传播的计算图如下所示。反向传播的导数仍然保持“上游传来的梯度”不变。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-14.png?x-oss-process=image/resize,w_800" /></p><h3 id="乘法节点">乘法节点</h3><p>乘法节点用于处理乘法运算，比如：<span class="math inline">\(z = x\times y\)</span>。此时，我们可以分别求出 <spanclass="math inline">\(z\)</span> 关于 <spanclass="math inline">\(x\)</span> 和 <spanclass="math inline">\(y\)</span> 的导数，分别是：</p><span class="math display">\[\begin{aligned}\frac{\partial z}{\partial x} = y\\\frac{\partial z}{\partial y} = x\end{aligned}\]</span><p>乘法节点的正向传播和反向传播如下所示，其中反向传播会将“上游传来的梯度”乘以“将正向传播时的输入替换后的值”。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-04.png?x-oss-process=image/resize,w_800" /></p><h3 id="分支节点">分支节点</h3><p>下图所示，分支节点是有分支的节点，本质上就是相同的值被复制并分叉，其反向传播是上游传来的梯度之和。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-05.png?x-oss-process=image/resize,w_800" /></p><h3 id="repeat-节点">Repeat 节点</h3><p>分支节点有两个分支，Repeat 节点则有 N个分支。与分支节点类似，其反向传播也是通过 N 个梯度的总和求出。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-06.png?x-oss-process=image/resize,w_800" /></p><h3 id="sum-节点">Sum 节点</h3><p>Sum 节点是通用的加法节点。Sum节点的反向传播将上游传来的梯度复制并分配至所有分支。我们可以发现，Sum节点和 Repeat 存在一种逆向关系，即 Sum 节点的正向传播相当于 Repeat节点的反向传播；Sum 节点的反向传播相当于 Repeat 节点的正向传播。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-07.png?x-oss-process=image/resize,w_800" /></p><h3 id="matmul-节点">MatMul 节点</h3><p>MatMul 节点，即矩阵乘积（MatrixMultiply）节点。我们考虑一个矩阵乘法的例子 <span class="math inline">\(y= xW\)</span>。其中，<span class="math inline">\(x\)</span>、<spanclass="math inline">\(W\)</span>、<span class="math inline">\(y\)</span>的形状分别是 <span class="math inline">\(1 \times D\)</span>、<spanclass="math inline">\(D \times H\)</span>、<span class="math inline">\(1\times H\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-08.png?x-oss-process=image/resize,w_800" /></p><p>此时，我们可以通过如下方式求解关于 <spanclass="math inline">\(x\)</span> 的第 <spanclass="math inline">\(i\)</span> 个元素的导数 <spanclass="math inline">\(\frac{\partial L}{\partial x_i}\)</span>。<spanclass="math inline">\(\frac{\partial L}{\partial x_i}\)</span>表示变化程度，当 <span class="math inline">\(x_i\)</span>发生微小变化时，<span class="math inline">\(L\)</span>会有多大程度的变化。如果此时改变 <spanclass="math inline">\(x_i\)</span>，则向量 <spanclass="math inline">\(y\)</span> 的所有元素都会发生变化。由于 <spanclass="math inline">\(y\)</span> 的各个元素发生变化，最终 <spanclass="math inline">\(L\)</span> 也会发生变化。因此，<spanclass="math inline">\(x_i\)</span> 到 <spanclass="math inline">\(L\)</span> 的链式法则路径存在多个，它们的和是<span class="math inline">\(\frac{\partial L}{\partialx_i}\)</span>。</p><span class="math display">\[\begin{aligned}\frac{\partial L}{\partial x_i} = &amp;\sum_j \frac{\partial L}{\partial y_j} \frac{\partial y_j}{\partial x_i}\\= &amp;\sum_j \frac{\partial L}{\partial y_j} W_{ij}\end{aligned}\]</span><p>由上式可知，<span class="math inline">\(\frac{\partial L}{\partialx_i}\)</span> 由向量 <span class="math inline">\(\frac{\partialL}{\partial y}\)</span> 和 <span class="math inline">\(W\)</span> 的第<span class="math inline">\(i\)</span>行向量的内积求得，进而推导得到：</p><span class="math display">\[\begin{aligned}\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} W^T\end{aligned}\]</span><p><span class="math inline">\(\frac{\partial L}{\partial x}\)</span>可由矩阵乘积一次求得，其中 <span class="math inline">\(W^T\)</span>表示矩阵 <span class="math inline">\(W\)</span> 的转置矩阵。</p><p>当我们考虑 mini-batch 处理的情况，即 <spanclass="math inline">\(x\)</span> 中保存了 <spanclass="math inline">\(N\)</span> 份数据。此时，<spanclass="math inline">\(x\)</span>、<spanclass="math inline">\(W\)</span>、<span class="math inline">\(y\)</span>的形状分别是 <span class="math inline">\(N \times D\)</span>、<spanclass="math inline">\(D \times H\)</span>、<span class="math inline">\(N\times H\)</span>，其计算图如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-09.png?x-oss-process=image/resize,w_800" /></p><p>从 <span class="math inline">\(\frac{\partial L}{\partial x}\)</span>的关系式，我们发现矩阵乘积的反向传播与乘法的反向传播类似，同样可以总结出“上游传来的梯度”乘以“将正向传播时的输入替换后的值”。最后，我们进一步通过确认矩阵的形状，可以推导出矩阵乘法的反向传播的数学式，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-10.png?x-oss-process=image/resize,w_800" /></p><h1 id="分层设计">分层设计</h1><p>在 <ahref="https://chuquan.me/2024/07/21/neural-network-introduce/">《初识人工神经网络（1）——基本原理》</a>中我们介绍了一个数字识别的神经网络，其结构如下所示，隐藏层的节点具备两个处理函数，分别是求和函数、激活函数。对此，为了实现分层设计，我们将进一步拆分成仿射层、激活函数层。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-14.png?x-oss-process=image/resize,w_800" /></p><h2 id="affine">Affine</h2><p>求和函数本质上对各个输入进行加权求和，通过矩阵点乘实现。此时，我们再引入一个偏置，用于控制神经元被激活的容易程度。由于神经网络的加权求和运算与加偏置运算，正好对应仿射变换的一次线性变换和一次平移，因此将其称为仿射层，或Affine 层。如下所示是 Affine 计算图的正向传播路径。其中，<spanclass="math inline">\(X\)</span> 表示输入矩阵，<spanclass="math inline">\(W\)</span> 表示权重矩阵，<spanclass="math inline">\(B\)</span> 表示偏置矩阵。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-11.png?x-oss-process=image/resize,w_800" /></p><h3 id="计算图-1">计算图</h3><p>通过上述计算图，我们可以发现 Affine 层是由一个 MatMul节点和一个加法节点组成。由此，我们可以得到其计算图的反向传播路径，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-12.png?x-oss-process=image/resize,w_800" /></p><h3 id="代码实现">代码实现</h3><p>根据计算图，我们可以很容易得到 Affine 层的代码实现，如下所示。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Affine</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, W, b</span>):</span><br><span class="line">        self.W = W </span><br><span class="line">        self.b = b </span><br><span class="line">        self.x = <span class="literal">None</span> </span><br><span class="line">        self.dW = <span class="literal">None</span> </span><br><span class="line">        self.db = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        self.x = x</span><br><span class="line">        out = np.dot(x, self.W) + self.b</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = np.dot(dout, self.W.T) </span><br><span class="line">        self.dW = np.dot(self.x.T, dout) </span><br><span class="line">        self.db = np.<span class="built_in">sum</span>(dout, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure></p><h2 id="sigmoid">Sigmoid</h2><p>Sigmoid 是激活函数的一种，其数学定义如下所示。</p><span class="math display">\[\begin{aligned}y = &amp;\frac{1}{1 + exp(-x)}\\= &amp;\frac{1}{1 + e^{-x}}\end{aligned}\]</span><p>根据其数学定义，我们对其运算步骤进行节点拆解，可以得到如下所示计算图的正向传播路径。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-13.png?x-oss-process=image/resize,w_800" /></p><h3 id="计算图-2">计算图</h3><p>在 sigmoid 计算图中，我们注意到有两个没有介绍过的节点，分别是：<spanclass="math inline">\(exp\)</span> 节和除法节点。</p><p>对于 <span class="math inline">\(exp\)</span> 节点，其数学表示为<span class="math inline">\(y = exp(x)\)</span>，其导数由下式表示。</p><span class="math display">\[\begin{aligned}\frac{\partial y}{\partial x}=&amp; log_e e \cdot e^x\\=&amp; e^x\\=&amp; exp(x)\end{aligned}\]</span><p>对于除法节点，其数学表示为 <span class="math inline">\(y =\frac{1}{x}\)</span>，其导数由下式表示。</p><span class="math display">\[\begin{aligned}\frac{\partial y}{\partial x}=&amp; -\frac{1}{x^2}\\=&amp; -y^2\end{aligned}\]</span><p>然后，我们根据链式法则可以推导出如下所示的反向传播路径。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-15.png?x-oss-process=image/resize,w_800" /></p><p>接下来，我们对 sigmoid 反向输出的导数进行简化，推导如下。</p><span class="math display">\[\begin{aligned}\frac{\partial L}{\partial y} y^2 exp(-x)=&amp; \frac{\partial L}{\partial y} \frac{1}{(1 + exp(-x))^2} exp(-x)\\=&amp; \frac{\partial L}{\partial y} \frac{1}{1 + exp(-x)}\frac{exp(-x)}{1 + exp(-x)} \\=&amp; \frac{\partial L}{\partial y} y (1 - y)\end{aligned}\]</span><p>然后，我们再隐藏计算图过程中节点，合并成一个 sigmoid节点，可以得到如下所示的计算图。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-16.png?x-oss-process=image/resize,w_800" /></p><h3 id="代码实现-1">代码实现</h3><p>根据计算图，我们可以很容易得到 Sigmoid 层的代码实现，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Sigmoid</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.out = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x)) </span><br><span class="line">        self.out = out</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * (<span class="number">1.0</span> - self.out) * self.out</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><h2 id="relu">ReLU</h2><p>ReLU 是另一种常用的激活函数，其数学表达式非常简单，如下所示。</p><span class="math display">\[\begin{aligned}y = \begin{cases}x &amp; (x &gt; 0)\\0 &amp; (x \leq 0)\end{cases}\end{aligned}\]</span><h3 id="计算图-3">计算图</h3><p>根据 ReLU 的数学定义，我们可以求解 <spanclass="math inline">\(y\)</span> 关于 <spanclass="math inline">\(x\)</span> 的导数，如下所示。</p><span class="math display">\[\begin{aligned}\frac{\partial y}{\partial x} = \begin{cases}1 &amp; (x &gt; 0)\\0 &amp; (x \leq 0)\end{cases}\end{aligned}\]</span><p>下图所示为 ReLU 的计算图。当正向传播时的输入 <spanclass="math inline">\(x\)</span> 大于0，则反向传播会将上游的值直接传递至下游；当正向传播时的输入 <spanclass="math inline">\(x\)</span> 小于等于 0，则导数传递停止。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-17.png?x-oss-process=image/resize,w_800" /></p><h3 id="代码实现-2">代码实现</h3><p>根据计算图，我们可以得到 ReLU 层的代码实现，如下所示。其中，变量<code>mask</code> 是由 True/False 构成的 NumPy 数组，它会把正向传播时的输入 <code>x</code> 的元素中小于等于 0的地方保存为 True，其他地方(大于 0 的元素)保存为 False。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Relu</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        self.mask = (x &lt;= <span class="number">0</span>) </span><br><span class="line">        out = x.copy() </span><br><span class="line">        out[self.mask] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>): </span><br><span class="line">        dout[self.mask] = <span class="number">0</span> </span><br><span class="line">        dx = dout</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><h2 id="softmax-with-loss">Softmax-with-Loss</h2><p>对于分类问题，尤其是多元分类问题，一般使用 Softmax函数作为计算输出。Softmax 函数的数学定义如下所示。</p><span class="math display">\[\begin{aligned}y_k = &amp;\frac{exp(a_k)}{\sum\limits_{i=1}\limits^n exp(a_i)}\end{aligned}\]</span><p>其中，<span class="math inline">\(exp(x)\)</span> 表示 <spanclass="math inline">\(e^x\)</span> 的指数函数。如果输出层有 <spanclass="math inline">\(n\)</span> 个神经元，<spanclass="math inline">\(y_k\)</span> 表示第 <spanclass="math inline">\(k\)</span> 个神经元的输出值。Softmax函数中的分子是输入信号 <span class="math inline">\(a_k\)</span>的指数函数，分母是所有输入信号的指数函数之和。Softmax 函数的输出是 0.0到 1.0 之间的实数，且 Softmax 函数的输出值总和为 1。因此，我们将 Softmax函数的输出解释为“概率”。</p><p>神经网络包含 <strong>推理</strong> 和 <strong>学习</strong>两个阶段。在推理阶段通常不使用 Softmax 层，因为 Softmax只是对前一层的数据进行了归一化处理。推理阶段只需要用最后的 Affine层输出的最大值即可。不过，在学习阶段则需要 Softmax层。为了配合监督数据的正确值进行学习，通常会使用 Softmax结合交叉熵误差（Cross Entropy Error）损失函数，构成一个<strong>Softmax-with-Loss</strong> 层。</p><p>上文，我们介绍过交叉熵误差，这里我们再简单回顾一下它的数学定义，如下所示。</p><span class="math display">\[\begin{aligned}L = - \sum\limits_k t_k log y_k\end{aligned}\]</span><h3 id="计算图-4">计算图</h3><p>下图所示是 Softmax-with-Loss层的计算图的整体示意图。计算图内部可以分为 Softmax 和 Cross EntropyError两个层，其各自则是由多个基本运算节点构成。注意，在计算图中，我们将指数之和简写为<span class="math inline">\(S\)</span>，最终的输出计为 <spanclass="math inline">\((y_1, y_2, y_3)\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-18.png?x-oss-process=image/resize,w_800" /></p><p>这里，我们重点看一下反向传播。</p><p>首先是 Cross Entropy Error 层的反向传播，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-19.png?x-oss-process=image/resize,w_800" /></p><p>其主要注意以下几个要点：</p><ul><li>反向传播初始值为 1。因为 <span class="math inline">\(\frac{\partialL}{\partial L} = 1\)</span>。</li><li>乘法节点的反向传播是将正向传播时的输入值进行翻转，乘以上游传来的导数后，传递至下游。</li><li>加法节点的反向传播是将上游传来的导数继续进行传递。</li><li>对数节点中对数的导数是 <span class="math inline">\(\frac{\partialy}{\partial x} =\frac{1}{x}\)</span>，其反向传播则根据链式法则，使用上游传来的导数乘以自身的导数，并将结果传递至下游。</li></ul><p>其次，我们来看 Softmax 层的反向传播，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/deep-learning-layer-20.png?x-oss-process=image/resize,w_800" /></p><p>对于乘法节点，其反向传播将正向传播时的输入值进行翻转，乘以上游传来的导数。其包含两个反向传播分支，具体计算如下所示。</p><span class="math display">\[\begin{aligned}- \frac{t_1}{y_1} exp(a_1) =&amp; - t_1 \frac{S}{exp(a_1)} exp(a_1) = -t_1 S\\- \frac{t_1}{y_1} \frac{1}{S} =&amp; -\frac{t_1}{exp(a_1)}\end{aligned}\]</span><p>接下来是一个 Repeat 节点和除法节点的组合。对于 Repeat节点，其反向传播时会将上游节点的导数进行求和，得到 <spanclass="math inline">\(-S(t_1 + t_2 +t_3)\)</span>；对于除法节点，其反向传播可以进一步得到 <spanclass="math inline">\(\frac{1}{S}(t_1 + t_2 + t_3)\)</span>。由于 <spanclass="math inline">\((t_1, t_2, t_3)\)</span> 采用 one-hot表示法，其和为 1，因此进一步得到反向传播的值为 <spanclass="math inline">\(\frac{1}{S}\)</span>。</p><p>然后是加法节点，其反向传播将上游传来的导数继续进行传递。</p><p>最后是 exp 节点，从计算图看，它其实包含了一个 exp 节点和一个 Repeat节点。对于 Repeat 节点，其反向传播会将上游节点的导数进行求和；对于 exp节点，其导数为 <span class="math inline">\(\frac{\partial y}{\partial x}= exp(x)\)</span>。反向传播推导如下：</p><span class="math display">\[\begin{aligned}\frac{\partial y}{\partial y} = (\frac{1}{S} - \frac{t_1}{exp(a_1)})exp(a_1) = y_1 - t_1\end{aligned}\]</span><h3 id="代码实现-3">代码实现</h3><p>最后，我们根据计算图进行代码实现，结果如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>): </span><br><span class="line">    delta = <span class="number">1e-7</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + delta))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">a</span>):</span><br><span class="line">    c = np.<span class="built_in">max</span>(a)</span><br><span class="line">    exp_a = np.exp(a - c) <span class="comment"># 溢出对策 sum_exp_a = np.sum(exp_a)</span></span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxWithLoss</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.loss = <span class="literal">None</span> <span class="comment"># 损失</span></span><br><span class="line">        self.y = <span class="literal">None</span> <span class="comment"># softmax的输出</span></span><br><span class="line">        self.t = <span class="literal">None</span> <span class="comment"># 监督数据(one-hot vector)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t</span>): self.t = t</span><br><span class="line">        self.y = softmax(x)</span><br><span class="line">        self.loss = cross_entropy_error(self.y, self.t)</span><br><span class="line">        <span class="keyword">return</span> self.loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span><br><span class="line">        batch_size = self.t.shape[<span class="number">0</span>]</span><br><span class="line">        dx = (self.y - self.t) / batch_size</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><h1 id="总结">总结</h1><p>在实践中，分层设计有利于快速调整和实现各种各样的神经网络。我们经常听说的深度学习，本质上就是一个神经网络层级比较多的结构。</p><p>为了介绍神经网络中的分层设计原理，本文以计算图作为出发点，作为分析每个分层正向传播和反向传播的依据。这里，我们还介绍了一个用于反向传播的关键法则——链式法则。最后，我们介绍了神经网络中几种常见的分层，如：Affine、Sigmoid、ReLU、Softmax-with-Loss等。</p><p>本文，我们大致了解了神经网络的分层设计，这种思想是研究复杂神经网络的基础。后续有时间，我们将继续探索神经网络的各种应用，敬请期待吧！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在深度学习中，为了便于组合任意结构、任意层次的神经网络，通常会对神经网络进行分层设计，这也是一种模块化的设计思想。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://chuquan.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="计算图" scheme="http://chuquan.me/tags/%E8%AE%A1%E7%AE%97%E5%9B%BE/"/>
    
    <category term="Sigmoid" scheme="http://chuquan.me/tags/Sigmoid/"/>
    
    <category term="Affine" scheme="http://chuquan.me/tags/Affine/"/>
    
    <category term="Softmax" scheme="http://chuquan.me/tags/Softmax/"/>
    
    <category term="Softmax-with-Loss" scheme="http://chuquan.me/tags/Softmax-with-Loss/"/>
    
    <category term="ReLU" scheme="http://chuquan.me/tags/ReLU/"/>
    
  </entry>
  
  <entry>
    <title>初识人工神经网络（2）——代码实现</title>
    <link href="http://chuquan.me/2024/07/31/neural-network-implement/"/>
    <id>http://chuquan.me/2024/07/31/neural-network-implement/</id>
    <published>2024-07-31T15:20:15.000Z</published>
    <updated>2024-07-31T15:37:52.196Z</updated>
    
    <content type="html"><![CDATA[<p>前一篇 <ahref="https://chuquan.me/2024/07/21/neural-network-introduce">文章</a>我们介绍了人工神经网络的基本原理，本文我们将使用 Python来实现一个简易的神经网络，可用于识别手写数字，从而加深对于神经网络的理解。</p><span id="more"></span><p>本文实现的完整代码 <ahref="https://github.com/baochuquan/implementing-neural-network">传送门</a>。</p><h1 id="神经网络实现">神经网络实现</h1><p>根据我们的理解，神经网络应该至少包含三个部分：</p><ul><li><strong>初始化</strong>：初始化输入层节点、隐藏层节点、输出层节点的数量。</li><li><strong>训练</strong>：通过特定的训练样本，优化连接权重。</li><li><strong>查询</strong>：给定输入，计算输出。</li></ul><p>对此，我们定义一个 <code>NeuralNetwork</code>类来表示神经网络，其内部包含三个函数分别对应初始化、训练、查询，如下所示。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># neural network class definition </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span> :</span><br><span class="line">    <span class="comment"># initialise the neural network </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>() :</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># train the neural network </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>() :</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># query the neural network </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">query</span>() :</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p><p>接下来，我们依次来实现神经网络的各个部分。</p><h2 id="初始化网络">初始化网络</h2><h3 id="网络结构">网络结构</h3><p>首先，我们定义神经网络的基本结构，其包含三个层：输入层、隐藏层、输出层。此外，我们还需要定义学习率，代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialise the neural network</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"> self , inputnodes, hiddennodes, outputnodes, learningrate </span>) :</span><br><span class="line">    <span class="comment"># set number of nodes in each input, hidden, output layer</span></span><br><span class="line">    self.inodes = inputnodes</span><br><span class="line">    self.hnodes = hiddennodes</span><br><span class="line">    self.onodes = outputnodes</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># learning rate</span></span><br><span class="line">    self.lr = learningrate</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="连接权重">连接权重</h3><p>其次，定义神经网络的核心参数——连接权重。这里涉及两部分连接权重，分别是：</p><ul><li>输入层与隐藏层之间的连接权重矩阵：<spanclass="math inline">\(W_{input\_hidden}\)</span>，其大小为 <spanclass="math inline">\(hidden\_nodes \times input\_nodes\)</span>。</li><li>隐藏层与输出层之间的连接权重矩阵：<spanclass="math inline">\(W_{hidden\_output}\)</span>，其大小为 <spanclass="math inline">\(hidden\_nodes \times output\_nodes\)</span>。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-15.png?x-oss-process=image/resize,w_800" /></p><p>在 Python 中，我们使用经典的数学库 <code>numpy</code>来实现矩阵的表示和运算，通过如下的方式我们可以定义一个 <spanclass="math inline">\(rows \times columns\)</span> 的数组，数组元素是 0~ 1 的随机值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.rand(rows, columns)</span><br></pre></td></tr></table></figure><p>在前一篇文章中，我们介绍过权重的范围在 <spanclass="math inline">\([-1. 1]\)</span>之间。这里我们可以通过上述方式生成随机数之后再减去0.5，从而使数组元素变成 <span class="math inline">\([-0.5, 0.5]\)</span>之间的随机值，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.wih = (numpy.random.rand(self.hnodes, self.inodes) - <span class="number">0.5</span>)</span><br><span class="line">self.who = (numpy.random.rand(self.onodes, self.hnodes) - <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p>我们之前还提到过一种优化的权重初始化方案：<strong>在一个节点传入连接数量平方根倒数的范围内进行正态分布采样</strong>，即权重范围是<span class="math inline">\([1/\sqrt{传入连接数},-1/\sqrt{传入连接数}]\)</span>，相关代码实现如下所示。其中<code>numpy.random.normal()</code>函数用于实现正态分布采样，传入的三个参数分别是：正态分布值的中心、标准方差、数组大小。<code>pow(self.hnodes, -0.5)</code>相当于节点数量的 -0.5 次方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.wih = numpy.random.normal( <span class="number">0.0</span> , <span class="built_in">pow</span>(self.hnodes, -<span class="number">0.5</span>) , (self.hnodes, self.inodes) )</span><br><span class="line">self.who = numpy.random.normal( <span class="number">0.0</span> , <span class="built_in">pow</span>(self.onodes, -<span class="number">0.5</span>) , (self.onodes, self.hnodes) )</span><br></pre></td></tr></table></figure><p>最后我们结合网络结构和连接权重，可以得到完整的初始化代码，如下所示。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialise the neural network</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"> self , inputnodes, hiddennodes, outputnodes, learningrate </span>) :</span><br><span class="line">    <span class="comment"># set number of nodes in each input, hidden, output layer</span></span><br><span class="line">    self.inodes = inputnodes</span><br><span class="line">    self.hnodes = hiddennodes</span><br><span class="line">    self.onodes = outputnodes</span><br><span class="line">    </span><br><span class="line">    self.wih = numpy.random.normal ( <span class="number">0.0</span> , <span class="built_in">pow</span>(self.hnodes, -<span class="number">0.5</span>) , (self.hnodes, self.inodes) )</span><br><span class="line">    self.who = numpy.random.normal ( <span class="number">0.0</span> , <span class="built_in">pow</span>(self.onodes, -<span class="number">0.5</span>) , (self.onodes, self.hnodes) )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># learning rate</span></span><br><span class="line">    self.lr = learningrate</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p><h2 id="查询网络">查询网络</h2><p>查询网络本质上就是信号转换的过程。我们知道每一个神经元都会对输入信号进行两次处理，分别是求和函数和激活函数，对应的矩阵表达式如下所示。</p><span class="math display">\[\begin{aligned}X_{hidden} = &amp; W_{input\_hidden} \cdot I\\\\O_{hidden} = &amp; sigmoid(X_{hidden})\end{aligned}\]</span><p>对于求和函数，我们可以通过 numpy库中的矩阵点乘函数来实现，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden_inputs = numpy.dot(self.wih, inputs)</span><br></pre></td></tr></table></figure><p>对于激活函数，我们使用 <span class="math inline">\(sigmoid\)</span>激活函数，它的表达式是 <spanclass="math inline">\(\frac{1}{1+e^{-x}}\)</span>。对此，SciPy Python库中的 <code>expit()</code> 函数实现了 <spanclass="math inline">\(sigmoid\)</span>函数。我们可以在初始化方法中加入一下这段代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.activation_function = <span class="keyword">lambda</span> x: scipy.special.expit(x)</span><br></pre></td></tr></table></figure><p>然后，我们将上述求和函数结果输入至激活函数中，即可得到隐藏层的输出，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden_outputs = self.activation_function(hidden_inputs)</span><br></pre></td></tr></table></figure><p>输出层的信号转换本质上与隐藏层一样，因此我们可以添加两行类似的代码来对输出层进行处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">final_inputs = numpy.dot(self.who, hidden_outputs)</span><br><span class="line">final_outputs = self.activation_function(final_inputs)</span><br></pre></td></tr></table></figure><p>最后我们得到完整的查询网络代码如下所示。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># query the neural network</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">query</span>(<span class="params">self, inputs_list</span>) :</span><br><span class="line">    <span class="comment"># convert inputs list to 2d array</span></span><br><span class="line">    inputs = numpy.array(inputs_list, ndmin=<span class="number">2</span>).T</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate signals into hidden layer</span></span><br><span class="line">    hidden_inputs = numpy.dot(self.wih, inputs)</span><br><span class="line">    <span class="comment"># calculate the signals emerging from hidden layer</span></span><br><span class="line">    hidden_outputs = self.activation_function(hidden_inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate signals into final output layer</span></span><br><span class="line">    final_inputs = numpy.dot(self.who, hidden_outputs)</span><br><span class="line">    <span class="comment"># calculate the signals emerging from final output layer</span></span><br><span class="line">    final_outputs = self.activation_function(final_inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br></pre></td></tr></table></figure></p><h2 id="训练网络">训练网络</h2><p>训练网络主要包含两部分：</p><ul><li>正向的信号转换</li><li>反向的权重更新</li></ul><h3 id="信号转换">信号转换</h3><p>信号转换的过程与上述查询网络一致，因此我们可以直接照搬相关代码，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, inputs_list, targets_list</span>):</span><br><span class="line">    <span class="comment"># convert inputs list to 2d array</span></span><br><span class="line">    inputs = numpy.array(inputs_list, ndmin=<span class="number">2</span>).T</span><br><span class="line">    targets = numpy.array(targets_list, ndmin=<span class="number">2</span>).T</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate signals into hidden layer</span></span><br><span class="line">    hidden_inputs = numpy.dot(self.wih, inputs)</span><br><span class="line">    <span class="comment"># calculate the signals emerging from hidden layer</span></span><br><span class="line">    hidden_outputs = self.activation_function(hidden_inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate signals into final output layer</span></span><br><span class="line">    final_inputs = numpy.dot(self.who, hidden_outputs)</span><br><span class="line">    <span class="comment"># calculate the signals emerging from final output layer</span></span><br><span class="line">    final_outputs = self.activation_function(final_inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="权重更新">权重更新</h3><p>对于一个三层神经网络，我们只需要更新两部分权重，分别是：</p><ul><li>隐藏层与输出层的权重更新</li><li>输入层与隐藏层的权重更新</li></ul><p>权重更新是基于误差实现的，因此我们首先要计算误差，对于输出层的误差，计算样本的预期目标输出值与实际计算输出值的差即可；对于隐藏层的误差，我们在上一篇文章中进行了公式推导，如下所示。</p><span class="math display">\[\begin{aligned}error_{hidden} = W^T_{hidden\_output} \cdot error_{output}\end{aligned}\]</span><p>由此，我们可以得到如下所示的误差计算代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># error is the (target - actual)</span></span><br><span class="line">output_errors = targets - final_outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># hidden layer error is the output_errors, split by weights, recombined at hidden nodes</span></span><br><span class="line">hidden_errors = numpy.dot(self.who.T, output_errors)</span><br></pre></td></tr></table></figure><p>然后，我们根据上一篇文章中推导的权重更新公式来实现相关代码，其公式如下所示。其中<span class="math inline">\(\alpha\)</span>是学习率。基于此，我们可以分别实现隐藏层与输出层的权重更新、输入层与隐藏层的权重更新。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-42.png?x-oss-process=image/resize,w_800" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># update the weights for the links between the hidden and output layers</span></span><br><span class="line">self.who += self.lr * numpy.dot(( output_errors * final_outputs * (<span class="number">1.0</span> - final_outputs)), numpy.transpose(hidden_outputs))</span><br><span class="line"></span><br><span class="line"><span class="comment"># update the weights for the links between the input and hidden layers</span></span><br><span class="line">self.wih += self.lr * numpy.dot(( hidden_errors * hidden_outputs * (<span class="number">1.0</span> - hidden_outputs)), numpy.transpose(inputs))</span><br></pre></td></tr></table></figure><p>最后，我们可以得到完整的训练网络的相关代码，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, inputs_list, targets_list</span>):</span><br><span class="line">    <span class="comment"># convert inputs list to 2d array</span></span><br><span class="line">    inputs = numpy.array(inputs_list, ndmin=<span class="number">2</span>).T</span><br><span class="line">    targets = numpy.array(targets_list, ndmin=<span class="number">2</span>).T</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate signals into hidden layer</span></span><br><span class="line">    hidden_inputs = numpy.dot(self.wih, inputs)</span><br><span class="line">    <span class="comment"># calculate the signals emerging from hidden layer</span></span><br><span class="line">    hidden_outputs = self.activation_function(hidden_inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculate signals into final output layer</span></span><br><span class="line">    final_inputs = numpy.dot(self.who, hidden_outputs)</span><br><span class="line">    <span class="comment"># calculate the signals emerging from final output layer</span></span><br><span class="line">    final_outputs = self.activation_function(final_inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output layer error is the (target - actual) </span></span><br><span class="line">    output_errors = targets - final_outputs</span><br><span class="line">    <span class="comment"># hidden layer error is the output_errors, split by weights, recombined at hidden nodes</span></span><br><span class="line">    hidden_errors = numpy.dot(self.who.T, output_errors)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update the weights for the links between the hidden and output layers</span></span><br><span class="line">    self.who += self.lr * numpy.dot((output_errors * final_outputs * (<span class="number">1.0</span> - final_outputs)), numpy.transpose(hidden_outputs))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update the weights for the links between the input and hidden layers</span></span><br><span class="line">    self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (<span class="number">1.0</span> - hidden_outputs)), numpy.transpose(inputs))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h1 id="神经网络测试">神经网络测试</h1><p>神经网络适合用于解决不具有固定模式或计算步骤的问题，比如：图像识别、语音识别等。这里我们尝试让神经网络解决一个类似的问题——手写数字识别。下图所示是一个手写数字的示例，我们可能会对于这个数字是4 还是 9产生分歧。此时，神经网络就可以作为判断的辅助工具，我们的目的也是如此。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-imp-01.png?x-oss-process=image/resize,w_800" /></p><h2 id="训练数据">训练数据</h2><p>经典的手写数字数据库 <ahref="https://yann.lecun.com/exdb/mnist/">MNIST</a>为我们提供了训练和测试的样本数据，由于 MNIST数据库的格式不易使用，我们使用别人构建的相对简单的 CSV 文件：</p><ul><li>训练集 http://www.pjreddie.com/media/files/mnist_train.csv</li><li>测试集 http://www.pjreddie.com/media/files/mnist_test.csv</li></ul><p><code>mnist_train.csv</code> 和 <code>mnist_test.csv</code>中的每一行代表一个样本数据，每个样本数据由 785个数字组成，由逗号进行分隔。其中第 1 个数字是样本的预期目标值；其余 784个数字素是样本的输入数据，本质上是一个打平的 28 x 28的二维矩阵，每个数字表示一个颜色值，范围在 <spanclass="math inline">\([0, 255]\)</span>之间。如下所示，是一个样本数据的示例。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,18,18,18,126,136,175,26,166,255,247,127,0,0,0,0,0,0,0,0,0,0,0,0,30,36,94,154,170,253,253,253,253,253,225,172,253,242,195,64,0,0,0,0,0,0,0,0,0,0,0,49,238,253,253,253,253,253,253,253,253,251,93,82,82,56,39,0,0,0,0,0,0,0,0,0,0,0,0,18,219,253,253,253,253,253,198,182,247,241,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,80,156,107,253,253,205,11,0,43,154,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,154,253,90,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,139,253,190,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,190,253,70,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,35,241,225,160,108,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,240,253,253,119,25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,186,253,253,150,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,93,252,253,187,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,249,253,249,64,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,130,183,253,253,207,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,39,148,229,253,253,253,250,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,114,221,253,253,253,253,201,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,66,213,253,253,253,253,198,81,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,171,219,253,253,253,253,195,80,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,55,172,226,253,253,253,253,244,133,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,136,253,253,253,212,135,132,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0</span><br></pre></td></tr></table></figure><p>将此样本数据转换成图像，可以得到如下所示的手写数字图像，通过肉眼判断手写数字与预期目标值5 是相符合的。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-imp-02.png?x-oss-process=image/resize,w_800" /></p><h2 id="构建网络">构建网络</h2><p>对于输入层节点，由于样本的输入数据是 28 x 28 =784，因此在初始化网络时，传入的 <code>input_nodes</code> 也应该是784。</p><p>对于输出层节点，由于神经网络是对图像进行分类，在此情况下，分类的结果应该是[0, 9] 共 10 个数字中的任意一个，因此在初始化网络时，传入的<code>output_nodes</code> 也应该是 10。</p><p>对于隐藏层节点，这也是一个可自由调节的参数。由于隐藏层是为了提取输入数据的特征和模式，理论上可以比输入更简洁，因此我们可以考虑使用一个比输入节点数更小的数来表示，比如：100。同样，学习率也是一个可自由调节的参数，这里我们暂停为0.3。</p><p>如下所示，是网络构建的相关代码实现。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># number of input, hidden and output nodes</span></span><br><span class="line">input_nodes = <span class="number">784</span></span><br><span class="line">hidden_nodes = <span class="number">100</span></span><br><span class="line">output_nodes = <span class="number">10</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># learning rate is 0.3</span></span><br><span class="line">learning_rate = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create instance of neural network</span></span><br><span class="line">n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)</span><br></pre></td></tr></table></figure></p><h2 id="样本数据预处理">样本数据预处理</h2><p>在上一篇文章中我们提到输入值的范围应该是 <spanclass="math inline">\((0, 1]\)</span>，而当前的样本输入值的范围是 <spanclass="math inline">\([0,255]\)</span>。因此我们需要进行样本数据处理，可以考虑将其缩小至 <spanclass="math inline">\([0.01, 1.0]\)</span>的范围之中，相关的处理代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = (numpy.asfarray(all_values[<span class="number">1</span>:]) / <span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span></span><br></pre></td></tr></table></figure><p>除了样本输出值，我们还需要处理样本目标值。当前输出层具有 10个节点，输出值最大的节点的索引就是手写数字的识别结果，如下所示。对此，理想情况是定义一个数组，只有一个元素为1，其余元素为 0。然而激活函数无法输出 0 和 1，此时如果将目标值设为 0 或1，将会导致神经网络反馈过大的权重。因此我们要对这些目标值进行微调，使用0.01 和 0.99 来代替 0 和 1，实现代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#output nodes is 10 (example)</span></span><br><span class="line">onodes = <span class="number">10</span></span><br><span class="line">targets = numpy.zeros(onodes) + <span class="number">0.01</span></span><br><span class="line">targets[<span class="built_in">int</span>(all_values[<span class="number">0</span>])] = <span class="number">0.99</span></span><br></pre></td></tr></table></figure><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-imp-03.png?x-oss-process=image/resize,w_800" /></p><p>如下所示是输入数据预处理的相关代码实现。我们首先打开样本数据的<code>csv</code>文件，然后循环依次遍历每一个样本数据，拆分成目标值和输入值，最后对它们进行预处理，从而匹配神经网络的值的约束。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the mnist training data CSV file into a list </span></span><br><span class="line">training_data_file = <span class="built_in">open</span>(<span class="string">&quot;./mnist_train.csv&quot;</span>, <span class="string">&#x27;r&#x27;</span>) </span><br><span class="line">training_data_list = training_data_file.readlines() </span><br><span class="line">training_data_file.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># go through all records in the training data set</span></span><br><span class="line"><span class="keyword">for</span> record <span class="keyword">in</span> training_data_list:</span><br><span class="line">    <span class="comment"># split the record by the &#x27;,&#x27; commas</span></span><br><span class="line">    all_values = record.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="comment"># scale and shift the inputs</span></span><br><span class="line">    inputs = (numpy.asfarray(all_values[<span class="number">1</span>:]) / <span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span> </span><br><span class="line">    <span class="comment"># create the target output values (all 0.01, except the desired label which is 0.99)</span></span><br><span class="line">    targets = numpy.zeros(output_nodes) + <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># all_values[0] is the target label for this record</span></span><br><span class="line">    targets[<span class="built_in">int</span>(all_values[<span class="number">0</span>])] = <span class="number">0.99</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># train the neural network</span></span><br><span class="line">    n.train(inputs, targets)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="训练与测试">训练与测试</h2><p>我们使用 <code>mnist_train.csv</code> 完成了训练之后，可以使用<code>mnist_test.csv</code>来进行测试。我们只要再读取测试数据，输入至已训练的神经网络中查询结果，并比对输出值与目标值。为了有一个衡量标准，我们对每次样本测试进行打分，查询结果正确+1，最后计算正确率，相关代码实现如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the mnist test data CSV file into a list</span></span><br><span class="line">test_data_file = <span class="built_in">open</span>(<span class="string">&quot;./mnist_test.csv&quot;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">test_data_list = test_data_file.readlines()</span><br><span class="line">test_data_file.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># test the neural network</span></span><br><span class="line"><span class="comment"># scorecard for how well the network performs, initially empty </span></span><br><span class="line">scorecard = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># go through all the records in the test data set</span></span><br><span class="line"><span class="keyword">for</span> record <span class="keyword">in</span> test_data_list:</span><br><span class="line">    <span class="comment"># split the record by the &#x27;,&#x27; commas</span></span><br><span class="line">    all_values = record.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="comment"># correct answer is first value</span></span><br><span class="line">    correct_label = <span class="built_in">int</span>(all_values[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># scale and shift the inputs</span></span><br><span class="line">    inputs = (numpy.asfarray(all_values[<span class="number">1</span>:]) / <span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span></span><br><span class="line">     <span class="comment"># query the network</span></span><br><span class="line">    outputs = n.query(inputs)</span><br><span class="line">    <span class="comment"># the index of the highest value corresponds to the label </span></span><br><span class="line">    label = numpy.argmax(outputs)</span><br><span class="line">    <span class="comment"># append correct or incorrect to list</span></span><br><span class="line">    <span class="keyword">if</span> (label == correct_label):</span><br><span class="line">        <span class="comment"># network&#x27;s answer matches correct answer, add 1 to scorecard</span></span><br><span class="line">        scorecard.append(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:<span class="comment"># network&#x27;s answer doesn&#x27;t match correct answer, add 0 to scorecard</span></span><br><span class="line">        scorecard.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the performance score, the fraction of correct answers</span></span><br><span class="line">scorecard_array = numpy.asarray(scorecard)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;performance = &quot;</span>, scorecard_array.<span class="built_in">sum</span>() / scorecard_array.size)</span><br></pre></td></tr></table></figure><h1 id="神经网络优化">神经网络优化</h1><p>至此，我们实现了一个手写数字识别的神经网络，并完成了评测。虽然评测得分还不错，正确率超过95%，但是它还可以进一步优化。下面我们简单介绍几种优化思路。</p><h2 id="调整学习率">调整学习率</h2><p>调整学习率是最直观的一种优化思路。学习率过小会导致学习反馈不足，学习率过大会导致学习反馈震荡。下图所示是我们所实现神经网络的性能评分与与学习率的关系曲线。整体而言，我们应该不断调整，选择一个适中的学习率。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-imp-04.png?x-oss-process=image/resize,w_800" /></p><h2 id="重复训练">重复训练</h2><p>重复训练也是一种经典的优化思路。我们将样本训练一次称为一个<strong>世代</strong>（Epoch）。我们可以在样本训练的代码的外层再嵌套一层遍历，尝试进行多个世代的训练，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train the neural network</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># epochs is the number of times the training data set is used for training</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> training_data_list:</span><br><span class="line">        <span class="comment"># split the record by the &#x27;, commas</span></span><br><span class="line">        all_values = record.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="comment"># scale and shift the inputs</span></span><br><span class="line">        inputs = (numpy.asfarray(all_values[<span class="number">1</span>:]) / <span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span></span><br><span class="line">        <span class="comment"># create the target output values (all 0.01, except the desired label which is 0.99)</span></span><br><span class="line">        targets = numpy.zeros(output_nodes) + <span class="number">0.01</span></span><br><span class="line">        <span class="comment"># all_values[0] is the target label for this record</span></span><br><span class="line">        targets[<span class="built_in">int</span>(all_values[<span class="number">0</span>])] = <span class="number">0.99</span></span><br><span class="line">        n.train(inputs, targets)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>与学习率类似，过少的训练会导致学习反馈不足，过多的训练会导致网络过渡拟合训练数据。下图所示是性能评分与世代数的关系曲线，我们同样也需要进行不断调整世代数，寻找一个最佳值。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-imp-05.png?x-oss-process=image/resize,w_800" /></p><h2 id="调整网络结构">调整网络结构</h2><p>调整网络结构也是一种优化思路。我们可以考虑调整隐藏层的节点数量。如果节点数太少的话，会导致节点无法承载过多的特征，从而表现不佳。下图所示是性能评测与隐藏层节点数之间的关系曲线，很显然节点越多，性能表现越好。当然这也是有代价的，节点数越多，神经网络的计算量就越大。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-imp-6.png?x-oss-process=image/resize,w_800" /></p><h1 id="总结">总结</h1><p>本文基于神经网络的基本原理，使用 Python依次实现了神经网络的初始化、训练、查询等部分。然后，我们使用 MNIST的样本数据和测试数据依次来对神经网络进行训练和测试。最后，我们提了三种神经网络性能优化的思路，包括：学习率、训练量、网络结构等。</p><p>后续有时间，我们将再来深入学习一下机器学习的其他相关技术。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前一篇 &lt;a
href=&quot;https://chuquan.me/2024/07/21/neural-network-introduce&quot;&gt;文章&lt;/a&gt;
我们介绍了人工神经网络的基本原理，本文我们将使用 Python
来实现一个简易的神经网络，可用于识别手写数字，从而加深对于神经网络的理解。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://chuquan.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="epoch" scheme="http://chuquan.me/tags/epoch/"/>
    
    <category term="MNIST" scheme="http://chuquan.me/tags/MNIST/"/>
    
    <category term="手写数字识别" scheme="http://chuquan.me/tags/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>初识人工神经网络（1）——基本原理</title>
    <link href="http://chuquan.me/2024/07/21/neural-network-introduce/"/>
    <id>http://chuquan.me/2024/07/21/neural-network-introduce/</id>
    <published>2024-07-21T05:30:55.000Z</published>
    <updated>2024-07-31T15:20:26.250Z</updated>
    
    <content type="html"><![CDATA[<p>最近看了《Python神经网络编程》一书之后，对于神经网络的基本原理有了一个初步的理解，于是产出此篇文章作为系统性的梳理和总结。</p><span id="more"></span><h1 id="概述">概述</h1><p>计算机设计的初衷是为了解决大量的数学运算，因此适用于解决具有固定模式或计算步骤的问题。对于不具备固定模式或计算步骤的问题，比如图像识别、语音识别等，传统的计算机程序很难有效地予以解决。</p><p>人工智能的重要研究方向之一就是通过计算机来解决这类相对困难的问题。机器学习便是人工智能领域中的一个重要分支，而人工神经网络则是机器学习中一种被广泛使用的算法。</p><h1 id="分类器">分类器</h1><p>为了能够理解神经网络的核心思想，我们先来介绍一个分类器的例子。</p><p>假如花园中有两种虫子：毛虫细而长，瓢虫宽而短。我们希望设计一个分类器，当给定一个虫子的长度和宽度，分类器能够自动进行分类。</p><h2 id="设计分析">设计分析</h2><p>我们考虑对虫子的长度和宽度进行分析，绘制一个二维坐标系，可以发现两种虫子是存在一定的聚类特征的，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-01.png?x-oss-process=image/resize,w_800" /></p><p>因此，分类器的设计目标就是通过数据训练进行学习，从而找到一条分界线，将两种类型的虫子进行有效分类。如下所示，在训练阶段，分界线会不断地进行修正，最终到达一个相对正确的位置。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-02.png?x-oss-process=image/resize,w_800" /></p><p>分类器最终结构如下所示，它有一个输入和一个输出，通过分类器实现内部分类逻辑，最终输出分类结果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-11.png?x-oss-process=image/resize,w_800" /></p><h2 id="样本训练">样本训练</h2><p>那么分类器该如何通过数据训练来调整分界线的斜率呢？首先，我们需要定义一个用于表示分界线的函数（或称测试函数），如下所示。</p><span class="math display">\[\begin{aligned}y = Ax\end{aligned}\]</span><p>然后，我们随机初始化 <span class="math inline">\(A\)</span>的值，假设初始化值为 <code>0.25</code>，那么将得到如下所示的直线 <spanclass="math inline">\(y = 0.25x\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-03.png?x-oss-process=image/resize,w_800" /></p><p>接下来，我们开始输入训练样本，如下所示。当输入两个虫子的样本数据后，我们发现分界线并没有正确对虫子进行有效分类。此时，我们需要对斜率进行调整，这也是训练的核心目标。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-04.png?x-oss-process=image/resize,w_800" /></p><p>我们观察第一个训练样本数据：宽度 3.0，长度 1.0，瓢虫。我们将 <spanclass="math inline">\(x = 3.0\)</span> 代入函数 <spanclass="math inline">\(y = Ax\)</span>，得到 <spanclass="math inline">\(y = 0.25 * 3.0 =0.75\)</span>。然而，样本数据告诉我们 <span class="math inline">\(y =1.0\)</span>，此时我们看到了误差的存在。值得注意的是，分界线是为了对实现分类，我们需要让<span class="math inline">\(x\)</span> 代入函数后得到的 <spanclass="math inline">\(y\)</span> 值大于 <spanclass="math inline">\(1.0\)</span>。为了避免调整过大，我们将 <spanclass="math inline">\(y\)</span> 的目标值设置为 <spanclass="math inline">\(1.1\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-05.png?x-oss-process=image/resize,w_800" /></p><p>由此，我们计算输出值和目标值之间的误差，基于此计算出调整的斜率差值。下图显示了初始分界线和一次训练后的分界线。</p><span class="math display">\[\begin{aligned}实际值: &amp; y = Ax = 0.25 * 3.0 = 0.75\\目标值: &amp; t = (A + \Delta A)x = 1.1\\误差值: &amp; E = t - y = (\Delta A)x = 1.1 - 0.75 = 0.35\\斜率差值: &amp; \Delta A = E / x = 0.35 / 3.0 = 0.1167\\斜率修正值: &amp; (A + \Delta A) = 0.25 + 0.1167 = 0.3667\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-06.png?x-oss-process=image/resize,w_800" /></p><p>然后，我们再观察第二个训练样本数据：宽度 1.0，长度3.0，毛虫。采用类似的方法，我们可以计算出调整后的斜率为 <spanclass="math inline">\(2.9\)</span>。下图显示了初始分界线和两次训练后的分界线。</p><span class="math display">\[\begin{aligned}实际值: &amp; y = Ax = 0.3667 * 1.0 = 0.3667\\目标值: &amp; t = (A + \Delta A)x = 2.9\\误差值: &amp; E = t - y = (\Delta A)x = 2.9 - 0.3667 = 2.5333\\斜率差值: &amp; \Delta A = E / x = 2.5333 / 1.0 = 2.5333\\斜率修正值: &amp; (A + \Delta A) = 0.3667 + 2.5333 = 2.9\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-07.png?x-oss-process=image/resize,w_800" /></p><h2 id="适度改进">适度改进</h2><p>如果我们仔细观测两次训练，会发现最终改进的直线与最后一次训练样本非常匹配。这种方式实际上抛弃了所有先前训练样本的学习结果，只对最后一次训练样本进行了学习。</p><p>那么如何解决这个问题呢？一个重要的思路就是<strong>适度改进（Moderate）</strong>。</p><p>我们可以在改进公式中增加一个调节系数 <spanclass="math inline">\(L\)</span> ，也称为 <strong>学习率（LearningRate）</strong>，如下所示。</p><span class="math display">\[\begin{aligned}\Delta A = L (E / x)\end{aligned}\]</span><p>基于新的调整公式，设定学习率 <span class="math inline">\(L =0.5\)</span>，我们再来计算一下斜率的改进过程。</p><span class="math display">\[\begin{aligned}第一次训练: &amp;\\实际值: &amp; y = Ax = 0.25 * 3.0 = 0.75\\目标值: &amp; t = (A + \Delta A)x = 1.1\\误差值: &amp; E = t - y = (\Delta A)x = 1.1 - 0.75 = 0.35\\斜率差值: &amp; \Delta A = L (E / x) = 0.5 * 0.35 / 3.0 = 0.0583\\斜率修正值: &amp; (A + \Delta A) = 0.25 + 0.0583 = 0.3083\\\\第二次训练: &amp;\\实际值: &amp; y = Ax = 0.3083 * 1.0 = 0.3083\\目标值: &amp; t = (A + \Delta A)x = 2.9\\误差值: &amp; E = t - y = (\Delta A)x = 2.9 - 0.3083 = 2.5917\\斜率差值: &amp; \Delta A = L (E / x) = 0.5 * 2.5917 / 1.0 = 1.2958\\斜率修正值: &amp; (A + \Delta A) = 0.3083 + 1.2958 = 1.6042\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-08.png?x-oss-process=image/resize,w_800" /></p><h2 id="分类器组合">分类器组合</h2><p>上述，我们介绍了单一分类器通过训练样本进行学习调整相关参数，最终可用于解决特定问题。</p><p>然而，现实中很多问题并不是一个分类器能够解决的，比如：如何在网格节点中输出逻辑异或（XOR）的值？</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-09.png?x-oss-process=image/resize,w_800" /></p><p>此时，我们无论如何都无法通过一条分界线来正确进行分类。于是，我们开始考虑采用多个分类器进行组合，共同完成对复杂问题的求解，这就是神经网络的基本思想。</p><h1 id="神经网络">神经网络</h1><p>人工神经网络（Artificial NeuralNetwork，ANN，简称神经网络），其设计思想借鉴了动物大脑的生物神经网络，构建了一套类似神经元互连的分层组织结构。</p><h2 id="神经元">神经元</h2><p>无论是人工神经网路，还是生物神经网络，神经元都是其中的基本组成单元，两种神经元的结构也基本差不多。</p><h3 id="生物神经元">生物神经元</h3><p>如下图所示，生物神经元主要包含三部分：</p><ul><li>树突：用于接收外部电信号</li><li>轴突：用于传导电信号</li><li>突触：用于将电信号传递至其他神经元或细胞</li></ul><p>此外，神经元还会通过阈值（threshold）抑制输入，直到电信号超出阈值，才会触发输入。因为神经元不希望传递各种微小的噪音信号，而只传递有意识的明显信号。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-13.png?x-oss-process=image/resize,w_800" /></p><h3 id="人工神经元">人工神经元</h3><p>如下图所示，人工神经元主要包含三个部分：</p><ul><li>输入：类似于树突，可以包含一个或多个输入端，用于接收外部信号</li><li>输出：类似于轴突和突触，用于传递和输出信号</li><li>节点：类似于细胞核，用于处理信号。本质上是两个函数，分别是：<ul><li>求和函数：对所有输入进行求和</li><li>激活函数：或称阈值函数，用于过滤噪音信号，类似于生物神经元中抑制噪音电信号。</li></ul></li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-12.png?x-oss-process=image/resize,w_800" /></p><p>对于激活函数，一个简单的阶跃函数即可实现类似的效果。但是为了更接近自然的效果，这里采用一种平滑且经典的Sigmoid 函数（简称 S函数）作为激活函数，其函数表达式如下所示。除此之外，神经网络中常用的激活函数还有很多，比如：双曲正切函数（HyperbolicTangent）、ReLU函数（Rectified Linear Unit）、LeakyReLU函数、ELU函数（Exponential Linear Unit）、SELU（Scaled ExponentialLinear Unit）、Softmax 等等，有兴趣的朋友可以自行了解。</p><span class="math display">\[\begin{aligned}y = \frac{1}{1+e^{-x}}\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-16.png?x-oss-process=image/resize,w_800" /></p><h2 id="基本结构">基本结构</h2><p>通过构建多层神经元，每一层中的神经元都与在其前后层中的所有神经元相互连接，即可得到一个分层的人工神经网络。</p><p>根据分层所在的位置，我们将分层分为三种类型：输入层、隐藏层、输出层。任意一个神经网络的输入层和输出层各自只有一个，隐藏层可以有多个。下图所示是一个三层结构的神经网络，可以看到每个节点都与前一层或后一层的其他每个节点相互连接。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-14.png?x-oss-process=image/resize,w_800" /></p><p>基于训练样本，神经网络会进行学习，那么它有没有可以调整的参数呢？类似上文介绍的分类器，我们可以调整斜率参数。在神经网络中，则是<strong>通过调整节点之间的连接强度作为训练样本的学习反馈</strong>。</p><p>下图所示展示了节点之间各个连接的连接强度，使用<strong>权重</strong>（Weight）表示，比如：<spanclass="math inline">\(w_{1,2}\)</span> 表示当前层节点 1 与后一层节点 2之间的连接强度。通常权重值的范围位于 <span class="math inline">\([0,1]\)</span> 之间，当权重值为 0 时，则表示连接断开。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-15.png?x-oss-process=image/resize,w_800" /></p><h1 id="信号转换">信号转换</h1><p>了解了神经网络的基本结构之后，我们来介绍一下输入信号是如何在神经网络中经过一层一层的神经元，最终转换成输出信号的。在这个过程中，我们会结合矩阵运算来进行表达。</p><p>下图所示是一个具有 3 个分层，每个分层 3个节点的神经网络，为了保持图示清晰，我们没有标注所有权重。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-17.png?x-oss-process=image/resize,w_800" /></p><p>下面，我们来依次看一下输入层、隐藏层、输出层对于信号的转换和处理。</p><h2 id="输入层">输入层</h2><p>在人工神经网络中，输入层的节点通常不会进行求和函数和激活函数的处理。输入层的节点主要负责接收原始的输入信号，并将其直接透传给下一层的隐藏层或输出层。</p><p>对于输入层，我们可以使用矩阵 <spanclass="math inline">\(I_{input}\)</span> 来表示输入信号，使用矩阵 <spanclass="math inline">\(O_{input}\)</span>表示输出信号。由此得到如下所示表示：</p><span class="math display">\[\begin{aligned}O_{input} = I_{input} =\left(\begin{matrix}0.9\\0.1\\0.8\end{matrix}\right)\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-18.png?x-oss-process=image/resize,w_800" /></p><h2 id="隐藏层">隐藏层</h2><p>对于隐藏层，我们可以使用矩阵 <spanclass="math inline">\(W_{input\_hidden}\)</span>来表示输入层与隐藏层之间的连接权重，如下所示：</p><span class="math display">\[\begin{aligned}W_{input\_hidden} =\left(\begin{matrix}0.9 &amp; 0.3 &amp; 0.4\\0.2 &amp; 0.8 &amp; 0.2\\0.1 &amp; 0.5 &amp; 0.6\end{matrix}\right)\end{aligned}\]</span><p>隐藏层神经元节点接收输入信号后，会依次使用求和函数、激活函数进行处理，然后进行输出。关于求和函数，我们可以使用矩阵点乘来表示，这里使用<span class="math inline">\(X_{hidden}\)</span>来表示隐藏层求和函数的计算结果。</p><span class="math display">\[\begin{aligned}X_{hidden} = &amp; W_{input\_hidden} \cdot O_{input}\\= &amp;\left(\begin{matrix}0.9 &amp; 0.3 &amp; 0.4\\0.2 &amp; 0.8 &amp; 0.2\\0.1 &amp; 0.5 &amp; 0.6\end{matrix}\right)\cdot\left(\begin{matrix}0.9\\0.1\\0.8\end{matrix}\right)\\= &amp;\left(\begin{matrix}1.16\\0.42\\0.62\end{matrix}\right)\end{aligned}\]</span><p>关于激活函数，我们使用 <span class="math inline">\(sigmoid\)</span>来表示，隐藏层的最终输出仍然可以矩阵来表示，这里使用 <spanclass="math inline">\(O_{hidden}\)</span> 表示隐藏层的最终输出。</p><span class="math display">\[\begin{aligned}O_{hidden} = &amp; sigmoid \left( X_{hidden} \right)\\= &amp;sigmoid\left(\begin{matrix}1.16\\0.42\\0.62\end{matrix}\right)\\= &amp;\left(\begin{matrix}0.761\\0.603\\0.650\end{matrix}\right)\end{aligned}\]</span><p>由此我们得到隐藏层的输出信号，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-19.png?x-oss-process=image/resize,w_800" /></p><h2 id="输出层">输出层</h2><p>输出层对于信号的转换和处理，本质上和隐藏层没有任何区别，计算的方法和流程是一样的。</p><p>我们使用 <span class="math inline">\(W_{hidden\_output}\)</span>表示隐藏层与输出层之间的连接权重，结合隐藏层的输出 <spanclass="math inline">\(O_{hidden}\)</span>，通过矩阵点乘来运用求和函数，得到<span class="math inline">\(X_{output}\)</span>，如下所示。</p><span class="math display">\[\begin{aligned}X_{output} = &amp; W_{hidden\_output} \cdot O_{hidden}\\= &amp;\left(\begin{matrix}0.3 &amp; 0.7 &amp; 0.5\\0.6 &amp; 0.5 &amp; 0.2\\0.8 &amp; 0.1 &amp; 0.9\end{matrix}\right)\cdot\left(\begin{matrix}0.761\\0.603\\0.650\end{matrix}\right)\\= &amp;\left(\begin{matrix}0.975\\0.888\\1.254\end{matrix}\right)\end{aligned}\]</span><p>最后再应用激活函数，得到输出层的结果 <spanclass="math inline">\(O_{output}\)</span> ，如下所示。</p><span class="math display">\[\begin{aligned}O_{output} = &amp; sigmoid \left( X_{output} \right)\\= &amp;sigmoid\left(\begin{matrix}0.975\\0.888\\1.254\end{matrix}\right)\\= &amp;\left(\begin{matrix}0.726\\0.708\\0.778\end{matrix}\right)\end{aligned}\]</span><p>由此我们得到输出层的输出信号，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-20.png?x-oss-process=image/resize,w_800" /></p><h1 id="训练反馈">训练反馈</h1><p>在实际应用神经网络之前，我们必须使用大量训练样本对其进行训练，训练的核心目的是<strong>调整各节点之间链接权重的值</strong>，使其调整为合适的值，从而让神经网络能够输出相对准确的结果。</p><h2 id="值的约束">值的约束</h2><p>首先我们来看下神经网络中的值，其主要包含四种：目标值、输入值、输出值、权重值。这些值的范围与神经网络运行和训练密切相关，下面我们分别来了解一下。</p><h3 id="输出值-目标值">输出值 &amp; 目标值</h3><p>输出值的范围与激活函数有关。下图所示是 Sigmoid激活函数的曲线图，其输出值的范围为 <span class="math inline">\((0,1)\)</span>。训练样本的目标值的范围也应该与输出值的范围一样；否则，神经网络会驱使更大（或更小）的权重，导致学习能力过犹不足。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-22.png?x-oss-process=image/resize,w_800" /></p><h3 id="输入值">输入值</h3><p>类似输出值，输入值范围也与激活函数有关。对于 sigmoid函数，当输入值超出某个范围之后，输出值会变得非常平坦，换句话说就是梯度（斜率）差异很小，从而会导致学习能力降低。</p><p>很显然，梯度变化适中的区域更适合用于神经网络的训练，如下图所示，输入值的范围在<span class="math inline">\([-1, 1]\)</span>之内的激活函数梯度变化适中。不过，我们要注意输入值为 0的情况，此时结合任意权重值后，信号都会变成 0，权重更新表达式也会变成0，从而导致学习能力丧失（后续权重更新中会详细进行介绍）。因此我们要避免输入值等于0，一般建议将输入值的范围设置为 <span class="math inline">\((0,1]\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-21.png?x-oss-process=image/resize,w_800" /></p><h3 id="权重值">权重值</h3><p>根据前面的介绍，我们知道权重值会影响求和函数的结果，并最终作为激活函数的输入。因此，初始权重值同样也会影响神经网络的学习能力，过大（或过小）的初始值会造成过大（或过小）的信号传递给激活函数，导致神经网络饱和，从而降低神经网络学习更好权重的能力。</p><p>如下所示，是我们认为 sigmoid激活函数的梯度适中的区域。对此，我们也可以简单地将权重的初始值范围设置为<span class="math inline">\([-1, 1]\)</span>，并随机均匀进行取值。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-23.png?x-oss-process=image/resize,w_800" /></p><p>当然，我们还有更好的权重值初始化方案。对此，我们回顾一下求和函数的输入和输出。一个神经元节点的求和函数输出等于与之相连的前一层所有节点的输出的加权求和。因此我们应该根据输出值的范围逆向推导各个连接的权重值，很显然，这与每个节点的连接的节点数量有关。对此，数学家提出了一种基于经验法则的权重初始化方案，即<strong>在一个节点传入连接数量平方根倒数的大致范围内随机采样，作为权重初始值</strong>。比如，每个节点具有100 条输入连接，那么权重的范围应该在 <spanclass="math inline">\([-1/\sqrt{100}, 1/\sqrt{100}]\)</span> 之间，即<span class="math inline">\([-0.1, 0.1]\)</span> 之间。</p><p>从直觉上讲，这种方案是有意义的。一个节点的输入连接越多，就会有越多的信号叠加在一起。因此，如果连接更多，那么减小权重的范围是有道理的。</p><p>当然，这种优化方案还定义了权重初始值应该遵循正态分布，下图总结了这种基于正态分布的权重初始化方案。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-24.png?x-oss-process=image/resize,w_800" /></p><h2 id="误差分割">误差分割</h2><p>神经网络是基于训练样本的目标值与实际运行的输出值之间的误差来进行反馈学习，从而调整各个连接的权重。然而，每个节点可能有多个输入连接，每个连接有各自的权重值。为了合理分配误差值，神经网络会根据连接的权重来进行分割误差。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-25.png?x-oss-process=image/resize,w_800" /></p><h2 id="误差传播">误差传播</h2><p>了解了误差分割后，我们再来看看神经网络是如何传播误差的。</p><h3 id="输出层-1">输出层</h3><p>我们先来看输出层的误差传播。下图展示了一个具有 2 个输入节点和 2个输出节点的神经网络。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-26.png?x-oss-process=image/resize,w_800" /></p><p>对于输出层节点 1，我们定义其实际输出值为 <spanclass="math inline">\(o_1\)</span>，目标输出值为 <spanclass="math inline">\(t_1\)</span>，那么由此可计算得出误差值 <spanclass="math inline">\(e_1 = (t_1 -o_1)\)</span>。然后我们可以按照连接权重来分割误差，很显然，<spanclass="math inline">\(e_1\)</span> 和 <spanclass="math inline">\(e_2\)</span> 的误差传播组成如下所示。</p><span class="math display">\[\begin{aligned}e_1 = e_1 \frac{w_{1,1}}{w_{1,1} + w_{2,1}} + e_1 \frac{w_{2,1}}{w_{1,1}+ w_{2,1}}\\\\e_2 = e_2 \frac{w_{1,2}}{w_{1,2} + w_{2,2}} + e_2 \frac{w_{2,2}}{w_{1,2}+ w_{2,2}}\end{aligned}\]</span><h3 id="隐藏层-1">隐藏层</h3><p>我们再来看隐藏层的误差传播。下图展示了一个包含输入层、隐藏层、输出层的3 层神经网络。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-27.png?x-oss-process=image/resize,w_800" /></p><p>对于隐藏层的各个节点，它们并没有所谓的目标值，因此无法直接计算其误差。对此，我们可以通过将输出层的误差进行反向传播，层层传递。</p><p>首先，我们根据输出层的误差传播可以计算等到输出层前置连接 <spanclass="math inline">\(w_{1,1}\)</span>、<spanclass="math inline">\(w_{1,2}\)</span>、<spanclass="math inline">\(w_{2,1}\)</span>、<spanclass="math inline">\(w_{2,2}\)</span>各自的误差分量，然后即可计算得到隐藏层节点的误差值 <spanclass="math inline">\(e_1\)</span>、<spanclass="math inline">\(e_2\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-29.png?x-oss-process=image/resize,w_800" /></p><p>由此方法继续计算，可以进一步得出隐层层前置连接各自的误差分量，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-28.png?x-oss-process=image/resize,w_800" /></p><h3 id="矩阵运算">矩阵运算</h3><p>对于各个层的误差值的计算，我们同样可以使用矩阵来表示。我们使用 <spanclass="math inline">\(error_{output}\)</span> 来表示输出层误差，使用<span class="math inline">\(error_{hidden}\)</span>来表示隐藏层误差，其表示如下。</p><span class="math display">\[\begin{aligned}error_{output} = &amp;\left(\begin{matrix}e_1\\e_2\end{matrix}\right)\\\\error_{hidden} = &amp;\left(\begin{matrix}\frac{w_{1,1}}{w_{1,1} + w_{2,1}} &amp; \frac{w_{1,2}}{w_{1,2} +w_{2,2}}\\\frac{w_{2,1}}{w_{2,1} + w_{1,1}} &amp; \frac{w_{2,2}}{w_{2,2} +w_{1,2}}\end{matrix}\right)\cdot\left(\begin{matrix}e_1\\e_2\end{matrix}\right)\end{aligned}\]</span><p><span class="math inline">\(error_{hidden}\)</span>的矩阵定义比较复杂，有没有办法进行简化呢？我们观察到，最终要的是输出误差与链接权重<span class="math inline">\(w_{i,j}\)</span>的乘法。更大的权重意味着更多的输出误差给隐藏层。这里的分母只是一种归一化因此，如果我们忽略这个因子，那么我们仅仅时区了误差的大小，但是能够换来更加简单的矩阵表示，如下所示。</p><span class="math display">\[\begin{aligned}error_{hidden} = &amp;\left(\begin{matrix}w_{1,1} &amp; w_{1,2}\\w_{2,1} &amp; w_{2,2}\end{matrix}\right)\cdot\left(\begin{matrix}e_1\\e_2\end{matrix}\right)\end{aligned}\]</span><p>对比之前的权重矩阵 <spanclass="math inline">\(W_{hidden\_output}\)</span>，可以发现这里的矩阵其实就是<span class="math inline">\(W_{hidden\_output}\)</span>的转置矩阵，因此我们可以将表示式进一步转换成如下所示。</p><span class="math display">\[\begin{aligned}error_{hidden} = W^T_{hidden\_output} \cdot error_{output}\end{aligned}\]</span><h2 id="权重更新">权重更新</h2><p>了解了误差分割与传播后，我们再来看看神经网络是如何通过误差来对权重进行更新，从而达到学习的目的。</p><p>事实上，我们很容易就可以基于神经网络的正向信号转换过程，推导出反向权重更新过程。由于正向过程中，神经元对信号依次进行求和函数、激活函数的处理，并且层与层之间是相互依赖的。对于一个3 x 3的神经网络，输出层的某个节点的计算公式将非常复杂，如下所示。一旦神经玩网络的节点数量、层级数量增加，计算公式会更加复杂，基于此进行逆向的权重更新，将会变得极其复杂！</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-30.png?x-oss-process=image/resize,w_800" /></p><h3 id="梯度下降法">梯度下降法</h3><p>为此，研究人员提出了一种 <strong>梯度下降</strong>（GradientDescent） 的方法来绕过这个问题，解决这个问题。</p><p>我们来举一个例子说明一下说明是梯度下降法。想象一下，一个非常复杂、有山峰山谷的群山峻岭。在黑夜中，伸手不见五指。此时，你在某个山坡上，需要到坡底，手里只有一把手电筒，你该怎么做呢？你只能通过手电筒看到脚下的土地是上坡还是下坡，于是你就小步地往这个方向走。通过这种方式，不需要完整的底图，也不需要事先指定路线，缓慢地前进，慢慢地下山。这种方法，在数学上被称为梯度下降。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-31.png?x-oss-process=image/resize,w_800" /></p><p>那么梯度下降法与神经网络有什么关系呢？其实，我们可以寻找一个误差函数<span class="math inline">\(y = f(x)\)</span>，<spanclass="math inline">\(y\)</span> 表示误差值，<spanclass="math inline">\(x\)</span>表示连接的权重值。对于神经网络的样本训练而言，本质上就是找到最小误差值所对应的权重值，从而更新权重，达到学习的目的。</p><p>为了正确理解梯度下降的思想，我们来使用一个简单的例子来演示一下。假设，误差函数为<span class="math inline">\(y = (x-1)^2 + 1\)</span>。我们希望找到 <spanclass="math inline">\(x\)</span>，从而最小化误差 <spanclass="math inline">\(y\)</span>。那么我们可以通过判断斜率（也称梯度）的方式来寻找，当斜率为负时，<spanclass="math inline">\(x\)</span> 可以尝试适当增大；当斜率为正时，<spanclass="math inline">\(x\)</span>可以尝试适当减小，通过这种方式逐步逼近，从而找到最小值，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-32.png?x-oss-process=image/resize,w_800" /></p><p>为了提高训练学习的效率，我们可以动态调节步长。当梯度较大时，可以使用较大的步长，提高学习效率；当梯度较小时，可以使用较小的步长，避免调整过度。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-33.png?x-oss-process=image/resize,w_800" /></p><p>实际情况下，神经网络中的误差函数并不一定那么简单，它可能会有多个局部最小值。为了避免终止于错误的函数最小值，我们可以使用不同的起点（权重初值值）来进行多次训练。如下图所示，我们使用梯度下降法进行了三次尝试，其中有一次终止于错误的最小值。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-34.png?x-oss-process=image/resize,w_800" /></p><h3 id="误差函数">误差函数</h3><p>神经网络具有非常多的节点和连接，那么我们该如何使用误差函数来描述这么多的连接权重并实现权重更新呢？这里我们针对每一个层都会使用一个误差函数来批量描述连接权重与误差的关系。</p><h4 id="误差表示">误差表示</h4><p>由于每一个层都包含多个节点，我们需要对每个节点的误差进行聚合。此时需要考虑多个误差值的情况下如何表示总体误差，对此下面列出了针对3 个输出节点的神经网络的几种误差表示方法。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-35.png?x-oss-process=image/resize,w_800" /></p><p>方案一：误差 = 目标值 -实际值。这种方案描述误差的方法非常直观，但是在处理多节点的误差聚合时，会出现总和为0 的情况，这会导致神经网络无法得到很好的训练。</p><p>方案二：误差 = |目标值 -实际值|。这种方案解决了多节点的误差聚合可能为 0的问题。但是它的斜率（或称梯度），在最小值附近会出现跳变，从而导致梯度下降法无法很好地发挥作用。下图所示对应的函数及其导数。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-36.png?x-oss-process=image/resize,w_800" /></p><p>方案三：误差 = (目标值 -实际值)^2。这种方案即解决了多节点的误差聚合可能为 0的问题，也解决了斜率跳变的问题。因此，我们采用这种方案作为误差的计算方式。</p><h4 id="斜率推导">斜率推导</h4><p>要使用梯度下降法，我们必须要计算出误差函数相对权重的斜率，这里涉及到微积分的知识。这里如果你对微积分不太熟悉也没关系，我们只要知道最后推导出来的公式即可。</p><p>下图所示分别展示了包含一个权重和两个权重的误差函数及其斜率的示意图。当误差函数中每增加一个权重变量时，函数就会增加一个维度。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-37.png?x-oss-process=image/resize,w_800" /></p><p>下面，我们先来看隐藏层和输出层之间的连接权重。我们使用 <spanclass="math inline">\(w_{i,k}\)</span>表示隐藏层和输出层之间的连接权重，使用 <spanclass="math inline">\(n\)</span> 表示输层节点的数量，然后对误差函数<span class="math inline">\(E\)</span> 进行展开，得到如下表示。</p><span class="math display">\[\begin{aligned}\frac{\partial E}{\partial w_{j,k}} = \frac{\partial \sum_n(t_n -o_n)^2}{\partial w_{j,k}}\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-38.png?x-oss-process=image/resize,w_800" /></p><p>注意，在节点 <span class="math inline">\(n\)</span> 的输出 <spanclass="math inline">\(o_n\)</span>只取决于链接到这个节点的连接。对于单个输出节点 <spanclass="math inline">\(k\)</span>，其只依赖于与它连接的节点及其权重。因此，我们可以进一步简化表达式，如下所示。</p><span class="math display">\[\begin{aligned}\frac{\partial E}{\partial w_{j,k}} = \frac{\partial (t_k -o_k)^2}{\partial w_{j,k}}\end{aligned}\]</span><p>为了继续推导，我们利用链式法则对斜率表达式进行拆分，进而推导，如下所示。</p><span class="math display">\[\begin{aligned}\frac{\partial E}{\partial w_{j,k}} = &amp;\frac{\partial E}{\partial o_k} \cdot  \frac{\partial o_k}{\partialw_{j,k}} \\= &amp;-2(t_k - o_k) \cdot \frac{\partial o_k}{\partial w_{j,k}} \\\end{aligned}\]</span><p>输出层的输出 <span class="math inline">\(o_k\)</span>依赖隐藏层的输出 <spanclass="math inline">\(o_j\)</span>，然后经过输出层节点应用求和函数、激活函数，从而转换成<span class="math inline">\(o_k\)</span>。因此，我们可以继续推导。</p><span class="math display">\[\begin{aligned}\frac{\partial E}{\partial w_{j,k}} = &amp;\frac{\partial E}{\partial o_k} \cdot  \frac{\partial o_k}{\partialw_{j,k}} \\= &amp;-2(t_k - o_k) \cdot \frac{\partial o_k}{\partial w_{j,k}} \\= &amp;-2(t_k - o_k) \cdot \frac{\partial sigmoid(\sum_j w_{j,k} \cdoto_j)}{\partial w_{j, k}} \\\end{aligned}\]</span><p>接下来涉及到微分 sigmoid函数，这里我们直接使用数学家们已经推导出来的结果进行应用，如下所示。</p><span class="math display">\[\begin{aligned}\frac{\partial sigmoid(x)}{\partial x} = sigmoid(x) (1 - sigmoid(x))\end{aligned}\]</span><p>将该结果代入上述斜率推导中，得到：</p><span class="math display">\[\begin{aligned}\frac{\partial E}{\partial w_{j,k}} = &amp;-2(t_k - o_k) \cdot sigmoid(\sum_j w_{j,k} \cdot o_j) (1- sigmoid(\sum_jw_{j,k} \cdot o_j)) \cdot \frac{\partial \sum_j w_{j,k} \cdoto_j}{\partial w_{j, k}} \\= &amp;-2(t_k - o_k) \cdot sigmoid(\sum_j w_{j,k} \cdot o_j) (1- sigmoid(\sum_jw_{j,k} \cdot o_j)) \cdot o_j\end{aligned}\]</span><p>由于在梯度下降法中，我们只关注斜率的方向，所以可以进一步去掉系数<code>2</code>，从而得到如下斜率表达式。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-39.png?x-oss-process=image/resize,w_800" /></p><p>由于权重改变方向与梯度方向相反，结合学习率 <spanclass="math inline">\(\alpha\)</span>，我们可以得到权重更新前后的关系式，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-40.png?x-oss-process=image/resize,w_800" /></p><h4 id="矩阵表示">矩阵表示</h4><p>为了推导矩阵表示，我们尝试使用矩阵乘法的形式进行计算，可以得到如下所示的表示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-41.png?x-oss-process=image/resize,w_800" /></p><p>我们可以发现矩阵表达式中的最后一部分，其实就是前一层 <spanclass="math inline">\(o_j\)</span>的输出的转置。最后，我们可以得到权重更新矩阵的矩阵表达式，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/neural-network-intro-42.png?x-oss-process=image/resize,w_800" /></p><h1 id="总结">总结</h1><p>本文我们主要介绍了人工神经网络工作的基本原理，从而能够对它产生一个初步认知，有利于后续的进阶学习。</p><p>首先，我们以一个分类器结构介绍了机器学习的基本思想。在此基础上进行扩展，组合多个分类器，这也是人工神经网络的基本思想。</p><p>然后，我们介绍了神经网络正向的信号转换过程，其中涉及连接权重、求和函数、激活函数等。此外，我们还介绍了神经网络的训练的相关概念，包括：误差分割、误差传播、权重更新等。关于权重更新，我们重点介绍了误差函数及斜率的推导。</p><p>后续有时间我们将进一步介绍如何使用 Python打造一个简单的人工神经网络。</p><h1 id="参考">参考</h1><ol type="1"><li>《Python神经网络编程》</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近看了《Python
神经网络编程》一书之后，对于神经网络的基本原理有了一个初步的理解，于是产出此篇文章作为系统性的梳理和总结。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://chuquan.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="神经网络" scheme="http://chuquan.me/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="激活函数" scheme="http://chuquan.me/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
    <category term="梯度下降" scheme="http://chuquan.me/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>基于 Threejs 实现 3D 魔方</title>
    <link href="http://chuquan.me/2024/06/01/rubiks-cube-01/"/>
    <id>http://chuquan.me/2024/06/01/rubiks-cube-01/</id>
    <published>2024-06-01T09:54:33.000Z</published>
    <updated>2024-06-01T11:26:13.014Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-12.png?x-oss-process=image/resize,w_800" /></p><span id="more"></span><p>最近这段时间学习了计算机图形学和Threejs，为了巩固一下学习效果，同时也希望给<ahref="https://apps.apple.com/cn/app/%E8%8E%AB%E8%B4%9F%E4%BC%91%E6%81%AF-%E4%BC%91%E6%81%AF%E6%8F%90%E9%86%92/id6474056217?mt=12">「莫负休息」</a>新增主题，于是基于Threejs 实现了一个魔方程序。当然，基于 Threejs的魔方程序其实早就已经有了，我只不过是站在前人的成果上做了一次实践和总结而已。</p><p>源码传送门——<ahref="https://github.com/baochuquan/rubiks-cube">Rubiks Cube</a>，Demo传送门——<a href="http://rubiks.chuquan.me">rubiks.chuquan.me</a>。</p><h1 id="魔方的定义">魔方的定义</h1><p>魔方（Rubik's Cube），是匈牙利建筑学教授和雕塑鲁比克·埃尔内，于 1974年发明的机械益智玩具。</p><p>魔方是一个正立方体，一共 6 个面，对应 6种颜色。魔方的官方配色是：白色、红色、橙色、黄色、绿色、蓝色，其中黄白相对，红橙相对，蓝绿相对，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-01.png?x-oss-process=image/resize,w_800" /></p><p>一个三阶魔方由 3 x 3 x 3 共 27 个方块组成，根据方块的位置，可以分为 3种类型，分别是：</p><ul><li>中心块：中心块有 6个，位于魔方每面的正中心，只有一种颜色。中心块彼此之间的相对位置不会变化。</li><li>棱块：棱块有 12个，位于魔方每个魔方中心块的上下左右，有两种颜色。</li><li>角块：角块有 8 个，位于魔方每个魔方中心块的斜对角，有三种颜色。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-02.png?x-oss-process=image/resize,w_800" /></p><h1 id="场景布置">场景布置</h1><p>对于任意 3D场景，我们都需要先对场景中的基本元素进行设置，主要包括：相机、灯光、渲染器。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-03.png?x-oss-process=image/resize,w_800" /></p><p>首先初始化一个场景<code>Scene</code>，后续所有相关元素都将添加至这个场景中，并设置位置坐标。</p><p>然后，我们初始化相机，Threejs中有两种相机：正交相机、透视相机。透视相机成像的画面具有近大远小的效果，所以我们这里使用透视相机。当然，相机的位置确立之后，我们还需要确定它的观测方向，这里使用<code>lookAt</code> 方法。此外，我们还可以设置相机的视场（Field ofView），它表示相机的可视角度值，决定了屏幕画面的可视范围。</p><p>对于灯光，这里我只设置了一个环境光，因此无需设置坐标。当然，Threejs中有很多光源，比如：点光源、面光源、射线光源等。</p><p>相关的代码实现如下所示。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> scene, camera, renderer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">setupScene</span>(<span class="params"></span>) &#123;</span><br><span class="line">  scene = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Scene</span>();</span><br><span class="line">  scene.<span class="property">background</span> = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Color</span>(<span class="number">0xFFFFFF</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">setupCamera</span>(<span class="params"></span>) &#123;</span><br><span class="line">  camera = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">PerspectiveCamera</span>(</span><br><span class="line">    <span class="number">45</span>,</span><br><span class="line">    <span class="variable language_">window</span>.<span class="property">innerWidth</span> / <span class="variable language_">window</span>.<span class="property">innerHeight</span>,</span><br><span class="line">    <span class="number">0.1</span>, </span><br><span class="line">    <span class="number">1000</span></span><br><span class="line">  );</span><br><span class="line">  camera.<span class="property">position</span>.<span class="title function_">set</span>(<span class="number">10</span>, <span class="number">12</span>, <span class="number">10</span>);</span><br><span class="line">  camera.<span class="title function_">lookAt</span>(<span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>);</span><br><span class="line">  camera.<span class="property">fov</span> = <span class="number">45</span></span><br><span class="line">  camera.<span class="title function_">updateProjectionMatrix</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">setupLights</span>(<span class="params"></span>) &#123;</span><br><span class="line">  <span class="keyword">const</span> ambientLight = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">AmbientLight</span>(<span class="number">0xFFFFFF</span>);</span><br><span class="line">  scene.<span class="title function_">add</span>(ambientLight);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后，我们还需要定义一个渲染器。通过渲染器我们才能够将 3D场景的渲染结果并绑定至 2D平面，相关代码如下所示。在具体实现中，我们将渲染器的 DOM 元素绑定至<code>body</code> 中，这样我们才能在 2D 网页中看到渲染效果。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">setupRenderer</span>(<span class="params"></span>) &#123;</span><br><span class="line">  renderer = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">WebGLRenderer</span>(&#123;</span><br><span class="line">    <span class="attr">antialias</span>: <span class="literal">true</span></span><br><span class="line">  &#125;);</span><br><span class="line">  renderer.<span class="title function_">setSize</span>(<span class="variable language_">window</span>.<span class="property">innerWidth</span>, <span class="variable language_">window</span>.<span class="property">innerHeight</span>);</span><br><span class="line">  <span class="variable language_">document</span>.<span class="property">body</span>.<span class="title function_">appendChild</span>(renderer.<span class="property">domElement</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外，为了方便查看空间效果，一般我们会创建一个轨道控制器。基于轨道控制器，我们可以通过鼠标旋转整个空间坐标系，从而可以在不同角度进行观测，相关代码如下所示。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">setupControls</span>(<span class="params"></span>) &#123;</span><br><span class="line">  <span class="comment">// 初始化控制器</span></span><br><span class="line">  controller = <span class="keyword">new</span> <span class="title class_">OrbitControls</span>(camera, renderer.<span class="property">domElement</span>);</span><br><span class="line">  controller.<span class="property">enableDamping</span> = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="魔方建模">魔方建模</h1><p>完成了场景布置之后，我们将在空间中对魔方进行建模。建模的过程非常简单，只需创建3 x 3 x 3 共 27个立方体即可，每个立方体的表面使用贴图作为材质。为了便于后续旋转魔方时获取同一平面中的9 个立方体，我们在建模时会对每个立方体设置编号索引，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-04.png?x-oss-process=image/resize,w_800" /></p><p>魔方建模的实现代码如下所示。 <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建立方体，并加入场景</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">setupCubes</span>(<span class="params"></span>) &#123;</span><br><span class="line">  cubes = <span class="title function_">createCube</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; cubes.<span class="property">length</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">var</span> cube = cubes[i];</span><br><span class="line">    scene.<span class="title function_">add</span>(cube);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建立方体，设置空间左边，使用贴图作为材质</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">createCube</span>(<span class="params">x, y, z, num, len</span>) &#123;</span><br><span class="line">  <span class="comment">// 魔方左上角坐标</span></span><br><span class="line">  <span class="keyword">var</span> leftUpX = x - num / <span class="number">2</span> * len;</span><br><span class="line">  <span class="keyword">var</span> leftUpY = y + num / <span class="number">2</span> * len;</span><br><span class="line">  <span class="keyword">var</span> leftUpZ = z + num / <span class="number">2</span> * len;</span><br><span class="line">  <span class="comment">// 根据颜色生成材质</span></span><br><span class="line">  <span class="keyword">const</span> loader = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">TextureLoader</span>();</span><br><span class="line">  <span class="keyword">const</span> textures = [</span><br><span class="line">    loader.<span class="title function_">load</span>(<span class="string">&quot;./img/blue.png&quot;</span>),</span><br><span class="line">    loader.<span class="title function_">load</span>(<span class="string">&quot;./img/green.png&quot;</span>),</span><br><span class="line">    loader.<span class="title function_">load</span>(<span class="string">&quot;./img/yellow.png&quot;</span>),</span><br><span class="line">    loader.<span class="title function_">load</span>(<span class="string">&quot;./img/white.png&quot;</span>),</span><br><span class="line">    loader.<span class="title function_">load</span>(<span class="string">&quot;./img/orange.png&quot;</span>),</span><br><span class="line">    loader.<span class="title function_">load</span>(<span class="string">&quot;./img/red.png&quot;</span>),</span><br><span class="line">  ];</span><br><span class="line">  <span class="keyword">const</span> materials = textures.<span class="title function_">map</span>(<span class="function"><span class="params">texture</span> =&gt;</span> <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">MeshBasicMaterial</span>(&#123; <span class="attr">map</span>: texture &#125;));</span><br><span class="line">  <span class="comment">// 生成小方块</span></span><br><span class="line">  <span class="keyword">var</span> cubes = [];</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; num; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> j = <span class="number">0</span>; j &lt; num * num; j++) &#123;</span><br><span class="line">      <span class="keyword">var</span> box = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">BoxGeometry</span>(len, len, len);</span><br><span class="line">      <span class="keyword">var</span> mesh = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Mesh</span>(box, materials);</span><br><span class="line">      <span class="comment">// 依次计算各个小方块中心点坐标</span></span><br><span class="line">      mesh.<span class="property">position</span>.<span class="property">x</span> = (leftUpX + len / <span class="number">2</span>) + (j % num) * len;</span><br><span class="line">      mesh.<span class="property">position</span>.<span class="property">y</span> = (leftUpY - len / <span class="number">2</span>) - <span class="built_in">parseInt</span>(j / num) * len;</span><br><span class="line">      mesh.<span class="property">position</span>.<span class="property">z</span> = (leftUpZ - len / <span class="number">2</span>) - i * len;</span><br><span class="line">      mesh.<span class="property">tag</span> = i * <span class="number">9</span> + j;</span><br><span class="line">      cubes.<span class="title function_">push</span>(mesh);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> cubes;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>至此，魔方建模实现完成，完整的代码可以参考 <ahref="https://github.com/baochuquan/rubiks-cube/blob/main/src/components/RubiksCube01.vue">RubiksCube01.vue</a>文件。</p><h1 id="魔方控制">魔方控制</h1><p>魔方控制是基于鼠标实现的，核心思想分为以下几个步骤：</p><ul><li>首先，通过鼠标触点确定触点目标方块和触点平面法向量</li><li>其次，根据鼠标移动方向和触点平面法向量确定旋转方向</li><li>然后，通过旋转方向和触点目标方块获取整个旋转平面</li><li>最后，对整个旋转平面中的所有方块执行旋转动画</li></ul><h2 id="监听鼠标事件">监听鼠标事件</h2><p>鼠标事件是控制魔方的基础，因此我们需要实现鼠标事件的监听。相关实现如下所示，我们同时处理了鼠标控制和触摸控制两种情况。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">setupEvents</span>(<span class="params"></span>) &#123;</span><br><span class="line">  <span class="variable language_">window</span>.<span class="title function_">addEventListener</span>(<span class="string">&#x27;mousedown&#x27;</span>, startMouse);</span><br><span class="line">  <span class="variable language_">window</span>.<span class="title function_">addEventListener</span>(<span class="string">&#x27;mousemove&#x27;</span>, moveMouse);</span><br><span class="line">  <span class="variable language_">window</span>.<span class="title function_">addEventListener</span>(<span class="string">&#x27;mouseup&#x27;</span>, stopMouse);</span><br><span class="line">  <span class="variable language_">window</span>.<span class="title function_">addEventListener</span>(<span class="string">&#x27;touchstart&#x27;</span>, startMouse);</span><br><span class="line">  <span class="variable language_">window</span>.<span class="title function_">addEventListener</span>(<span class="string">&#x27;touchmove&#x27;</span>, moveMouse);</span><br><span class="line">  <span class="variable language_">window</span>.<span class="title function_">addEventListener</span>(<span class="string">&#x27;touchend&#x27;</span>, stopMouse);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="确定触点方块与平面法向量">确定触点方块与平面法向量</h2><p>对于确定目标触点方块和平面法向量，这里有两个问题：</p><ul><li>如何通过二维平面中的鼠标位置确定三维空间中的位置呢？</li><li>立方体的位置不固定，那么该如何确定触点平面的方向呢？</li></ul><p>对于第一个问题，解决方法是<strong>射线</strong>（Raycaster），其基本原理是：通过相机位置和鼠标位置确定三维空间中的一根射线，延伸射线，找到三维空间中与射线相交的物体，根据自定义规则（比如：第一个）来找到目标物体。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-05.png?x-oss-process=image/resize,w_800" /></p><p>对于第二个问题，我们首先需要了解一下 Threejs 中的坐标系统： -全局坐标系：也称世界坐标系，是整个 3D 场景的坐标系。 -局部坐标系：也称物体坐标系。在 iOS/Android 中存在视图层级树，在 Threejs中同样存在场景层级树，整个 3D场景是根场景，空间中的物体可以作为子场景，子场景又可以继续添加场景。每个场景有自己的坐标系，当对一个场景进行仿射变换，那么它的子场景也会发生仿射变换，这就是物体坐标系的作用。</p><p>由于魔方旋转过程中，每个立方体自身的也在不停的旋转和移动，此时每个物体的局部坐标系也会发生变换，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-06.png?x-oss-process=image/resize,w_800" /></p><p>此时，如果基于目标立方体获取其表面法向量，那么获取到的法向量是基于局部坐标系的，不具备全局意义。因此，我们必须要将基于<strong>局部坐标系</strong> 的表面法向量转换成基于<strong>全局坐标系</strong> 的表面法向量。</p><p>对此，有两种解决方法：</p><ul><li>对基于局部坐标系的法向量通过矩阵变换，转换成基于全局坐标系。</li><li>增加一个固定不变的透明物体，通过射线获取其表面法向量，以代表立方体的表面法向量。</li></ul><p>对于前者，我们需要记录立方体从原始位置到当前位置的所有变换操作，再对基于局部坐标系的法向量做逆变换。这种方案实现难度且计算量都很大。</p><p>对于后者，其实现难度显然更低。我们只需创建一个透明的立方体，其大小与魔方整体相同，如下图所示。当判断表面法向量时，通过该透明立方体获取即可，由此得到的是基于全局坐标系的法向量。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-07.png?x-oss-process=image/resize,w_800" /></p><p>如下所示为确定触点方块与平面法向量的核心代码逻辑。 <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">setupRubiks</span>(<span class="params"></span>) &#123;</span><br><span class="line">  <span class="comment">// 透明正方体</span></span><br><span class="line">  <span class="keyword">let</span> box = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">BoxGeometry</span>(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>);</span><br><span class="line">  <span class="keyword">let</span> mesh = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">MeshBasicMaterial</span>(&#123;<span class="attr">vertexColors</span>: <span class="variable constant_">THREE</span>.<span class="property">FaceColors</span>, <span class="attr">opacity</span>: <span class="number">0</span>, <span class="attr">transparent</span>: <span class="literal">true</span>&#125;);</span><br><span class="line">  rubiks = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Mesh</span>(box, mesh);</span><br><span class="line">  rubiks.<span class="property">cubeType</span> = <span class="string">&#x27;coverCube&#x27;</span>;</span><br><span class="line">  scene.<span class="title function_">add</span>(rubiks);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取操作焦点以及该焦点所在平面的法向量 </span></span><br><span class="line"><span class="comment"> * */</span> </span><br><span class="line"><span class="keyword">function</span> <span class="title function_">getIntersectAndNormalize</span>(<span class="params">event</span>) &#123;</span><br><span class="line">  <span class="keyword">let</span> mouse = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector2</span>();</span><br><span class="line">  <span class="keyword">if</span> (event.<span class="property">touches</span>) &#123;</span><br><span class="line">    <span class="comment">// 触摸事件</span></span><br><span class="line">    <span class="keyword">var</span> touch = event.<span class="property">touches</span>[<span class="number">0</span>];</span><br><span class="line">    mouse.<span class="property">x</span> = (touch.<span class="property">clientX</span> / <span class="variable language_">window</span>.<span class="property">innerWidth</span>) * <span class="number">2</span> - <span class="number">1</span>;</span><br><span class="line">    mouse.<span class="property">y</span> = -(touch.<span class="property">clientY</span> / <span class="variable language_">window</span>.<span class="property">innerHeight</span>) * <span class="number">2</span> + <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 鼠标事件</span></span><br><span class="line">    mouse.<span class="property">x</span> = (event.<span class="property">clientX</span> / <span class="variable language_">window</span>.<span class="property">innerWidth</span>) * <span class="number">2</span> - <span class="number">1</span>;</span><br><span class="line">    mouse.<span class="property">y</span> = -(event.<span class="property">clientY</span> / <span class="variable language_">window</span>.<span class="property">innerHeight</span>) * <span class="number">2</span> + <span class="number">1</span>;</span><br><span class="line">  &#125; </span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> raycaster = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Raycaster</span>();</span><br><span class="line">  raycaster.<span class="title function_">setFromCamera</span>(mouse, camera);</span><br><span class="line">  <span class="comment">// Raycaster方式定位选取元素，可能会选取多个，以第一个为准</span></span><br><span class="line">  <span class="keyword">var</span> intersects = raycaster.<span class="title function_">intersectObjects</span>(scene.<span class="property">children</span>);</span><br><span class="line">  <span class="keyword">var</span> intersect, normalize;</span><br><span class="line">  <span class="keyword">if</span> (intersects.<span class="property">length</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (intersects[<span class="number">0</span>].<span class="property">object</span>.<span class="property">cubeType</span> === <span class="string">&#x27;coverCube&#x27;</span>) &#123;</span><br><span class="line">        intersect = intersects[<span class="number">1</span>];</span><br><span class="line">        normalize = intersects[<span class="number">0</span>].<span class="property">face</span>.<span class="property">normal</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        intersect = intersects[<span class="number">0</span>];</span><br><span class="line">        normalize = intersects[<span class="number">1</span>].<span class="property">face</span>.<span class="property">normal</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span>(err) &#123;</span><br><span class="line">      <span class="comment">//nothing</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> &#123;<span class="attr">intersect</span>: intersect, <span class="attr">normalize</span>: normalize&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="确定旋转方向">确定旋转方向</h2><p>下面，我们基于触点目标方块、表面法向量，再结合鼠标移动方向，计算旋转方向。具体实现原理主要包括以下几个步骤：</p><ul><li>计算鼠标的平移向量</li><li>判断平移向量与全局坐标系 6 个方向之间的夹角，选择夹角最小的方向</li><li>结合表面法向量，确定旋转方向</li></ul><p>为什么要结合表面法向量来确定旋转方向？因为同一平移向量时，表面法向量不同，则魔方的旋转方向也不同。如下所示，当鼠标平移方向接近<code>x</code> 轴方向，如果表面法向量与 <code>z</code>轴方向相同，那么魔方将环绕 <code>y</code>轴进行逆时针旋转；如果表面法向量与 <code>y</code>轴方向相同，那么魔方将环绕 <code>z</code> 轴进行顺时针旋转。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-08.png?x-oss-process=image/resize,w_800" /></p><p>如下所示，为判断魔方旋转方向的代码逻辑。我们根据不同的拖拽方向分情况讨论，最终确定魔方的6 种旋转方向。 <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 魔方转动的六个方向</span></span><br><span class="line"><span class="keyword">const</span> xLine = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span> );     <span class="comment">// X轴正方向</span></span><br><span class="line"><span class="keyword">const</span> xLineAd = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span> );  <span class="comment">// X轴负方向</span></span><br><span class="line"><span class="keyword">const</span> yLine = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span> );     <span class="comment">// Y轴正方向</span></span><br><span class="line"><span class="keyword">const</span> yLineAd = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( <span class="number">0</span>, -<span class="number">1</span>, <span class="number">0</span> );  <span class="comment">// Y轴负方向</span></span><br><span class="line"><span class="keyword">const</span> zLine = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span> );     <span class="comment">// Z轴正方向</span></span><br><span class="line"><span class="keyword">const</span> zLineAd = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( <span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span> );  <span class="comment">// Z轴负方向</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获得旋转方向</span></span><br><span class="line"><span class="comment"> * vector3: 鼠标滑动的方向</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">getDirection</span>(<span class="params">vector3</span>) &#123;</span><br><span class="line">  <span class="keyword">var</span> direction;</span><br><span class="line">  <span class="comment">// 判断差向量和 x、y、z 轴的夹角</span></span><br><span class="line">  <span class="keyword">var</span> xAngle = vector3.<span class="title function_">angleTo</span>(xLine);</span><br><span class="line">  <span class="keyword">var</span> xAngleAd = vector3.<span class="title function_">angleTo</span>(xLineAd);</span><br><span class="line">  <span class="keyword">var</span> yAngle = vector3.<span class="title function_">angleTo</span>(yLine);</span><br><span class="line">  <span class="keyword">var</span> yAngleAd = vector3.<span class="title function_">angleTo</span>(yLineAd);</span><br><span class="line">  <span class="keyword">var</span> zAngle = vector3.<span class="title function_">angleTo</span>(zLine);</span><br><span class="line">  <span class="keyword">var</span> zAngleAd = vector3.<span class="title function_">angleTo</span>(zLineAd);</span><br><span class="line">  <span class="keyword">var</span> minAngle = <span class="title class_">Math</span>.<span class="title function_">min</span>(...[xAngle, xAngleAd, yAngle, yAngleAd, zAngle, zAngleAd]);  <span class="comment">// 最小夹角</span></span><br><span class="line">  <span class="keyword">switch</span>(minAngle)&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">xAngle</span>:</span><br><span class="line">      direction = <span class="number">10</span>;  <span class="comment">// 向x轴正方向旋转90度（还要区分是绕z轴还是绕y轴）</span></span><br><span class="line">      <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(yLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">5</span>;  <span class="comment">// 绕z轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(yLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">6</span>;  <span class="comment">// 绕z轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(zLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">4</span>;  <span class="comment">// 绕y轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(zLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">3</span>;  <span class="comment">// 绕y轴顺时针</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">xAngleAd</span>:</span><br><span class="line">      direction = <span class="number">20</span>;  <span class="comment">// 向x轴反方向旋转90度</span></span><br><span class="line">      <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(yLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">6</span>;  <span class="comment">// 绕z轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(yLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">5</span>;  <span class="comment">// 绕z轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(zLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">3</span>;  <span class="comment">// 绕y轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(zLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">4</span>;  <span class="comment">// 绕y轴逆时针</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">yAngle</span>:</span><br><span class="line">      direction = <span class="number">30</span>;  <span class="comment">// 向y轴正方向旋转90度</span></span><br><span class="line">      <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(zLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">1</span>;  <span class="comment">// 绕x轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(zLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">2</span>;  <span class="comment">// 绕x轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(xLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">6</span>;  <span class="comment">// 绕z轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        direction = direction + <span class="number">5</span>;  <span class="comment">// 绕z轴顺时针</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">yAngleAd</span>:</span><br><span class="line">      direction = <span class="number">40</span>;  <span class="comment">// 向y轴反方向旋转90度</span></span><br><span class="line">      <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(zLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">2</span>;  <span class="comment">// 绕x轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(zLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">1</span>;  <span class="comment">// 绕x轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(xLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">5</span>;  <span class="comment">// 绕z轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        direction = direction + <span class="number">6</span>;  <span class="comment">// 绕z轴逆时针</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">zAngle</span>:</span><br><span class="line">      direction = <span class="number">50</span>;  <span class="comment">// 向z轴正方向旋转90度</span></span><br><span class="line">      <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(yLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">2</span>;  <span class="comment">// 绕x轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(yLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">1</span>;  <span class="comment">// 绕x轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(xLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">3</span>;  <span class="comment">// 绕y轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(xLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">4</span>;  <span class="comment">// 绕y轴逆时针</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="attr">zAngleAd</span>:</span><br><span class="line">      direction = <span class="number">60</span>;  <span class="comment">// 向z轴反方向旋转90度</span></span><br><span class="line">      <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(yLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">1</span>;  <span class="comment">// 绕x轴顺时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(yLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">2</span>;  <span class="comment">// 绕x轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(xLine)) &#123;</span><br><span class="line">        direction = direction + <span class="number">4</span>;  <span class="comment">// 绕y轴逆时针</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (normalize.<span class="title function_">equals</span>(xLineAd)) &#123;</span><br><span class="line">        direction = direction + <span class="number">3</span>;  <span class="comment">// 绕y轴顺时针</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="attr">default</span>:</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> direction;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="确定旋转平面">确定旋转平面</h2><p>随后，我们可以根据触点目标方块的位置，结合旋转方向，找到与它同一旋转平面的立方体。比如，对于绕<code>x</code> 轴旋转时，我们只需要找到所有与触点目标方块的<code>x</code> 坐标相同的立方体即可。相关实现如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-11.png?x-oss-process=image/resize,w_800" /></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据立方体和旋转方向，找到同一平面上的所有立方体</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">getPlaneCubes</span>(<span class="params">cube, direction</span>) &#123;</span><br><span class="line">  <span class="keyword">let</span> results = [];</span><br><span class="line">  <span class="keyword">let</span> orientation = direction % <span class="number">10</span>;</span><br><span class="line">  <span class="keyword">let</span> radians = (orientation % <span class="number">2</span> == <span class="number">1</span>) ? <span class="number">90</span> : -<span class="number">90</span>;</span><br><span class="line">  <span class="keyword">switch</span> (orientation) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">      <span class="comment">// 绕x轴</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; cubes.<span class="property">length</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">let</span> curr = cubes[i];</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(curr.<span class="property">position</span>);</span><br><span class="line">        <span class="keyword">if</span> (<span class="title class_">Math</span>.<span class="title function_">abs</span>(curr.<span class="property">position</span>.<span class="property">x</span> - cube.<span class="property">position</span>.<span class="property">x</span>) &lt; <span class="number">0.2</span>) &#123;</span><br><span class="line">          results.<span class="title function_">push</span>(curr);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">      <span class="comment">// 绕y轴</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; cubes.<span class="property">length</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">let</span> curr = cubes[i];</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(curr.<span class="property">position</span>);</span><br><span class="line">        <span class="keyword">if</span> (<span class="title class_">Math</span>.<span class="title function_">abs</span>(curr.<span class="property">position</span>.<span class="property">y</span> - cube.<span class="property">position</span>.<span class="property">y</span>) &lt; <span class="number">0.2</span>) &#123;</span><br><span class="line">          results.<span class="title function_">push</span>(curr);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">6</span>:</span><br><span class="line">      <span class="comment">// 绕z轴</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; cubes.<span class="property">length</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">let</span> curr = cubes[i];</span><br><span class="line">        <span class="variable language_">console</span>.<span class="title function_">log</span>(curr.<span class="property">position</span>);</span><br><span class="line">        <span class="keyword">if</span> (<span class="title class_">Math</span>.<span class="title function_">abs</span>(curr.<span class="property">position</span>.<span class="property">z</span> - cube.<span class="property">position</span>.<span class="property">z</span>) &lt; <span class="number">0.2</span>) &#123;</span><br><span class="line">          results.<span class="title function_">push</span>(curr);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> results;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实现旋转动画">实现旋转动画</h2><p>最后，我们需要实现旋转动画。对此，我们首先定义动画时长，根据当前时长与动画时长的比例，计算当前旋转角度的比例，并更新位置，从而实现旋转效果。关于旋转变换，我们在<ahref="https://chuquan.me/2024/01/18/foundation-of-computer-graphic-02/#%E6%97%8B%E8%BD%AC%E5%8F%98%E6%8D%A2">《计算机图形学基础（2）——变换》</a>一文中也介绍过。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-09.png?x-oss-process=image/resize,w_800" /></p><p>我们以 2D平面中的物体旋转来推导旋转矩阵。上图所示，我们将左边的图片进行旋转得到右边的图片，那么我们必须求解如下所示的矩阵运算公式，其中<span class="math inline">\(A\)</span>、<spanclass="math inline">\(B\)</span>、<spanclass="math inline">\(C\)</span>、<span class="math inline">\(D\)</span>为待求解的变量。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39;\end{matrix}\right)=\left(\begin{matrix}A &amp; B \\C &amp; D \\\end{matrix}\right)\left(\begin{matrix}x \\y\end{matrix}\right)\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-10.png?x-oss-process=image/resize,w_800" /></p><p>为了求解 <span class="math inline">\(A\)</span>、<spanclass="math inline">\(B\)</span>、<spanclass="math inline">\(C\)</span>、<span class="math inline">\(D\)</span>四个变量，我们将以 <span class="math inline">\((0, 1)\)</span> 和 <spanclass="math inline">\((1, 0)\)</span> 两个点的旋转为例求解方程。</p><p>对于 <span class="math inline">\((0, 1)\)</span>点的旋转，我们可以得到如下方程：</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}-sin \theta\\cos \theta\end{matrix}\right)=\left(\begin{matrix}A &amp; B \\C &amp; D \\\end{matrix}\right)\left(\begin{matrix}0 \\1\end{matrix}\right)\\-sin \theta = A * 0 + B * 1 = B\\cos \theta = C * 0 + D * 1 = D\end{aligned}\]</span><p>对于 <span class="math inline">\((1, 0)\)</span>点的旋转，我们可以得到如下方程：</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}cos \theta\\sin \theta\end{matrix}\right)=\left(\begin{matrix}A &amp; B \\C &amp; D \\\end{matrix}\right)\left(\begin{matrix}1 \\0\end{matrix}\right)\\cos \theta = A * 1 + B * 0 = A\\sin \theta = C * 1 + D * 0 = C\end{aligned}\]</span><p>至此 <span class="math inline">\(A\)</span>、<spanclass="math inline">\(B\)</span>、<spanclass="math inline">\(C\)</span>、<span class="math inline">\(D\)</span>四个变量均已求解，由此得到旋转矩阵如下：</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39;\\y&#39;\end{matrix}\right)=\left(\begin{matrix}cos\theta &amp; -sin\theta \\sin\theta &amp; cos\theta \\\end{matrix}\right)\left(\begin{matrix}1 \\0\end{matrix}\right)\end{aligned}\]</span><p>进而得到 <span class="math inline">\(x&#39;\)</span> 和 <spanclass="math inline">\(y&#39;\)</span> 的计算公式如下：</p><span class="math display">\[\begin{aligned}x&#39; = cos\theta x - sin\theta y\\y&#39; = sin\theta x + cos\theta y\end{aligned}\]</span><p>由于魔方的旋转都是沿着一个轴进行旋转，所以我们可以将它看成三种情况的2D 平面旋转，由此得到如下 3 个旋转方法。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">rotateAroundWorldX</span>(<span class="params">cube, rad</span>)&#123;</span><br><span class="line">    <span class="keyword">var</span> y0 = cube.<span class="property">position</span>.<span class="property">y</span>;</span><br><span class="line">    <span class="keyword">var</span> z0 = cube.<span class="property">position</span>.<span class="property">z</span>;</span><br><span class="line">    <span class="keyword">var</span> q = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Quaternion</span>(); </span><br><span class="line">    q.<span class="title function_">setFromAxisAngle</span>(<span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span> ), rad);</span><br><span class="line">    cube.<span class="property">quaternion</span>.<span class="title function_">premultiply</span>(q);</span><br><span class="line">    cube.<span class="property">position</span>.<span class="property">y</span> = <span class="title class_">Math</span>.<span class="title function_">cos</span>(rad) * y0 - <span class="title class_">Math</span>.<span class="title function_">sin</span>(rad) * z0;</span><br><span class="line">    cube.<span class="property">position</span>.<span class="property">z</span> = <span class="title class_">Math</span>.<span class="title function_">cos</span>(rad) * z0 + <span class="title class_">Math</span>.<span class="title function_">sin</span>(rad) * y0;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">function</span> <span class="title function_">rotateAroundWorldY</span>(<span class="params">cube, rad</span>)&#123;</span><br><span class="line">    <span class="keyword">var</span> x0 = cube.<span class="property">position</span>.<span class="property">x</span>;</span><br><span class="line">    <span class="keyword">var</span> z0 = cube.<span class="property">position</span>.<span class="property">z</span>;</span><br><span class="line">    <span class="keyword">var</span> q = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Quaternion</span>(); </span><br><span class="line">    q.<span class="title function_">setFromAxisAngle</span>(<span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span> ), rad);</span><br><span class="line">    cube.<span class="property">quaternion</span>.<span class="title function_">premultiply</span>( q );</span><br><span class="line">    cube.<span class="property">position</span>.<span class="property">x</span> = <span class="title class_">Math</span>.<span class="title function_">cos</span>(rad) * x0 + <span class="title class_">Math</span>.<span class="title function_">sin</span>(rad) * z0;</span><br><span class="line">    cube.<span class="property">position</span>.<span class="property">z</span> = <span class="title class_">Math</span>.<span class="title function_">cos</span>(rad) * z0 - <span class="title class_">Math</span>.<span class="title function_">sin</span>(rad) * x0;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">function</span> <span class="title function_">rotateAroundWorldZ</span>(<span class="params">cube, rad</span>)&#123;</span><br><span class="line">    <span class="keyword">var</span> x0 = cube.<span class="property">position</span>.<span class="property">x</span>;</span><br><span class="line">    <span class="keyword">var</span> y0 = cube.<span class="property">position</span>.<span class="property">y</span>;</span><br><span class="line">    <span class="keyword">var</span> q = <span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Quaternion</span>(); </span><br><span class="line">    q.<span class="title function_">setFromAxisAngle</span>(<span class="keyword">new</span> <span class="variable constant_">THREE</span>.<span class="title class_">Vector3</span>( <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span> ), rad);</span><br><span class="line">    cube.<span class="property">quaternion</span>.<span class="title function_">premultiply</span>( q );</span><br><span class="line">    cube.<span class="property">position</span>.<span class="property">x</span> = <span class="title class_">Math</span>.<span class="title function_">cos</span>(rad) * x0 - <span class="title class_">Math</span>.<span class="title function_">sin</span>(rad) * y0;</span><br><span class="line">    cube.<span class="property">position</span>.<span class="property">y</span> = <span class="title class_">Math</span>.<span class="title function_">cos</span>(rad) * y0 + <span class="title class_">Math</span>.<span class="title function_">sin</span>(rad) * x0;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>由于这几个方法仅仅旋转物体、更新坐标，实际上我们需要在一段时间内连续进行调用，从而实现一个完整的旋转动画，具体的调用实现如下所示。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">rotateAnimation</span>(<span class="params">cubes, direction, currentstamp, startstamp, laststamp</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span>(startstamp === <span class="number">0</span>)&#123;</span><br><span class="line">        startstamp = currentstamp;</span><br><span class="line">        laststamp = currentstamp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(currentstamp - startstamp &gt;= rotateDuration)&#123;</span><br><span class="line">      currentstamp = startstamp + rotateDuration;</span><br><span class="line">      isRotating = <span class="literal">false</span>;</span><br><span class="line">      startPoint = <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">let</span> orientation = direction % <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">let</span> radians = (orientation % <span class="number">2</span> == <span class="number">1</span>) ? -<span class="number">90</span> : <span class="number">90</span>;</span><br><span class="line">    <span class="keyword">switch</span> (orientation) &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; cubes.<span class="property">length</span>; i++) &#123;</span><br><span class="line">          <span class="title function_">rotateAroundWorldX</span>(cubes[i], radians * <span class="title class_">Math</span>.<span class="property">PI</span> / <span class="number">180</span> * (currentstamp - laststamp) / rotateDuration);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">      <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; cubes.<span class="property">length</span>; i++) &#123;</span><br><span class="line">          <span class="title function_">rotateAroundWorldY</span>(cubes[i], radians * <span class="title class_">Math</span>.<span class="property">PI</span> / <span class="number">180</span> * (currentstamp - laststamp) / rotateDuration);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">      <span class="keyword">case</span> <span class="number">6</span>:</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; cubes.<span class="property">length</span>; i++) &#123;</span><br><span class="line">          <span class="title function_">rotateAroundWorldZ</span>(cubes[i], radians * <span class="title class_">Math</span>.<span class="property">PI</span> / <span class="number">180</span> * (currentstamp - laststamp) / rotateDuration);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(currentstamp - startstamp &lt; rotateDuration)&#123;</span><br><span class="line">      <span class="title function_">requestAnimationFrame</span>(<span class="function">(<span class="params">timestamp</span>) =&gt;</span> &#123;</span><br><span class="line">        <span class="title function_">rotateAnimation</span>(cubes, direction, timestamp, startstamp, currentstamp);</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>至此，我们实现了通过鼠标控制魔方的旋转，完整的代码可以参考 <ahref="https://github.com/baochuquan/rubiks-cube/blob/main/src/components/RubiksCube02.vue">RubiksCube02.vue</a>文件。</p><h1 id="总结">总结</h1><p>本文我们基于 Threejs 实现了一个 3D魔方，并支持了通过鼠标控制魔方旋转的功能。后续，我们将进一步介绍如何实现魔方的自动还原算法。</p><h1 id="参考">参考</h1><ol type="1"><li><ahref="https://mofang.1or9.com/mofangdingyi.shtml">魔方基本定义</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rubiks-cube-12.png?x-oss-process=image/resize,w_800&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Threejs" scheme="http://chuquan.me/categories/Threejs/"/>
    
    
    <category term="Threejs" scheme="http://chuquan.me/tags/Threejs/"/>
    
    <category term="vue" scheme="http://chuquan.me/tags/vue/"/>
    
  </entry>
  
  <entry>
    <title>欢迎关注我的 Twitter 账号！</title>
    <link href="http://chuquan.me/2024/05/16/my-twitter-account/"/>
    <id>http://chuquan.me/2024/05/16/my-twitter-account/</id>
    <published>2024-05-16T08:43:14.000Z</published>
    <updated>2024-07-19T00:19:00.096Z</updated>
    
    <content type="html"><![CDATA[<p>最近在思考一个问题：如何打造个人品牌？思来想去，我觉得要做到以下几点：</p><ul><li>确定目标受众</li><li>培养专业能力</li><li>维持社交媒体</li><li>建立人脉关系</li></ul><span id="more"></span><p>一直以来，我并没有主动意识到要去打造个人品牌，也白白浪费了写博客的这几年。另一方面，我使用的社交媒体较少，仅限于微信、微博。而其中的朋友和粉丝基本上都是同事、同学、亲戚、朋友。在这样的熟人关系中经常发表一些日常的想法和看法，显然不太合适。从产品的角度解释就是目标人群定位不准确，所以很少更新社交媒体。</p><p>后续，我准备长期维持一个 Twitter账号，发表一些日常的想法和看法。欢迎各位朋友关注 <ahref="https://twitter.com/Baochuquan">BaoChuquan</a>，你们的支持是我创作的动力！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在思考一个问题：如何打造个人品牌？思来想去，我觉得要做到以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;确定目标受众&lt;/li&gt;
&lt;li&gt;培养专业能力&lt;/li&gt;
&lt;li&gt;维持社交媒体&lt;/li&gt;
&lt;li&gt;建立人脉关系&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="生活" scheme="http://chuquan.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
    <category term="Twitter" scheme="http://chuquan.me/tags/Twitter/"/>
    
  </entry>
  
  <entry>
    <title>如何找到最合适的代码审查者？</title>
    <link href="http://chuquan.me/2024/05/11/git-reviewer/"/>
    <id>http://chuquan.me/2024/05/11/git-reviewer/</id>
    <published>2024-05-11T14:02:51.000Z</published>
    <updated>2024-05-11T14:19:09.384Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/git-reviewer-01.png?x-oss-process=image/resize,w_800" /></p><span id="more"></span><p>我的日常工作中有相当一部分时间会用于审查团队成员所提交的代码，即 CodeReview。然而，由我来审查所有代码并不合适，毕竟很多时候我并不了解代码改动的上下文。如果让我重新熟悉别人写的原始逻辑，然后再审查代码改动，很显然是一件费时费力的事情。对此，最好的办法是，针对一次代码提交，合理地找出其最合适的代码审查者。</p><p>为了解决这个问题，我开发了一个 Git 插件——<ahref="https://github.com/baochuquan/git-reviewer">Git Reviewer</a>。</p><h1 id="功能">功能</h1><h2 id="核心功能">核心功能</h2><p>我们知道 Git使用新增和删除两种操作来表示代码改动。事实上，我们还可以从新增和删除两种操作的排列关系中得出第三种操作——编辑。当删除操作和新增操作相互紧邻，那么我们可以将其归为编辑操作。如下所示的代码差异中包含了新增、编辑、删除三种操作。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/git-reviewer-03.png?x-oss-process=image/resize,w_800" /></p><p>Git Reviewer 正是基于这三种操作进行了分析：</p><ul><li><p>对于删除类型，Git Reviewer认为删除行的原始作者应该作为每一行的审查者。</p></li><li><p>对于编辑类型，Git Reviewer认为此部分中的新增内容应该全部由紧邻的最后删除行的原始作者作为审查者。注意，为什么是最后删除行？因为Git 所采用的 Myers差分算法默认差分内容优先展示删除操作，其次才是新增操作。因此，从最后的删除行开始，展示的是新增的内容。</p></li><li><p>对于新增类型，Git Reviewer 的策略是基于<code>.gitreviewer.yml</code>配置文件进行分析。<code>.gitreviewer.yml</code> 文件定义了项目所有者<code>project owner</code>、目录所有者<code>folder owner</code>、文件所有者 <code>file owner</code>。此时，GitReviewer 会对新增行的文件与 <code>.gitreviewer.yml</code>所定义的内容进行匹配。</p><ul><li>如果该文件匹配到了文件所有者，那么相关新增类型优先由文件所有者审查。</li><li>如果该文件匹配到了目录所有者，那么相关新增类型其次由目录所有者审查。</li><li>如果前两者均没有匹配该文件，那么将由项目所有者来进行审查。</li></ul></li></ul><p>基于对上述三种操作类型进行分析，Git Reviewer最终将生成一个分析表格，其中罗列了审查者、文件数量、文件占比、代码行数量、代码行占比等信息。GitReviewer 建议以代码行占比为依据，对审查者进行排序。</p><p>如下所示，为核心功能的分析结果示例。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+------------------------------------------------------------------------+</span><br><span class="line">|                 Suggested reviewers <span class="keyword">for</span> code changes                   |</span><br><span class="line">+--------------------+------------+------------+------------+------------+</span><br><span class="line">| Suggested Reviewer | File Count | File Ratio | Line Count | Line Ratio |</span><br><span class="line">+--------------------+------------+------------+------------+------------+</span><br><span class="line">| developerA         | 5          | 50.0%      | 1000       | 50.0%      |</span><br><span class="line">+--------------------+------------+------------+------------+------------+</span><br><span class="line">| developerB         | 3          | 30.0%      | 500        | 25.0%      |</span><br><span class="line">+--------------------+------------+------------+------------+------------+</span><br><span class="line">| developerC         | 2          | 20.0%      | 500        | 25.0%      |</span><br><span class="line">+--------------------+------------+------------+------------+------------+</span><br></pre></td></tr></table></figure><h2 id="附加功能">附加功能</h2><p>Git Reviewer还提供了分析代码改动的所涉及的作者分布的功能。此功能相对简单，其分析了所有删除行的原始作者和新增行的现有作者，并同样以表格的形式呈现，罗列作者、文件数量、文件占比、代码行数量、代码行占比等信息，以供用户进行评估和参考。</p><p>如下所示，为附加功能的分析结果示例。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+--------------------------------------------------------------------+</span><br><span class="line">|             Relevant authors involved <span class="keyword">in</span> code changes              |</span><br><span class="line">+----------------+------------+------------+------------+------------+</span><br><span class="line">| Related Author | File Count | File Ratio | Line Count | Line Ratio |</span><br><span class="line">+----------------+------------+------------+------------+------------+</span><br><span class="line">| developerA     | 5          | 50.0%      | 2000       | 66.6%      |</span><br><span class="line">+----------------+------------+------------+------------+------------+</span><br><span class="line">| developerB     | 3          | 30.0%      | 500        | 16.7%      |</span><br><span class="line">+----------------+------------+------------+------------+------------+</span><br><span class="line">| developerC     | 2          | 30.0%      | 500        | 16.7%      |</span><br><span class="line">+----------------+------------+------------+------------+------------+</span><br></pre></td></tr></table></figure><h1 id="安装">安装</h1><p>Git Reviewer 支持通过 Homebrew进行安装，命令如下所示。当然，这也是建议的安装方式。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install baochuquan/tap/git-reviewer</span><br></pre></td></tr></table></figure><p>或者，也可以通过 Ruby Gem 进行安装，命令如下所示。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ gem install git-reviewer</span><br></pre></td></tr></table></figure><h1 id="使用">使用</h1><p>对于任意 Git 项目，在使用 Git Reviewer之前应该先在根目录下执行初始化命令，如下所示。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reviewer --init</span><br></pre></td></tr></table></figure><p>该命令会自动创建一个 <code>.gitreviewer.yml</code> 文件，内部定义了<code>project_owner</code>，<code>folder_owner</code>，<code>file_owner</code>等字段，其中后两者是数组类型，我们可以定义多个<code>path</code>、<code>owner</code>字段，从而对项目进行更精准的划分。</p><p>此外，<code>.gitreviewer.yml</code> 文件还包含<code>ignore_folders</code>、<code>ignore_files</code>字段，它们可以定义一系列目录或文件，以避免加入分析，从而影响分析结果。</p><p>如下所示，是一个 <code>.gitreviewer.yml</code>的示例，我们可以编辑相关字段，从而实现更精准的分析。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">project_owner: admin,</span><br><span class="line">folder_owner:</span><br><span class="line">- owner: developerA,</span><br><span class="line">  path: /path/to/folderA</span><br><span class="line">- owner: developerB</span><br><span class="line">  path: /path/to/folderB</span><br><span class="line"> </span><br><span class="line">file_owner:</span><br><span class="line">- owner: developerC</span><br><span class="line">  path: /path/to/fileC</span><br><span class="line">- owner: developerD</span><br><span class="line">  path: /path/to/fileD</span><br><span class="line"> </span><br><span class="line">ignore_files:</span><br><span class="line">- path/to/file1</span><br><span class="line">- path/to/file2</span><br><span class="line"> </span><br><span class="line">ignore_review_folders:</span><br><span class="line">- path/to/folder1</span><br><span class="line">- path/to/folder2</span><br></pre></td></tr></table></figure><h2 id="分析">分析</h2><p>Git Reviewer 基于两个 Git 分支进行分析，分别是源分支和目标分支。</p><p>源分支，即代码修改所在的分支。默认情况下，Git Reviewer自动获取当前所在分支作为源分支。当然，也可以使用选项来指定源分支<code>--source=&lt;source-branch&gt;</code>。除了分支名，Git Reviewer也支持 Commit ID。</p><p>目标分支，即准备合入的目标分支。对此，Git Reviewer 提供了相关选项<code>--target=&lt;target-branch&gt;</code>。</p><p>如下所示是使用 Git Reviewer 进行分析的命令示例。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reviewer --target=main</span><br></pre></td></tr></table></figure><p>默认情况下，Git Reviewer会同时展示核心功能和附加功能的分析结果。如果我们只希望查看核心功能的结果，可以指定选项<code>--reviewer</code>；如果我们只希望查看附加功能的结果，可以指定选项<code>--author</code>。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git reviewer --target=main --reviewer</span><br><span class="line"></span><br><span class="line">$ git reviewer --target=main --author</span><br></pre></td></tr></table></figure><p>为了查看更多分析信息，我们可以加上 --verbose 选项。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reviewer --target=main --verbose</span><br></pre></td></tr></table></figure><h1 id="后续">后续</h1><p>目前，Git Reviewer仅仅经历了我自己的自测，未来我会在项目上进行实践。如果遇到问题也会逐步进行修复。当然，在使用过程中还会遇到不足之处或者新的痛点，因此会逐步进行完善和迭代。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/git-reviewer-01.png?x-oss-process=image/resize,w_800&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="作品集" scheme="http://chuquan.me/categories/%E4%BD%9C%E5%93%81%E9%9B%86/"/>
    
    
    <category term="git-reviewer" scheme="http://chuquan.me/tags/git-reviewer/"/>
    
    <category term="Myers" scheme="http://chuquan.me/tags/Myers/"/>
    
  </entry>
  
  <entry>
    <title>计算机图形学基础（8）——光线追踪</title>
    <link href="http://chuquan.me/2024/04/27/foundation-of-computer-graphic-08/"/>
    <id>http://chuquan.me/2024/04/27/foundation-of-computer-graphic-08/</id>
    <published>2024-04-27T03:13:09.000Z</published>
    <updated>2024-09-22T13:26:51.280Z</updated>
    
    <content type="html"><![CDATA[<p>之前我们介绍了在游戏领域中广泛使用的实时渲染技术——光栅化，本文我们来介绍一下在特效领域中广泛使用的离线渲染技术——光线追踪。</p><span id="more"></span><h1 id="概述">概述</h1><p>那么有了光栅化渲染技术，为什么还要用光线追踪呢？根本原因在于光栅化的着色局部性，使得它无法解决很多全局效果，比如：</p><ul><li><strong>软阴影</strong>（Soft shadows）</li><li><strong>光泽反射</strong>（Glossy reflection）</li><li><strong>间接光照</strong>（Indirect illumination）</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-01.png?x-oss-process=image/resize,w_800" /></p><p>软阴影的产生是由于光源的大小和距离。一个面积较大的光源照射一个物体，可以从多个角度投射光线到物体上。当光线从不同角度照射时，阴影边缘的重叠区域会因光线部分遮挡而出现阴影的渐变效果。</p><p>光泽反射是一种既不完全镜面反射也不完全漫反射的光反射现象。光泽度高、粗糙度低的表面产生的反射相对清晰，接近镜面反射；而光泽度低、粗糙度高的表面，反射图像就会更加模糊，趋向漫反射。</p><p>间接光照是一个场景中光线经过一次或多次反射后照射到物体表面的光照效果。在真实世界中，一个房间只通过一扇窗投过光照，但是房间里没有一个地方是完全黑色的，这就是光的多次反射造成的。</p><h1 id="底层依据">底层依据</h1><p>光线追踪依赖以下几个基本的底层依据，分别是：</p><ul><li><strong>光线沿直线传播</strong>：在微观角度，光是沿着波形传播；在宏观角度，光是沿着直线传播。</li><li><strong>光线相互不碰撞</strong>：当光线的传播路径发生交叠时，我们认为光线的传播互不干扰。</li><li><strong>光线具有可逆性</strong>：真实世界中，视觉成像是因为物体发射或反射光线，进入视网膜；在图形学中，屏幕成像是因为从相机向像素建立了反向的光线传播路径，采集路径上的所有着色信息。</li></ul><h1 id="基本原理">基本原理</h1><p>光线追踪的基本原理非常简单，其采用了<strong>针孔相机模型</strong>（Pinhole Camera Model），分为<strong>视线生成</strong> 和 <strong>像素着色</strong> 两个阶段。</p><p>对于视线生成，以相机为起点，以像素为锚点，利用光的可逆性，建立一条视觉射线，如下所示。此时，我们需要找到视线与空间中的物体所产生的最近的交点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-02.png?x-oss-process=image/resize,w_800" /></p><p>对于像素着色，当我们找到视线与物体之间的最近交点时，随即建立交点与光源的连线，判断交点是否在阴影之中，并根据结果来计算着色，写回像素结果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-03.png?x-oss-process=image/resize,w_800" /></p><h1 id="光线追踪">光线追踪</h1><p>上述基本原理只考虑了光源的直接影响，没有考虑间接影响。在实际情况中，光线会经过空间中物体的多次弹射，汇聚至交点上，从而对着色产生间接影响。</p><p>对此，<strong>经典光线追踪</strong>，也称<strong>惠特式光线追踪</strong>（Whitted-Style Ray Tracing）或<strong>递归光线追踪</strong>（Recursive RayTracing），引入了间接光照的处理。</p><p>在视线生成之后，我们找到最近的交点，然后根据光线的可逆性，找到反射光线（ReflectedRay）与折射光线（Refracted Ray）所产生的交点，依次类推，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-08.png?x-oss-process=image/resize,w_800" /></p><p>之后，我们将所有交点分别与光源进行连线，判断这些交点是否在阴影之中，并根据结果计算着色，写回像素结果。当然，在反射和折射过程中会存在能量折损，因此，在计算着色时也会考虑光线折损的影响。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-09.png?x-oss-process=image/resize,w_800" /></p><h1 id="技术细节">技术细节</h1><p>光线追踪的整体原理非常简单，下面，我们来考虑其中所涉及到的一些技术细节。</p><h2 id="交点判定原理">交点判定原理</h2><p>光线追踪中最重要的技术是交点判定，其中包含光线表示和几何表示两部分。</p><p>对于光线表示，我们只需要使用原点、方向向量即可进行表示，如下所示。</p><span class="math display">\[\begin{aligned}\vec{r}(t) = \vec{o} + t\vec{d}\end{aligned}\]</span><p>对于几何表示，在 <ahref="http://chuquan.me/2024/04/10/foundation-of-computer-graphic-06/">《计算机图形学基础（6）——几何》</a>中，我们提到隐式几何表示和显式几何表示两种。</p><h3 id="隐式几何的交点">隐式几何的交点</h3><p>隐式几何使用数学关系式表示，因此我们可以结合光线表示来求解方程组，即可计算得到交点，如下所示。其中，<spanclass="math inline">\(f\)</span>为隐式几何的数学关系式，将光线表示作为参数代入，求解 <spanclass="math inline">\(t\)</span> 即可得到交点。</p><span class="math display">\[\begin{aligned}f(\vec{o} + t\vec{d}) = 0\end{aligned}\]</span><h3 id="显式几何的交点">显式几何的交点</h3><p>显式几何直接或间接（通过参数映射的方式）定义点、线、面等元素集合，基本上最终都会转换成多边形网格来表示。这里，我们介绍平面和三角形两种情况下的交点判定技术。</p><h4 id="平面交点判定">平面交点判定</h4><p>对于任意一个平面，我们可以使用一个向量和一个点来定义。其中，向量是法向量，平面上任意一个点与<span class="math inline">\(\vec{p&#39;}\)</span>所构成的向量都与法向量垂直，两者的点积为0，因此得到如下所示的平面定义。</p><span class="math display">\[\begin{aligned}(\vec{p} - \vec{p&#39;}) \cdot \vec{N} = 0\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-10.png?x-oss-process=image/resize,w_800" /></p><p>此时，我们再结合平面定义和光线表示得到如下所示的方程组，进而解得<span class="math inline">\(t\)</span> 的值，即交点。</p><span class="math display">\[\begin{aligned}(\vec{o} + t \vec{d} - \vec{p&#39;}) \cdot \vec{N} = 0\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-11.png?x-oss-process=image/resize,w_800" /></p><h4 id="三角形交点判定">三角形交点判定</h4><p>对于三角形交点，我们可以通过 Moller Trumbore Algorithm快速求解。算法利用重心坐标定义三角形内的一个点，以此得到如下关系式。</p><span class="math display">\[\begin{aligned}\vec{o} + t \vec{d} = (1 - b_1 - b_2)\vec{P_0} + b_1\vec{P_1} +b_2\vec{P_2}\end{aligned}\]</span><p>根据重心坐标的特性，当 <span class="math inline">\(1- b_1 - b_2 =0\)</span> 时，点在三角形内，因此关系式中存在三个变量 <spanclass="math inline">\(t\)</span>、<spanclass="math inline">\(b_1\)</span>、<spanclass="math inline">\(b_2\)</span>。由于关系式中的向量都是三维空间中的向量，因此每个向量都由三个坐标值构成，可以进一步得到三个线性方程组，从而求解各个变量。</p><h2 id="交点判定加速">交点判定加速</h2><p>在实际应用中，三维空间可能包含非常多的物体，每个物体可能包含非常多的三角形。假如，我们要判断光线与一个物体的交点，那么需要遍历物体的每一个三角形，如下所示。很显然，这是一个非常耗时的操作，尤其是场景中物体非常多的情况下。对此，图形学中也有一些针对交点判定加速的优化方法。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-12.png?x-oss-process=image/resize,w_800" /></p><h3 id="包围盒">包围盒</h3><p><strong>包围盒</strong>（BoundingBox）的原理是使用一个简单的几何体包围一个复杂的物体，如果光线与包围盒没有交点，那么必然与内部物体没有交点，从而达到交点判定加速的目的。</p><h4 id="轴对齐包围盒">轴对齐包围盒</h4><p>包围盒通常是长方体。对于长方体，我们将它理解成<strong>三对不同的平行面形成的交集</strong>，在实际应用中，我们会使用一种特殊的长方体，即三对面各自与坐标系的轴对齐，因此称为<strong>轴对齐包围盒</strong>（Axis-Aligned Bounding Box，AABB）。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-13.png?x-oss-process=image/resize,w_800" /></p><h4 id="交点判定">交点判定</h4><p>很显然，接下来的问题就是如何判断光线是否与轴对齐包围盒相交。这里，我们首先考虑2D 情况下的交点判定，然后进一步延伸至 3D 情况下的交点判定。</p><p>对于 2D 的情况，如下所示，分为三个步骤：</p><ul><li>考虑光线与 <span class="math inline">\(x_0\)</span>、<spanclass="math inline">\(x_1\)</span>两个面的交点，根据方程组可以计算出一组 <spanclass="math inline">\(t_{min}\)</span> 和 <spanclass="math inline">\(t_{max}\)</span>。</li><li>考虑光线与 <span class="math inline">\(y_0\)</span>、<spanclass="math inline">\(y_1\)</span>两个面的交点，根据方程组又可以计算出一组 <spanclass="math inline">\(t_{min}\)</span> 和 <spanclass="math inline">\(t_{max}\)</span>。</li><li>对于两组 <span class="math inline">\(t_{min}\)</span> 和 <spanclass="math inline">\(t_{max}\)</span>，我们必须保证 <spanclass="math inline">\(t_{min}\)</span> 大于0，否则表示交点在光源的背后，没有实际意义。两组 <spanclass="math inline">\(t_{min}\)</span> 和 <spanclass="math inline">\(t_{max}\)</span>各自构成一个线段，我们只需要计算两者的交集，即可得到光线在长方形内的传播路径，而传播路径的两个端点正是光线与长方形的两个交点。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-14.png?x-oss-process=image/resize,w_800" /></p><p>对于 3D 的情况，我们只需要额外延伸一步即可。</p><ul><li>考虑光线与 <span class="math inline">\(z_0\)</span>、<spanclass="math inline">\(z_1\)</span>两个面的交点，根据方程组又可以计算出一组 <spanclass="math inline">\(t_{min}\)</span> 和 <spanclass="math inline">\(t_{max}\)</span>。</li><li>求解三组 <span class="math inline">\(t_{min}\)</span> 和 <spanclass="math inline">\(t_{max}\)</span>各自构成的线段，求线段的交集即可得到光线在包围盒内的传播路径，传播路径的两个端点正是光线与包围盒的两个交点。</li></ul><p>最终求解得到的 <span class="math inline">\(t_{enter}\)</span> 和<span class="math inline">\(t_{exit}\)</span>的值可能存在一下几种情况：</p><ul><li>当 <span class="math inline">\(t_{exit}\)</span> &lt; 0时，表示包围盒在光源的背后，没有交点。</li><li>当 <span class="math inline">\(t_{enter}\)</span> &lt; 0 且 <spanclass="math inline">\(t_{exit}\)</span> &gt;= 0时，表示光线在包围盒的内部。</li><li>当且仅当 <span class="math inline">\(t_{enter}\)</span> &lt; <spanclass="math inline">\(t_{exit}\)</span>，<spanclass="math inline">\(t_{exit}\)</span> &gt;= 0时，光线和包围盒存在交点。</li></ul><h3 id="统一网格">统一网格</h3><p>统一网格（Uniformgrids）是包围盒技术的延伸，它对包围盒内部的空间进行预处理。</p><ul><li>将包围盒拆分成统一的立体网格</li><li>将与物体发生重叠的网格进行标识</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-15.png?x-oss-process=image/resize,w_800" /></p><p>在判断光线与包围盒内部物体的交点时，会先遍历所有网格。当网格与光线有交点时，会继续判断网格内的物体是否与光线有交点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-16.png?x-oss-process=image/resize,w_800" /></p><p>关于网格的划分，不同的划分方法，加速效果存在差异。</p><ul><li>当只有一个网格时，没有加速效果。</li><li>当网格划分非常密集时，计算交点时遍历网格的开销会增大，甚至会出现性能降低的情况。</li></ul><p>相对而言，一个加速效果良好的网格划分经验是：网格的数量等于物体的数量乘以一个常数（经验值是27）。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-17.png?x-oss-process=image/resize,w_800" /></p><p>然而，统一网格划分并不是万能的，它只适用于一些物体分布均匀的场景。如下所示的场景中，在餐桌上方存在大量没有物体的空间，此时使用统一网格划分的效果并不好。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-18.png?x-oss-process=image/resize,w_800" /></p><h3 id="空间划分">空间划分</h3><p>空间划分（Spatialpartitions）主要是为了解决空间中物体分布不均匀的问题，其主要有<strong>Oct-Tree</strong>、<strong>KD-Tree</strong>、<strong>BSP-Tree</strong>等几种技术。</p><p>Oct-Tree，又称八叉树，其基本原理是把立方体分割成八等份，递归进行分割，直到格子为空或者物体足够少。</p><p>KD-Tree，与 Oct-Tree类似，区别在于每次只将格子一分为二，总是沿着有个轴进行分割。相比于Oct-Tree，其节点数量的复杂度不会随着维度而指数型增长。</p><p>BSP-Tree，对空间一分为二，每次选择一个方向进行分割。相比于KD-Tree，其切割的方向并不一定与轴平行。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-19.png?x-oss-process=image/resize,w_800" /></p><h4 id="kd-tree">KD-Tree</h4><p>这里我们着重介绍一下 KD-Tree 的工作原理。</p><p>首先，KD-Tree会对包围盒空间进行预处理，对包围盒空间进行二分，同时构建二叉树，如下所示。其中，非叶子节点表示二分之前的整体空间，它不会存储物体；叶子节点表示存储物体的真实空间。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-20.png?x-oss-process=image/resize,w_800" /></p><p>在进行交点判定时，本质上就是对二叉树进行先序遍历。对于上图中的例子，我们判定光线是否与整体的空间A 有交点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-22.png?x-oss-process=image/resize,w_800" /></p><p>显然，上述例子中光线与空间 A 存在交点，那么它会节点 A 的两个叶子节点1 和 B分别进行交点判定，如下所示。如果交点存在则进一步判断内部物体是否与光线有交点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-23.png?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-28.png?x-oss-process=image/resize,w_800" /></p><p>下图所示，当与空间 B 存在交点时，则进一步遍历空间 B 的子节点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-24.png?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-25.png?x-oss-process=image/resize,w_800" /></p><p>以此类推，空间 C与光线仍然存在交点，则继续遍历二叉树，直到判定光线与物体之间存在交点或不存在交点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-26.png?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-27.png?x-oss-process=image/resize,w_800" /></p><p>直观而言，KD-Tree 确实能够有效加速交点判定。但是 KD-Tree也存在一些无法解决的问题，比如：</p><ul><li>存在无法判定的例子，比如：三角形反包围了包围盒。</li><li>同一物体被划分在多个包围盒中，会有重复判断的问题。</li></ul><h3 id="物体划分">物体划分</h3><p>针对空间划分无法解决的问题，图形学采用物体划分来解决。下面，我们来介绍物体划分的典型技术——BVH。</p><h4 id="bvh">BVH</h4><p>BVH（Bounding Volume Hierarchy）的整体原理与 KD-Tree类似，唯一的区别在于对于空间划分的方式不同。KD-Tree是对空间进行划分，BVH 是对物体进行划分。</p><p>如下所示，空间中的物体彼此之间非常靠近。如果我们采用 KD-Tree的划分方式，那么一个物体可能会包含在多个空间中，进而存在重复判定的情况。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-29.png?x-oss-process=image/resize,w_800" /></p><p>对此，BVH对物体进行划分，并重新计算包围盒，形成根节点的子节点。以此类推，最终在所有叶子节点存储物体列表。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-30.png?x-oss-process=image/resize,w_800" /></p><h2 id="交点渲染原理">交点渲染原理</h2><p>当我们获取到光线直射、反射至物体表面的交点之后，我们可以进一步计算这些交点的着色，从而完成渲染。</p><h3 id="光线追踪的问题">光线追踪的问题</h3><p>关于经典光线追踪，即 Whitted-Style RayTracing，对光线作出了如下假设。</p><ul><li><strong>光线只进行镜面反射和折射</strong></li><li><strong>光线在漫反射面停止弹射</strong></li></ul><p>对于第一种情况，经典光线追踪只能渲染镜面反射，无法渲染磨砂反射，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-33.png?x-oss-process=image/resize,w_800" /></p><p>对于第二种情况，对于漫反射物体，光线转播至表面时会停止弹射，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-34.png?x-oss-process=image/resize,w_800" /></p><p>既然经典光线追踪是存在问题的，那么我们该如何对交点进行渲染呢？答案是渲染方程。</p><h3 id="渲染方程定义">渲染方程定义</h3><p>关于渲染，我们在 <ahref="http://chuquan.me/2024/04/20/foundation-of-computer-graphic-07/">上一篇文章</a>中介绍了辐射度量学，文中我们提到了渲染方程，如下所示。</p><span class="math display">\[\begin{aligned}L_r(p, {\omega}_r) = L_e(p, {\omega}_r) + \int_{H^2}L_i(p, {\omega}_i)f_r(p, {\omega}_i, {\omega}_r) (n \cdot {\omega}_i) d{\omega}_i\end{aligned}\]</span><h3 id="渲染方程展开">渲染方程展开</h3><p>渲染方程经过一系列变换可以简化成如下表达式。</p><span class="math display">\[\begin{aligned}l(u) = e(u) + \int l(v) K(u, v) dv\end{aligned}\]</span><p>我们将 <span class="math inline">\(K\)</span>作为反射操作符，可以进一步将简化为如下表达式，<spanclass="math inline">\(L\)</span> 是一个递归项。</p><span class="math display">\[\begin{aligned}L = E + KL\end{aligned}\]</span><p>我们的目的是求解 <span class="math inline">\(L\)</span>，前面说 <spanclass="math inline">\(K\)</span>是一个反射操作符，其实我们也可以将其理解为反射次数，反射次数越多，展开项越多，比如：<spanclass="math inline">\(K^2\)</span> 表示光在空间中弹射两次。</p><span class="math display">\[\begin{aligned}L = &amp; E + KL\\IL - KL = &amp; E\\(I - K)L = &amp; E\\L = &amp; (I - K)^{-1}E\\L = &amp; (I + K + K^2 + K^3 + ...)E\\L = &amp; E + KE + K^{2}E + K^{3}E + ...\\\end{aligned}\]</span><p>我们可以非常容易地理解展开后的渲染方程中的各个项，如下所示。全局光照由<strong>光源</strong>、<strong>直接光照</strong>、<strong>间接光照</strong>组合而成，其中光栅化只包含了光源和直接光照两部分，难以实现间接光照；光线追踪则包含了全局光照。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-31.png?x-oss-process=image/resize,w_800" /></p><p>下图所示，展示了全局光照包含不同数量的光照项的渲染对比效果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-32.png?x-oss-process=image/resize,w_800" /></p><h3 id="求解理论基础">求解理论基础</h3><p>关于如何求解渲染方程，我们首先介绍两部分与之相关的内容。</p><ul><li>概率论基础</li><li>蒙特卡洛积分</li></ul><h4 id="概率论基础">概率论基础</h4><p>这里我们主要介绍概率、概率分布函数、期望等几个概念。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-35.png?x-oss-process=image/resize,w_800" /></p><p>上图所示是一个的连续的概率分布曲线 <span class="math inline">\(X \simp(x)\)</span>，横坐标表示目标值 <spanclass="math inline">\(x\)</span>，纵坐标表示目标值为 <spanclass="math inline">\(x\)</span> 的概率 <spanclass="math inline">\(p(x)\)</span>，即<strong>概率分布函数</strong>（Probability DistributionFunction，PDF），那么我们可以得到如下两个关系式：</p><ul><li>所有取值的概率之和为 <span class="math inline">\(1\)</span></li><li>期望为概率与目标值乘积的积分 <spanclass="math inline">\(E[X]\)</span></li></ul><span class="math display">\[\begin{aligned}\int p(x) dx = &amp; 1; &amp;&amp;  p(x) \geq 0\\E[X] = &amp; \int x p(x) dx\end{aligned}\]</span><h4 id="蒙特卡洛积分">蒙特卡洛积分</h4><p>假设我们希望求解如下所示的不定积分，那么该如何求解？答案是蒙特卡洛积分。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-36.png?x-oss-process=image/resize,w_800" /></p><p>蒙特卡洛积分（Monte CarloIntegration）的基本思想是：在积分域内不断采样 <spanclass="math inline">\(f(x)\)</span>，不断地与 <spanclass="math inline">\(ab\)</span>构成一个个长方形，然后对所有的长方形的面积之和求平均值。由此我们可以得到离散形式的蒙特卡洛方程，如下所示。</p><span class="math display">\[\begin{aligned}F_N = \frac{1}{N} \sum^{1}_{i=1} \frac{f(x_i)}{p(x_i)}\end{aligned}\]</span><p>我们将其进一步表示为更加通用的连续形式的蒙特卡洛方程，如下所示。</p><span class="math display">\[\begin{aligned}\int_{a}^{b} f(x) dx = \frac{1}{N} \sum_{i=1}^{N} \frac{f(x_i)}{p(x_i)}\end{aligned}\]</span><p>对于蒙特卡洛积分，当 <span class="math inline">\(N\)</span>越大，求解的结果越精准。</p><h3 id="渲染方程求解">渲染方程求解</h3><p>在了解了概率论基础和蒙特卡洛积分之后，我们来正式求解渲染方程。</p><h4 id="求解过程分析">求解过程分析</h4><p>假设，我们只考虑一个像素点的着色过程，如下图所示。其中，<spanclass="math inline">\({\omega}_o\)</span>为观测方向，即从着色点到观测点的方向；<spanclass="math inline">\({\omega}_i\)</span>为光线入射方向，即从光源到着色点的方向。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-37.png?x-oss-process=image/resize,w_800" /></p><p>同时，我们先忽略着色点本身的发光项，那么可以得到一个简化的渲染方程，如下所示。</p><span class="math display">\[\begin{aligned}L_o(p, {\omega}_o) = \int_{H^2} L_i(p, {\omega}_i) f_r(p, {\omega}_i,{\omega}_o) (n \cdot {\omega}_i) d{\omega}_i\end{aligned}\]</span><p>我们将渲染等价为在半球面上进行采样。因此，任意一点的采样概率 <spanclass="math inline">\(p({\omega}_i)\)</span> 为 <spanclass="math inline">\(1/2\pi\)</span>，即概率分布函数（PDF）。对应，采样值<span class="math inline">\(f({\omega}_i)\)</span> 为 <spanclass="math inline">\(L_i(p, {\omega}_i) f_r(p, {\omega}_i, {\omega}_o)(n \cdot{\omega}_i)\)</span>。由此可以用蒙特卡洛积分来求解渲染方程，如下所示。</p><span class="math display">\[\begin{aligned}L_o(p, {\omega}_o) = &amp;\int_{H^2}L_i(p, {\omega}_i) f_r(p, {\omega}_i, {\omega}_o) (n \cdot{\omega}_i) d{\omega}_i\\\approx &amp;\frac{1}{N} \sum_{i=1}^{N} \frac{f({\omega}_i)}{p({\omega}_i)}\\\approx &amp;\frac{1}{N} \sum_{i=1}^{N} \frac{L_i(p, {\omega}_i) f_r(p, {\omega}_i,{\omega}_o) (n \cdot {\omega}_i) }{p({\omega}_i)}\end{aligned}\]</span><p>根据上述的渲染方程的求解关系式，我们可以实现对应的伪代码，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-38.png?x-oss-process=image/resize,w_800" /></p><p>上述，我们只处理了光的一次弹射，而全局光照还包括二次弹射、三次弹射...我们该如何处理呢？</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-39.png?x-oss-process=image/resize,w_800" /></p><p>上图所示，我们希望计算 Q 点反射到 P点的光线。对此，我们可以转换一下思路：Q 点反射到 P 点的光线，不就等于从P 点观测 Q 点，Q点的着色吗？于是全局光照就转换成了一个递归着色的过程。如下所示，我们加入了处理全局光照的递归着色。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-40.png?x-oss-process=image/resize,w_800" /></p><p>至此，着色算法的整体框架已经实现了，但是还存在两个问题：</p><ul><li>分治爆炸问题</li><li>无限递归问题</li></ul><h4 id="分治爆炸问题">分治爆炸问题</h4><p>在上述算法实现中，对于每一个点的着色，我们都会采样各个方向，而每一个方向所经过的反射点又会采样各个方向。如果我们对于每个点采样100 个方向，那么在下一次递归中将会采样 10000个方向，依次类推，产生分治爆炸问题。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-41.png?x-oss-process=image/resize,w_800" /></p><p>那么如何解决这个问题？很显然，只有当采样数量为 1时，递归时才不会出现分治爆炸问题。在计算机图形学中，对于每个着色点，只通过一条射线进行光线追踪的方式被称为<strong>路径追踪</strong>（Path Tracing）。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-42.png?x-oss-process=image/resize,w_800" /></p><p>如下所示，为解决了分支爆炸问题的着色算法。我们不再采样各个方向，而是随机采样一个方向，进而计算着色。然而，我们知道在蒙特卡洛积分中，采样数越小，准确性越低。算法中将采样数降至1，很显然，会出现很大的噪声。那么这又该如何解决呢？</p><p>对此，有一种方法另辟蹊径：<strong>每个像素生成多条光线，进行多次路径追踪，并求解平均值</strong>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-43.png?x-oss-process=image/resize,w_800" /></p><p>如下所示，我们定义了一个 <code>ray_generation</code>方法，它以相机位置和像素点作为参数，内部对这个像素点进行多次路径追踪，即调用<code>shade</code> 方法，最终求解平均值。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-44.png?x-oss-process=image/resize,w_800" /></p><h4 id="无限递归问题">无限递归问题</h4><p>对于无限递归问题，很显然，我们必须要找到一个停止递归的策略。对此，图形学借鉴了<strong>俄罗斯轮盘赌</strong>（Russion Roulette，RR）的思想。</p><p>俄罗斯轮盘赌的基本原理是设定一个概率值 <span class="math inline">\(0&lt; P &lt; 1\)</span>。</p><ul><li>射出光线的概率为 <spanclass="math inline">\(P\)</span>，由此返回的着色结果为 <spanclass="math inline">\(L_0/P\)</span></li><li>不射出光线的概率为 <spanclass="math inline">\(1-P\)</span>，由此返回的着色结果为 <spanclass="math inline">\(0\)</span></li></ul><p>使用这种方式，我们仍然可以期望得到 <spanclass="math inline">\(L_0\)</span>，如下所示为俄罗斯轮盘赌的期望公式.</p><span class="math display">\[\begin{aligned}E = P * (L_0 / P) + (1 - P) * 0 = L_0\end{aligned}\]</span><p>于是，我们可以进一步优化着色算法，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-45.png?x-oss-process=image/resize,w_800" /></p><h4 id="光源采样问题">光源采样问题</h4><p>经过上述改进，着色算法是正确了，但是它并不高效。如下所示，像素采样必须达到一定阈值才能得到相对高质量的结果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-46.png?x-oss-process=image/resize,w_800" /></p><p>这里的根本原因在于均匀采样，其中只有极少数方向能够打到光源，大多数方向则浪费掉了。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-47.png?x-oss-process=image/resize,w_800" /></p><p>对此，我们可以考虑直接对光源进行采样。考虑到我们在求解蒙特卡洛积分时是在立体角进行采样，因此我们将空间中的面光源投影到球面上，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-48.png?x-oss-process=image/resize,w_800" /></p><p>我们得到立体角和光源微分面积的关系如下所示。</p><span class="math display">\[\begin{aligned}d\omega = \frac{dA cos{\theta}&#39;}{||x&#39; - x||^2}\end{aligned}\]</span><p>然后，我们微分立体角代入渲染方程，如下所示。</p><span class="math display">\[\begin{aligned}L_o(p, {\omega}_o) = &amp;\int_{H^2}L_i(p, {\omega}_i) f_r(p, {\omega}_i, {\omega}_o) cos\thetad{\omega}_i\\= &amp;\int_{H^2} L_i(p, {\omega}_i) f_r(p, {\omega}_i, {\omega}_o)\frac{cos\theta cos{\theta}&#39;}{||x&#39; - x||^2} dA\end{aligned}\]</span><p>现在，我们认为着色结果来源于两部分：</p><ul><li>光源的直接作用：直接采样光源，无需俄罗斯轮盘赌</li><li>光源的间接作用：反射、漫反射，需要俄罗斯轮盘赌</li></ul><p>由此，我们结合两部分得到如下所示的优化算法。当然，对于光源的直接作用，我们需要判断光源与着色点之间是否存在遮挡，这一点一定要注意。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/ray-tracing-49.png?x-oss-process=image/resize,w_800" /></p><h1 id="总结">总结</h1><p>本文我们主要介绍了光线追踪的原理。首先，我们我们介绍了光线追踪的基本原理，其主要包括：视线生成和像素着色两个阶段。</p><p>然后，我们介绍了经典光线追踪技术，也称为 Whitted-Style光线追踪，其引入了间接光照的处理。</p><p>在光线追踪中，主要设计两个关键技术，分别是交点判定和交点渲染。对于交点判定，我们介绍了包围盒原理、统一网格、空间划分、物体划分等技术；对于交点渲染，我们在经典光线追踪的基础上引入了漫反射的处理，整体围绕渲染方程进行展开，使用蒙特卡洛积分进行求解。</p><p>在实现渲染方程算法的过程中，我们遇到了分治爆炸、无限递归、光源采样等问题，对此我们也依次进行了处理。最终实现了一个完整的着色算法。</p><h1 id="参考">参考</h1><ol type="1"><li>《GAMES 101》</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;之前我们介绍了在游戏领域中广泛使用的实时渲染技术——光栅化，本文我们来介绍一下在特效领域中广泛使用的离线渲染技术——光线追踪。&lt;/p&gt;</summary>
    
    
    
    <category term="图形学" scheme="http://chuquan.me/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="Whitted-Stype Ray Tracing" scheme="http://chuquan.me/tags/Whitted-Stype-Ray-Tracing/"/>
    
    <category term="交点判定" scheme="http://chuquan.me/tags/%E4%BA%A4%E7%82%B9%E5%88%A4%E5%AE%9A/"/>
    
    <category term="交点渲染" scheme="http://chuquan.me/tags/%E4%BA%A4%E7%82%B9%E6%B8%B2%E6%9F%93/"/>
    
    <category term="包围盒" scheme="http://chuquan.me/tags/%E5%8C%85%E5%9B%B4%E7%9B%92/"/>
    
    <category term="轴对齐包围盒" scheme="http://chuquan.me/tags/%E8%BD%B4%E5%AF%B9%E9%BD%90%E5%8C%85%E5%9B%B4%E7%9B%92/"/>
    
    <category term="KD-Tree" scheme="http://chuquan.me/tags/KD-Tree/"/>
    
    <category term="BVH" scheme="http://chuquan.me/tags/BVH/"/>
    
    <category term="蒙特卡洛积分" scheme="http://chuquan.me/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%A7%AF%E5%88%86/"/>
    
    <category term="路径追踪" scheme="http://chuquan.me/tags/%E8%B7%AF%E5%BE%84%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="光线追踪" scheme="http://chuquan.me/tags/%E5%85%89%E7%BA%BF%E8%BF%BD%E8%B8%AA/"/>
    
    <category term="俄罗斯轮盘赌" scheme="http://chuquan.me/tags/%E4%BF%84%E7%BD%97%E6%96%AF%E8%BD%AE%E7%9B%98%E8%B5%8C/"/>
    
  </entry>
  
  <entry>
    <title>计算机图形学基础（7）——辐射度量学</title>
    <link href="http://chuquan.me/2024/04/20/foundation-of-computer-graphic-07/"/>
    <id>http://chuquan.me/2024/04/20/foundation-of-computer-graphic-07/</id>
    <published>2024-04-20T14:11:09.000Z</published>
    <updated>2024-04-20T14:13:37.201Z</updated>
    
    <content type="html"><![CDATA[<p>在介绍光线追踪之前，我们先来学习一下其所涉及的重要内容——辐射度量学。由于该内容相对独立，这里单开一篇文章来进行介绍。</p><span id="more"></span><h1 id="概述">概述</h1><p>本质上，辐射度量学是在物理层面准确定义光照的方法。在 <ahref="http://chuquan.me/2024/04/05/foundation-of-computer-graphic-05/">《计算机图形学基础（5）——着色》</a>一文中我们提到了光照强度，我们将半径为 <spanclass="math inline">\(r\)</span> 时，对应的球面的一个点的光照强度为<spanclass="math inline">\(I\)</span>，那么具体它的单位是什么呢？在了解辐射度量学之后，这个疑问就能得到解答。</p><p>在辐射度量学中涉及了很多相关术语，这里我将它们分为两部分进行介绍。</p><h1 id="几何">几何</h1><p>首先我们来看几何的相关术语，主要包含以下几个：</p><ul><li><strong>角（Angles）</strong></li><li><strong>立体角（Solid Angles）</strong></li><li><strong>微分立体角（Differential Solid Angles）</strong></li></ul><h2 id="角">角</h2><p>在平面几何中，角的定义是弧长与半径的比值，其中 <spanclass="math inline">\(\theta\)</span> 表示角，<spanclass="math inline">\(l\)</span> 表示弧长，<spanclass="math inline">\(r\)</span> 表示圆的半径，如下所示。圆自身的角度为<span class="math inline">\(2\pi\)</span>。</p><span class="math display">\[\begin{aligned}\theta = \frac{l}{r}\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-01.png?x-oss-process=image/resize,w_800" /></p><h2 id="立体角">立体角</h2><p>在立体几何中，立体角的定义则是弧面积与半径平方的比值，其中 <spanclass="math inline">\(\Omega\)</span> 表示立体角，<spanclass="math inline">\(A\)</span> 表示弧面积，<spanclass="math inline">\(r\)</span>表示球的半径，如下所示。球自身的立体角为 <spanclass="math inline">\(4\pi\)</span>。立体角主要用于描述空间中的一个锥体的张开角度。</p><span class="math display">\[\begin{aligned}\Omega = \frac{A}{r^2}\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-02.png?x-oss-process=image/resize,w_800" /></p><h2 id="微分立体角">微分立体角</h2><p>微分立体角通过两个角度来定义球面上的一个唯一的方向，如下所示。其中，两个角度分别是：</p><ul><li>与 <span class="math inline">\(z\)</span> 轴形成的夹角 <spanclass="math inline">\(\theta\)</span></li><li>绕 <span class="math inline">\(z\)</span> 轴形成的夹角 <spanclass="math inline">\(\phi\)</span></li></ul><p>计算微分立体角，我们首先两个角度在球面所形成方向所占据的单位面积<spanclass="math inline">\(dA\)</span>。按照微分的思想，我们认为该面积等同于矩形的面积，因此我们要计算它的长和宽，分别为<span class="math inline">\(rsin{\theta}d\phi\)</span> 和 <spanclass="math inline">\(rd\theta\)</span>，由此可以计算该单位面积。从而进一步求解微分立体角<span class="math inline">\(d\omega\)</span>，如下所示。</p><span class="math display">\[\begin{aligned}dA = &amp;(r d\theta)(r sin{\theta} d\phi) = r^2 sin\theta d\theta d\phi\\d\omega = &amp;\frac{dA}{r^2} = sin\theta d\theta d\phi\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-03.png?x-oss-process=image/resize,w_800" /></p><p>在辐射度量学中，我们通常用 <spanclass="math inline">\(\omega\)</span>来表示三维空间中的一个方向。我们可以使用 <spanclass="math inline">\(\theta\)</span> 和 <spanclass="math inline">\(\phi\)</span> 来确定其方向，并且还可以结合 <spanclass="math inline">\(d\theta\)</span> 和 <spanclass="math inline">\(d\phi\)</span> 来计算其微分立体角。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-04.png?x-oss-process=image/resize,w_800" /></p><h1 id="光学物理">光学物理</h1><p>在辐射度量学中，主要涉及以下几个光学物理相关术语。</p><ul><li><strong>辐射能量（Radiant Energy）</strong></li><li><strong>辐射通量（Radiant Flux）</strong></li><li><strong>辐射强度（Radiant Intensity）</strong></li><li><strong>辐射照度（Irradiance）</strong></li><li><strong>辐射亮度（Radiance）</strong></li></ul><h2 id="辐射能量">辐射能量</h2><p>辐射能量（Radiant Energy）表示电磁辐射的能量，用符号 <spanclass="math inline">\(Q\)</span> 表示，以 <strong>焦耳</strong>（J =Joule）为单位。</p><span class="math display">\[\begin{aligned}Q\\unit: [J = Joule]\end{aligned}\]</span><h2 id="辐射通量">辐射通量</h2><p>辐射通量（RadiantFlux）表示单位时间内流通（发射、反射、传输、接收）的能量，用符号 <spanclass="math inline">\(\Phi\)</span> 表示，以 <strong>瓦特</strong>（W =Watt）或 <strong>流明</strong>（lm =lumen）为单位。<strong>在实际应用中，我们经常将辐射通量称为能量，本质上是因为辐射通量可以结合时间快速计算出辐射能量</strong>。</p><span class="math display">\[\begin{aligned}\Phi = \frac{dQ}{dt}\\unit: [W = Watt]/[lm = lumen]\end{aligned}\]</span><p>在光学物理中，我们也将辐射通量定义为<strong>单位时间辐射出光子的数量</strong>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-05.png?x-oss-process=image/resize,w_800" /></p><h2 id="辐射强度">辐射强度</h2><p>辐射强度（Radiant Intensity）表示<strong>单位立体角的辐射通量</strong>。其中 <spanclass="math inline">\(\Phi\)</span> 表示辐射通量，<spanclass="math inline">\(\omega\)</span>表示立体角。我们可以用辐射强度描述一束光线，即形成一个锥体角度的光线，在单位时间内流通的能量。</p><span class="math display">\[\begin{aligned}I(\omega) = \frac{d\Phi}{d\omega}\\unit:[\frac{W}{sr}][\frac{lm}{sr} = cd = candela]\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-06.png?x-oss-process=image/resize,w_800" /></p><h2 id="辐射照度">辐射照度</h2><p>辐射照度（Irradiance）表示<strong>单位（正交）面积的辐射通量</strong>。注意，这里计算的辐射通量必须与面垂直，如果光照不垂直于面，则计算其垂直分量。其中<span class="math inline">\(\Phi\)</span> 表示辐射通量，<spanclass="math inline">\(A\)</span> 表示正交面积，$$ 表示位置。</p><span class="math display">\[\begin{aligned}E(p) = \frac{d\Phi(p)}{dA}\\unit: [\frac{W}{m^2}][\frac{lm}{m^2} = lux]\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-07.png?x-oss-process=image/resize,w_800" /></p><p>此时，我们回顾 <ahref="http://chuquan.me/2024/04/05/foundation-of-computer-graphic-05/">《计算机图形学基础（5）——着色》</a>文中的兰伯特余弦定理，可以计算辐射照度，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-08.png?x-oss-process=image/resize,w_800" /></p><p>同样，我们还可以计算空间中一个点光源在空间中辐射的衰减到底是什么？如下所示，其实我们可以发现辐射照度随着半径增加而指数级衰减，辐射强度并没有发生变化。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-09.png?x-oss-process=image/resize,w_800" /></p><h2 id="辐射亮度">辐射亮度</h2><p>辐射亮度（Radiance）表示<strong>单位立体角、单位（正交）面积的辐射通量</strong>。辐射亮度一般用于描述光线在传输过程中的属性。我们可以用辐射亮度来描述某个面光源在某个（锥形）方向的辐射通量。</p><p>从定义上看，辐射亮度在立体角、正交面积两个维度做了两次微分。其中，<spanclass="math inline">\(cos\theta\)</span>表示光线在单位面积垂直方向上的角度分量，<spanclass="math inline">\(p\)</span> 表示位置，<spanclass="math inline">\(\omega\)</span> 表示立体角。</p><span class="math display">\[\begin{aligned}L(p, \omega) = \frac{d^2\Phi(p, \omega)}{d{\omega}dA cos\theta}\\unit: [\frac{W}{sr m^2}][\frac{cd}{m^2} = \frac{lm}{sr m^2} = nit]\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-10.png?x-oss-process=image/resize,w_800" /></p><p>下面，我们回顾一下辐射强度和辐射照度的定义。</p><ul><li>辐射强度：单位立体角的辐射通量。</li><li>辐射照度：单位（正交）面积的辐射通量</li></ul><p>结合辐射强度和辐射照度的定义，我们可以将辐射亮度的定义进行转换，<strong>单位立体角的辐射照度</strong>或<strong>单位（正交）面积的辐射强度</strong>。这两种定义正好可以应用到两种不同方向的辐射亮度，分别是<strong>入射辐射亮度</strong> 和 <strong>出射辐射亮度</strong>。</p><h3 id="入射辐射亮度">入射辐射亮度</h3><p>入射辐射亮度（IncidentRadiance）即单位立体角的辐射照度。我们可以理解从某个方向向一个面进行辐射，该面所接收的辐射照度，如下图所示。如果我们进一步考虑来自四面八方的辐射，比如环境光，那么可以计算得到这个面所接收的全部辐射照度。</p><span class="math display">\[\begin{aligned}L(p, \omega) = \frac{dE{p}}{d{\omega} cos\theta}\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-11.png?x-oss-process=image/resize,w_800" /></p><h3 id="出射辐射亮度">出射辐射亮度</h3><p>出射辐射亮度（ExitingRadiance）即单位（正交）面积的辐射强度。我们可以理解一个面向某个方向进行辐射，该方向所发射的辐射强度，如下所示。如果我们进一步考虑一个面向四面八方的辐射，那么可以计算得到这个面所发射的全部辐射强度。</p><span class="math display">\[\begin{aligned}L(p, \omega) = \frac{dI(p, \omega)}{dA cos\theta}\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-10.png?x-oss-process=image/resize,w_800" /></p><h2 id="辐射照度-vs.-辐射亮度">辐射照度 vs. 辐射亮度</h2><p>在图形学中，辐射照度（Irradiance）和辐射亮度（Radiance）用的非常多。在下图所示的场景中，辐射照度表示<span class="math inline">\(dA\)</span> 接收的所有能量；辐射亮度表示<span class="math inline">\(dA\)</span>从某个方向接收的能量。相比而言，辐射亮度是一个更细粒度的分析属性。这样，我们就把辐射照度和辐射亮度联系起来了。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-12.png?x-oss-process=image/resize,w_800" /></p><p>对于上图所示的场景，我们可以计算得到某个位置的辐射照度，如下所示，其中<span class="math inline">\(H^2\)</span> 表示单位半球面积。</p><span class="math display">\[\begin{aligned}dE(p, \omega) = &amp;L_{i}(p, \omega) cos\theta d\omega\\E(p) = &amp;\int_{H^2} L_{i}(p, \omega) cos\theta d{\omega}\end{aligned}\]</span><h1 id="应用">应用</h1><h2 id="双反射分布函数">双反射分布函数</h2><p>已知入射光线的能量和角度，当辐射到物体表面后，光线会向各个角度反射，且各个角度反射的能量是不同的。双反射分布函数（BidirectionalReflectance DistributionFunction，BRDF）就是用于计算这种场景下各个角度的反射能量。</p><p>如下所示，是一个入射和反射的场景，我们从辐射度量学的角度分别进行分析。</p><ul><li>入射阶段：当入射光线从 <spanclass="math inline">\({\omega}_i\)</span>角度辐射到物体表面的一个位置时，可以认为该位置吸收了所有的光线能量，用辐射照度来表示，即<span class="math inline">\(dE({\omega}_i)\)</span>。</li><li>反射阶段：可以认为该位置将光线能量向各个方向进行辐射。对于方向 <spanclass="math inline">\({\omega}_i\)</span>的光线能量，用辐射亮度来表示，即 <span class="math inline">\(dL_r(x,{\omega}_r)\)</span>。</li></ul><span class="math display">\[\begin{aligned}dE({\omega}_i) = &amp;L({\omega}_i) cos{\theta}_i d{\omega}_i\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-13.png?x-oss-process=image/resize,w_800" /></p><p>本质上，BRDF 就是表示由入射方向 <spanclass="math inline">\({\omega}_i\)</span> 向反射方向 <spanclass="math inline">\({\omega}_r\)</span> 辐射能量的比例函数 <spanclass="math inline">\(fr({\omega}_i \rightarrow{\omega}_r)\)</span>，其定义如下所示。</p><span class="math display">\[\begin{aligned}BRDF: f_r({\omega}_i \rightarrow {\omega}_r) =\frac{dL_r({\omega}_r)}{dE_i({\omega}_i)} =\frac{dL_r({\omega}_r)}{L_i({\omega}_i) cos{\theta}_i d{\omega}_i}\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-14.png?x-oss-process=image/resize,w_800" /></p><p>在实际应用中，对于镜面反射，BRDF定义反射方向包含所有能量，其他方向均为 0；对于漫反射，BRDF定义所有方向的能量分布相同。事实上，BRDF就是描述了光线和物体表面的作用，也就是决定了物体的材质。</p><h2 id="反射方程">反射方程</h2><p>基于 BRDF，我们可以进一步推导出 <strong>反射方程</strong>（TheReflection Equation）。</p><p>由于 BRDF 定义了一个入射方向 <spanclass="math inline">\({\omega}_i\)</span> 向一个反射方向 <spanclass="math inline">\({\omega}_r\)</span>反射时的能量比例。那么，我们可以通过积分计算所有方向对一个反射方向 <spanclass="math inline">\({\omega}_r\)</span>的能量聚合。因此，我们可以推导得出如下所示的反射方程。</p><span class="math display">\[\begin{aligned}L_r(p, {\omega}_r) = \int_{H^2}f_r(p, {\omega}_i \rightarrow {\omega}_r)L_i(p, {\omega}_i) cos{\theta}_i d{\omega}_i\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/radiometry-14.png?x-oss-process=image/resize,w_800" /></p><h2 id="渲染方程">渲染方程</h2><p>基于反射方程，我们还可以进一步推导出 <strong>渲染方程</strong>（TheRendering Equation）。</p><p>这里我们只需要额外考虑物体本身会发光的情况，于是增加一个发光项即可得到渲染方程，如下所示。其中，<spanclass="math inline">\(cos{\theta}_i\)</span>转换成了法向量和入射方向的点积 <span class="math inline">\(n \cdot{\omega}_i\)</span>。现代图形学中，所有基于表面反射的渲染都是基于该渲染方程实现的。</p><span class="math display">\[\begin{aligned}L_r(p, {\omega}_r) = L_e(p, {\omega}_r) + \int_{H^2} L_i(p, {\omega}_i)f_r(p, {\omega}_i, {\omega}_r)(n \cdot {\omega}_i) d{\omega}_i\end{aligned}\]</span><h1 id="总结">总结</h1><p>本文我们主要介绍了辐射度量学中的几个概念：辐射能量、辐射通量、辐射强度、辐射照度、辐射亮度等。基于这些概念，我们引入了BRDF 的定义。然后依次推导出反射方程和渲染方程。</p><h1 id="参考">参考</h1><ol type="1"><li>《GAMES 101》</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;在介绍光线追踪之前，我们先来学习一下其所涉及的重要内容——辐射度量学。由于该内容相对独立，这里单开一篇文章来进行介绍。&lt;/p&gt;</summary>
    
    
    
    <category term="图形学" scheme="http://chuquan.me/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="Radiance" scheme="http://chuquan.me/tags/Radiance/"/>
    
    <category term="Irradiance" scheme="http://chuquan.me/tags/Irradiance/"/>
    
    <category term="Intensity" scheme="http://chuquan.me/tags/Intensity/"/>
    
    <category term="立体角" scheme="http://chuquan.me/tags/%E7%AB%8B%E4%BD%93%E8%A7%92/"/>
    
    <category term="微分立体角" scheme="http://chuquan.me/tags/%E5%BE%AE%E5%88%86%E7%AB%8B%E4%BD%93%E8%A7%92/"/>
    
  </entry>
  
  <entry>
    <title>计算机图形学基础（6）——几何</title>
    <link href="http://chuquan.me/2024/04/10/foundation-of-computer-graphic-06/"/>
    <id>http://chuquan.me/2024/04/10/foundation-of-computer-graphic-06/</id>
    <published>2024-04-10T15:10:24.000Z</published>
    <updated>2024-04-11T01:02:20.747Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们介绍了观测变换、光栅化、着色等几个图形学中比较复杂的主题，本文我们稍微放松一下，介绍一个相对比较简单的主题——几何。</p><span id="more"></span><h1 id="几何表示">几何表示</h1><p>通过图形学建模表示现实生活中的各种物体，要解决的第一个问题就是如何定义物体形状，而这就涉及到了几何。</p><p>物体的形状非常多，那么如何通过几何方法表示物体呢？对此，图形学中定义了两种几何表示方法：</p><ul><li><strong>隐式几何表示</strong>（Implicit Representations ofGeometry）</li><li><strong>显示几何表示</strong>（Explicit Representations ofGeometry）</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-01.png?x-oss-process=image/resize,w_800" /></p><h2 id="隐式几何表示">隐式几何表示</h2><p>隐式几何表示是一种 <strong>使用数学关系式来描述几何形状</strong>的方法，而不是直接描述其顶点和边界等元素。在隐式几何表示法中，几何形状被定义为方程的解集，即满足某些条件的一组点的集合。比如，下面的关系式定义了一个圆环结构。</p><span class="math display">\[\begin{aligned}f(x, y, z) = (2 - \sqrt{x^2 + y^2})^2 + z^2 - 1\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-02.png?x-oss-process=image/resize,w_800" /></p><p>隐式几何表示的常用技术有以下这些：</p><ul><li><strong>代数曲面</strong>（Algebraic Surface）</li><li><strong>构造实体几何</strong>（Constructive Solid Geometry）</li><li><strong>距离函数</strong>（Distance Function）</li><li><strong>水平集</strong>（Level Set）</li><li><strong>分形</strong>（Fractals）</li></ul><p>下面，我们来介绍一下这些常用的隐式几何表示技术。</p><h3 id="代数曲面">代数曲面</h3><p>代数曲面是通过一组参数方程定义的曲线和表面。它适用于一些简单的，可以使用数学关系式表示的几何体。下图所示，这些几何体就比较适合使用代数曲面来表示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-04.png?x-oss-process=image/resize,w_800" /></p><h3 id="构造实体几何">构造实体几何</h3><p>构造实体几何是通过布尔运算来组合不同的几何体。下图所示，一些复杂的几何体可以通过简单的几何体来组合构造。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-05.png?x-oss-process=image/resize,w_800" /></p><h3 id="距离函数">距离函数</h3><p>距离函数描述空间中任何一个点到几何体表面的最小距离。一种特殊的距离函数，符号距离函数（SignedDistanceFunction），其以空间中任意一个点作为输入，根据距离函数的返回值，可以进行判断：</p><ul><li>当距离函数的值大于 0，表示点在几何体外部</li><li>当距离函数的值小于 0，表示点在几何体内部</li><li>当距离函数的值等于 0，表示点在几何体表面</li></ul><h3 id="水平集">水平集</h3><p>对于表面规则的几何体，我们可以使用距离函数来表示；对于表面复杂的几何体，距离函数难以适用，此时，我们可以使用水平集来表示。</p><p>水平集的核心思想与距离函数一样，区别在于：距离函数使用通过输入空间点来计算该点到几何体表面的距离，水平集则存储了一系列距离值，我们可以通过插值法找到距离为0 的位置，拟合出一条曲面用于表示几何体的表面。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-06.png?x-oss-process=image/resize,w_800" /></p><h3 id="分形">分形</h3><p>分形，类似于递归，即局部和整体的形状相似，如下图所示。分形通过迭代函数系统（IFS）来生成。IFS是一种迭代的过程，该过程将函数反复应用于某个起始点或起始数据。这些函数通常是缩放、旋转、平移等操作，同时保持自相似性。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-07.png?x-oss-process=image/resize,w_800" /></p><!--缺点：难以判断哪些点属于关系式--><!--优点：容易判断点是否在几何的表面--><h2 id="显示几何表示">显示几何表示</h2><p>显式几何表示是一种<strong>直接或间接（通过参数映射的方式）定义点、线、面等元素集合</strong>的方法。在显式几何表示中，各元素的位置通常由坐标值直接给出，各元素之间的关系通常由数据结构来表示。比如，下面的关系式通过参数映射的方式间接定义了点的集合。</p><span class="math display">\[\begin{aligned}f: R^2 \rightarrow &amp;R^3\\(u, v) \rightarrow &amp;(x, y, z)\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-03.png?x-oss-process=image/resize,w_800" /></p><p>显式几何表示的常用技术有以下这些：</p><ul><li><strong>点云</strong>（Point Cloud）</li><li><strong>网格模型</strong>（Polygon Mesh）</li></ul><p>下面，我们来介绍一下这两种显式几何表示技术。</p><!--缺点：难以判断是否在内部或外部--><h3 id="点云">点云</h3><p>点云是显式几何表示中最简单的技术，其核心思想是使用大量的点来表示几何体的表面。点的密度越高，几何体的精度越高。由于点云的缺点很明显，内存占用大，因此一般会被再次转换成多边形网格。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-08.png?x-oss-process=image/resize,w_800" /></p><h3 id="多边形网格">多边形网格</h3><p>多边形网格是图形学中最常用的几何表示方法，它存储点和多边形（一般是三角形或四边形），这种形式非常容易处理、模拟、采样。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-09.png?x-oss-process=image/resize,w_800" /></p><p>在 3D 建模中，我们经常会用到 <code>.obj</code>格式的模型文件，其本质上是一个文本文件，记录了顶点、法线、纹理坐标、连接关系，由此构成几何体的形状。如下所示，是一个立方体结构的表示。</p><ul><li><code>v</code> 表示顶点</li><li><code>vn</code> 表示法线（多了两条是因为建模误差）</li><li><code>vt</code> 表示纹理坐标</li><li><code>f</code> 表示面，比如 <code>f 5/1/1 1/2/1 4/3/1</code>表示三角形面是由第 5、1、4 个顶点组成，三个点的纹理坐标是第 1、2、3对应的纹理坐标，面的法线是第 1 条法线。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-10.png?x-oss-process=image/resize,w_800" /></p><h1 id="曲线">曲线</h1><p>曲线（Curves）在图形学中应用非常广泛，比如：相机的拍摄路径、物体的移动路径、动画曲线、矢量字体等。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-11.png?x-oss-process=image/resize,w_800" /></p><h2 id="贝塞尔曲线">贝塞尔曲线</h2><p>贝塞尔曲线是通过一系列控制点进行定义的曲线。如下图所示，4个控制点定义了一条贝塞尔曲线，起始方向沿着 <spanclass="math inline">\(p_0p_1\)</span>，结束方向沿着 <spanclass="math inline">\(p_2p_3\)</span>，曲线不必经过所有控制点，但必须经过起始点和结束点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-12.png?x-oss-process=image/resize,w_800" /></p><!--特性：--><!--- 必须过起点和终点--><!--- 仿射不变性--><!--- 凸包性质--><h3 id="绘制算法">绘制算法</h3><p>那么控制点是如何影响曲线的呢？贝塞尔曲线绘制算法的原理是什么呢？</p><p>贝塞尔曲线的绘制算法是 <strong>De Casteljau'sAlgorithm</strong>，算法的基本思想是利用线性插值的原理，将高阶贝塞尔曲线转化为一阶贝塞尔曲线的组合。对于一个N阶贝塞尔曲线，首先构建一系列的二维点，然后在这些点上构建线段，以此类推，直到计算出贝塞尔曲线上的一个点。重复这个过程就可以得到贝塞尔曲线上的所有点，从而绘制出完整的贝塞尔曲线。</p><p>下面，我们以 3 个控制点绘制贝塞尔曲线的例子来进行介绍。</p><p>N 个控制点绘制的贝塞尔曲线，称为 <strong>N-1阶贝塞尔曲线</strong>。如下所示，我们定义了 3个控制点，由此绘制的贝塞尔曲线称之为<strong>二阶贝塞尔曲线</strong>（Quadratic Bezier）。对于这 3个控制点，我们首先对相邻控制点进行连线。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-13.png?x-oss-process=image/resize,w_800" /></p><p>我们定义一个变量 <code>t</code>，其值的范围为<code>[0, 1]</code>，作为算法的输入值。当 <code>t = 0</code>时，表示贝塞尔曲线起始点的输入值，当 <code>t = 1</code>时，表示贝塞尔曲线结束点的输入值。随后，我们在控制点所构成的各个连线上定义一个点，这个点的位置取决于<code>t</code> 的值，即一个比例值。比如：<spanclass="math inline">\(b_0b_1\)</span> 连线上定义点 <spanclass="math inline">\(b_{0}^{1}\)</span>，<spanclass="math inline">\(b_1b_2\)</span> 连线上定义点 <spanclass="math inline">\(b_{1}^{1}\)</span>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-14.png?x-oss-process=image/resize,w_800" /></p><p>然后，我们继续对 <span class="math inline">\(b_{0}^{1}\)</span> 和<span class="math inline">\(b_{1}^{1}\)</span>进行连线，并按照上述规则，在 <spanclass="math inline">\(b_{0}^{1}b_{1}^{1}\)</span> 连线上定义点 <spanclass="math inline">\(b_{0}^{2}\)</span>，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-15.png?x-oss-process=image/resize,w_800" /></p><p>当新定义的点只有一个时，我们可以将 <code>t</code> 的值逐步从<code>0</code> 变到 <code>1</code>。在这个过程中，<spanclass="math inline">\(b_{0}^{1}\)</span>、<spanclass="math inline">\(b_{0}^{2}\)</span>、<spanclass="math inline">\(b_{1}^{1}\)</span> 的位置都会随着 <code>t</code>的变化而变化。对于最终的贝塞尔曲线，我们只需要关注最后定义的点 <spanclass="math inline">\(b_{0}^{2}\)</span> 的路径即可，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-16.png?x-oss-process=image/resize,w_800" /></p><p>当我们扩展至更多控制点时，比如 4个控制点时，我们仍然按照上述规则来处理，将高阶贝塞尔曲线转化为一阶贝塞尔曲线的组合，最终绘制曲线。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-17.png?x-oss-process=image/resize,w_800" /></p><h3 id="代数公式">代数公式</h3><p>对于上述通过 3个控制点绘制贝塞尔曲线，我们可以用代数的方式来表示，如下所示。</p><span class="math display">\[\begin{aligned}b_{0}^{1}(t) = &amp;(1 - t)b_0 + tb_1\\b_{1}^{1}(t) = &amp;(1 - t)b_1 + tb_2\\b_{0}^{2}(t) = &amp;(1 - t)b_{0}^{1} + tb_{1}^{1}\\b_{0}^{2}(t) = &amp;(1 - t)^2b_0 + 2t(1 - t)b_1 + t^2b_2\end{aligned}\]</span><p>由此，我们可以推导出 N 阶贝塞尔曲线的代数公式，如下所示。其中，<spanclass="math inline">\(n\)</span> 表示 N 阶贝塞尔曲线，<spanclass="math inline">\(b_j\)</span> 表示控制点，<spanclass="math inline">\(B_i^n(t)\)</span> 为伯恩斯坦多项式（BernsteinPolynomials）。</p><span class="math display">\[\begin{aligned}b^n(t) = &amp;b_{0}^{n}(t) = \sum_{j=0}^{n}b_jB_{j}^{n}(t)\\B_i^n(t) = &amp;\left(\begin{matrix}n \\i \\\end{matrix}\right)t^i(1-t)^{n-i}\end{aligned}\]</span><h3 id="曲线性质">曲线性质</h3><p>贝塞尔曲线具有以下几个特性：</p><ul><li>一定过起点和终点。</li><li>不受仿射变换影响，受投影变换影响。</li><li>凸包（Convex Hull）性质：贝塞尔曲线在所有控制点的凸包范围内。</li></ul><h2 id="分段贝塞尔曲线">分段贝塞尔曲线</h2><p>根据贝塞尔曲线绘制算法，我们可以知道，改变任意一个控制点的位置都会影响整个贝塞尔曲线。因此，当控制点比较多时，我们很难进行精准的控制和调整。于是就有了分段贝塞尔曲线，即采用多条贝塞尔曲线进行串联。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-18.png?x-oss-process=image/resize,w_800" /></p><h1 id="曲面">曲面</h1><p>曲面（Surface）在图形学中应用同样非常广泛，可以用它来表示各种三维物体。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-19.png?x-oss-process=image/resize,w_800" /></p><h2 id="贝塞尔曲面">贝塞尔曲面</h2><p>贝塞尔曲线控制点都是在同一平面内，由此进行扩展，贝塞尔曲面的控制点则是分部在三维空间中。下图所示，展示了空间中4 x 4 个控制点所构成的贝塞尔曲面。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-20.png?x-oss-process=image/resize,w_800" /></p><h3 id="绘制算法-1">绘制算法</h3><p>贝塞尔曲面的绘制算法本质上还是基于 De Casteljau's Algorithm进行多次绘制。以下图为例，首先基于预设的所有控制点（比如：4 x 4的控制点），绘制 4条贝塞尔曲线。然后在与曲线垂直的平面中开始绘制曲线，按照固定间距，以 4条贝塞尔曲线上的点作为控制点，绘制贝塞尔曲线。以此类推，最终得到一个贝塞尔曲面。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-21.png?x-oss-process=image/resize,w_800" /></p><h2 id="曲面处理">曲面处理</h2><p>根据上述绘制算法，我们可以得到基于多边形网格的曲面。在实际应用中，我们会对曲面进行进一步的处理。常见的曲面处理操作有以下两种：</p><ul><li><strong>网格细分</strong>（Mesh Subdivision）</li><li><strong>网格简化</strong>（Mesh Simplification）</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-22.png?x-oss-process=image/resize,w_800" /></p><h3 id="网格细分">网格细分</h3><p>网格细分就是把一个多边形拆分成多个多边形。这里我们介绍两种细分算法：Loop细分和 Catmull-Clark 细分。</p><h4 id="loop-细分">Loop 细分</h4><p>Loop 细分只适用于三角形面的细分，具体可以分为两步：</p><ul><li>将一个三角形拆分成四个三角形</li><li>更新新顶点和旧顶点的位置，使模型变得更加光滑</li></ul><p>三角形的拆分非常简单，连接每条边的中点，即可将拆分成四个三角形，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-23.png?x-oss-process=image/resize,w_800" /></p><p>对于新顶点的更新，它会基于周围四个旧顶点求加权平均，离它近的顶点权重大，设为3/8，离它远的顶点权重小，设为 1/8，如下所示，白点为待更新的新顶点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-24.png?x-oss-process=image/resize,w_800" /></p><p>对于旧顶点的更新，它会基于周围几个旧顶点求加权平均，其中各个点的权重值与待更新点的度（Degree）有关，最终可以得到如下所示的更新方法。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-25.png?x-oss-process=image/resize,w_800" /></p><p>Loop 细分只针对三角形面进行细分，整体效果如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-29.png?x-oss-process=image/resize,w_800" /></p><h4 id="catmull-clark-细分">Catmull-Clark 细分</h4><p>相比对 Loop 细分，Catmull-Clark细分是一种更加通用的细分方法，适用于各种多边形网格曲面。Catmull-Clark细分涉及到一个概念 <strong>奇异点</strong>（ExtraordinaryVertex），即度不为 4 的点。</p><p>Catmull-Clark 细分的第一步同样是拆分多边形，主要包含以下几点：</p><ul><li>对于边，取其中点；对于面，也取其一个点（比如：重心）</li><li>连接边的中点和面的中点</li></ul><p>我们可以发现，当对非四边形进行一次细分后，所有的非四边形都消失了。不过，一次细分后，会引入两个新的奇异点。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-26.png?x-oss-process=image/resize,w_800" /></p><p>对于新顶点的更新，可以分为两种情况，分别边上的点和面中的点，其规则如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-27.png?x-oss-process=image/resize,w_800" /></p><p>对于旧顶点的更新，其更新规则如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-28.png?x-oss-process=image/resize,w_800" /></p><p>Catmull-Clark 适用于任何多边形网格面，整体效果如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-30.png?x-oss-process=image/resize,w_800" /></p><h3 id="网格简化">网格简化</h3><p>网格简化与网格细分正好相反，其目的是为了减少三角形数量，从而提升性能。对于近的物体三角形多，远的物体三角形少。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-31.png?x-oss-process=image/resize,w_800" /></p><p>网格简化是通过 <strong>边坍缩</strong>（EdgeCollapse）实现的，它会减少边的数量，并更新相关顶点的位置。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-32.png?x-oss-process=image/resize,w_800" /></p><p>那么边坍缩的底层依据是什么呢？这里涉及到<strong>二次度量误差</strong>（Quadirc ErrorMetrics）的概念。二次度量误差用来表示网格简化带来的误差大小，其计算方法是新顶点与它关联的面的垂直距离的平方和，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/geometry-33.png?x-oss-process=image/resize,w_800" /></p><p>当删除一条边时，我们会引入一个新的顶点，当新顶点调整至二次度量误差最小时，我们将其设置为边坍缩后的新顶点。利用这种贪心思想，就能实现网格简化。</p><h1 id="总结">总结</h1><p>本文我们主要介绍了计算机图形学中的几何相关部分。首先，我们介绍了几何的几种表示方法：隐式几何表示和显式集合表示，两者各自又有着很多实现方法。</p><p>然后，我们介绍了曲线，特别是贝塞尔曲线，详细介绍了其绘制算法 DeCasteljau's Algorithm。由此延伸值曲面的绘制，特别是贝塞尔曲面。</p><p>最后，我们介绍了曲面的两种常见的处理方式：网格细分和网格简化。</p><p>至此，几何相关的内容均已介绍完毕。后续，我们将探讨光线追踪渲染器的相关内容。</p><h1 id="参考">参考</h1><ol type="1"><li>《GAMES 101》</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;前面我们介绍了观测变换、光栅化、着色等几个图形学中比较复杂的主题，本文我们稍微放松一下，介绍一个相对比较简单的主题——几何。&lt;/p&gt;</summary>
    
    
    
    <category term="图形学" scheme="http://chuquan.me/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="几何" scheme="http://chuquan.me/tags/%E5%87%A0%E4%BD%95/"/>
    
    <category term="Loop Subdivision" scheme="http://chuquan.me/tags/Loop-Subdivision/"/>
    
    <category term="Catmull-Clark Subdivision" scheme="http://chuquan.me/tags/Catmull-Clark-Subdivision/"/>
    
    <category term="贝塞尔曲线" scheme="http://chuquan.me/tags/%E8%B4%9D%E5%A1%9E%E5%B0%94%E6%9B%B2%E7%BA%BF/"/>
    
    <category term="De Casteljau&#39;s Algorithm" scheme="http://chuquan.me/tags/De-Casteljau-s-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>计算机图形学基础（5）——着色</title>
    <link href="http://chuquan.me/2024/04/05/foundation-of-computer-graphic-05/"/>
    <id>http://chuquan.me/2024/04/05/foundation-of-computer-graphic-05/</id>
    <published>2024-04-05T07:52:33.000Z</published>
    <updated>2024-04-06T03:22:26.023Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇 <ahref="http://chuquan.me/2024/03/30/foundation-of-computer-graphic-04/">文章</a>我们介绍了光栅化所涉及的基本内容。通过光栅化，我们可以实现将 3D空间模型的投影绘制到 2D屏幕。然而，仅仅实现光栅化，还不足以让渲染结果具有真实感，如下图左部所示。我们希望能够模拟光线所带来的的明暗效果，如下图右部所示。</p><span id="more"></span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-01.png?x-oss-process=image/resize,w_800" /></p><p>在计算机图形学中，<strong>着色</strong>（Shading）就是通过计算来决定三维模型表面每个像素的颜色和亮度的过程。本质而言，着色就是<strong>对不同物体应用不同材质</strong>。</p><h1 id="着色模型">着色模型</h1><h2 id="着色局部性">着色局部性</h2><p>具体分析着色时，我们会分析光线照射到物体表面的每一个点，也称<strong>着色点</strong>（ShadingPoint）。对于每个着色点，我们将其视为一个微平面（或称单位平面），由此我们可以构建法线。整体而言，着色的最终结果受以下几种输入影响，分别是：</p><ul><li>观测方向 <span class="math inline">\(v\)</span></li><li>表面法向 <span class="math inline">\(n\)</span></li><li>光线方向 <span class="math inline">\(l\)</span></li><li>表面参数，如：颜色、材质。</li></ul><blockquote><p>注意：对于着色过程，我们只考虑光照对于物体表面的影响，而不考虑其他物体的阴影对本物体产生的影响。</p></blockquote><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-03.png?x-oss-process=image/resize,w_800" /></p><h2 id="布林-冯反射模型">布林-冯反射模型</h2><p>关于光线与物体表面的作用，根据我们的认知，其实可以分为三种类型：</p><ul><li><strong>漫反射</strong>（Diffuse）</li><li><strong>高光</strong>（Specular）</li><li><strong>环境光</strong>（Ambient）</li></ul><p>在计算机图形学中，有一种广泛使用的光照和颜色计算模型——<strong>布林-冯反射模型</strong>（Blinn-PhongRelectanceModel），其考虑了上述三种光照的叠加效果对物体表面颜色的影响。</p><p>下面，我们分别来介绍这三种光照类型。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-02.png?x-oss-process=image/resize,w_800" /></p><h3 id="漫反射">漫反射</h3><p>当光线照射到一个点时，光线会向各个方向发生反射，这种现象称为<strong>漫反射</strong>。漫反射的反射光强主要受到光照角度、光照强度、漫反射系数等因素的影响。</p><h4 id="光照角度">光照角度</h4><p>在图形学中，<strong>兰伯特余弦定理</strong>（Lambert's cosinelaw）详细描述了光照角度对于表面接收光照照射量的影响。下图所示，列举了三种光照角度。</p><ul><li>情况一，入射角度为 <spanclass="math inline">\(90^{\circ}\)</span>，单位平面会接收全部光照。</li><li>情况二，入射角度为 <spanclass="math inline">\(30^{\circ}\)</span>，单位平面只会接收到一半光照。</li><li>情况三，入射角度为 <spanclass="math inline">\(90^{\circ}-\theta\)</span>，单位平面接收到的光照占全部光照的比例为<span class="math inline">\(cos{\theta} = \hat{l} \cdot\hat{n}\)</span>。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-04.png?x-oss-process=image/resize,w_800" /></p><p>基于兰伯特余弦定理，我们可以推导出一个函数表示单位平面接收的光照照射量占全部光照的比例，如下所示。由于<span class="math inline">\(cos\theta\)</span>可能会负数，但这没有意义，所以我们使用 <spanclass="math inline">\(max(0, cos\theta)\)</span> 来保证其值大于等于0。</p><span class="math display">\[\begin{aligned}f(\theta) = max(0, cos\theta) = max(0, \hat{l} \cdot \hat{n})\end{aligned}\]</span><h4 id="光照强度">光照强度</h4><p>对于光照强度，我们考虑如下所示 3D空间中的一个点光源。根据能量守恒定理，以光源为球心，任意距离为半径的球体，球面所覆盖的光线强度是相等的。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-34.png?x-oss-process=image/resize,w_800" /></p><p>由此，我们可以推导光照强度与光源距离之间的关系。假设半径 <spanclass="math inline">\(r\)</span> 为 <code>1</code>时，球面一个点的光照强度为 <spanclass="math inline">\(I\)</span>。那么当半径为任意值 <spanclass="math inline">\(r\)</span> 时，我们可以根据能量守恒定理得到：</p><span class="math display">\[\begin{aligned}单位球面光照强度：&amp;4{\pi}I\\任意球面光照强度：&amp;4{\pi}r^2I_r\\根据能量守恒定理：&amp;4{\pi}r^2I_r = 4{\pi}I\\任意点的光照强度：&amp;I_r=I/r^2\end{aligned}\]</span><h4 id="漫反射系数">漫反射系数</h4><p>不同的材质具有不同的漫反射系数，我们将漫反射系数定义为 <spanclass="math inline">\(k_d\)</span>。如下所示，<spanclass="math inline">\(k_d\)</span>越大，反射的光线强度越大，看到的物体越亮。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-05.png?x-oss-process=image/resize,w_800" /></p><h4 id="漫反射公式">漫反射公式</h4><p>漫反射光线的计算公式其实就是由上述三部分组成，如下所示。</p><span class="math display">\[\begin{aligned}L_d = k_d(I/r^2)max(0, \hat{l} \cdot \hat{n})\end{aligned}\]</span><h3 id="高光">高光</h3><p>高光反射，当观测向量趋近于光线的反射向量时，我们可以看到镜面反射所产生的高光，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-06.png?x-oss-process=image/resize,w_800" /></p><h4 id="高光区域">高光区域</h4><p>那么如何判断高光区域呢？我们可以通过计算光照方向向量和观测方向向量之间的<strong>半程向量</strong>（HalfVector）。然后再计算半程向量与平面法线之间的夹角，判断两者是否接近。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-07.png?x-oss-process=image/resize,w_800" /></p><p>如下所示为半程向量的计算公式，有了半程向量之后，我们可以计算法向量与半程向量之间的夹角。</p><span class="math display">\[\begin{aligned}\hat{h} = bisector(\hat{v}, \hat{l}) = \frac{\hat{v} + \hat{l}}{|\hat{v}+ \hat{l}|}\end{aligned}\]</span><h4 id="高光突变">高光突变</h4><p>根据日常经验，我们可以发现当法向量与半程向量之间的夹角大于某个阈值之后，高光效应会发生突变。如果我们使用<span class="math inline">\(cos\theta\)</span>来描述这种突变，显示是不合适的。在布林-冯模型中，我们对 <spanclass="math inline">\(cos^p\theta\)</span> 来描述高光突变，其中 <spanclass="math inline">\(p\)</span> 是一个经验值。下图所示，展示了不同<span class="math inline">\(p\)</span> 值随角度变化的曲线。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-08.png?x-oss-process=image/resize,w_800" /></p><h4 id="高光系数">高光系数</h4><p>类似于漫反射系数，对于高光，这里也有一个高光系数，使用 <spanclass="math inline">\(k_s\)</span> 表示。下图所示为不同 <spanclass="math inline">\(k_s\)</span> 和不同 <spanclass="math inline">\(p\)</span>的情况下，高光效果的对比。可以看出，高光系数越大，观测的效果越明亮。高光突变的<span class="math inline">\(p\)</span> 值越大，高光区域则越小。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-09.png?x-oss-process=image/resize,w_800" /></p><h4 id="高光公式">高光公式</h4><p>高光的计算公式其实也是由三部分组成：高光系数、光线强度、高光突变，具体公式如下所示。</p><span class="math display">\[\begin{aligned}L_s = k_s(I/r^2)max(0, \hat{n} \cdot \hat{h})^p\end{aligned}\]</span><h3 id="环境光">环境光</h3><p>在现实世界中，我们知道即使没有光源直接照射物体，物体也并不是完全是黑色的。对此，布林-冯着色模型也近似处理了这种情况，即环境光。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-10.png?x-oss-process=image/resize,w_800" /></p><h4 id="环境光公式">环境光公式</h4><p>环境光的计算公式非常简单，由环境光系数和环境光强度组成，具体公式如下所示。</p><span class="math display">\[\begin{aligned}L_a = k_aI_a\end{aligned}\]</span><h3 id="光线反射公式">光线反射公式</h3><p>布林-冯反射模型定义了一个光线反射公式，该公式由上述三种光照反射类型的计算公式组合，具体公式如下所示。</p><span class="math display">\[\begin{aligned}L = L_a + L_d + L_s = k_aI_a + k_d(I/r^2)max(0, \hat{n} \cdot \hat{l}) +k_s(I/r^2)max(0, \hat{n} \cdot \hat{h})^p\end{aligned}\]</span><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-11.png?x-oss-process=image/resize,w_800" /></p><h1 id="着色频率">着色频率</h1><p>在布林-冯反射模型中，我们以着色点（单位平面）为单位介绍三种光照反射类型。那么在真实着色过程中，以什么为单位进行着色呢？考虑到着色性能的开销，实际上可以分为三种类型，分别是：</p><ul><li><strong>平面着色</strong>（Flat Shading）</li><li><strong>顶点着色</strong>（Gouraud Shading）</li><li><strong>像素着色</strong>（冯-着色，Phong Shading）</li></ul><h2 id="平面着色">平面着色</h2><p>平面着色会对每一个平面做一次着色。相对而言，着色频率低，性能开销小，但是着色效果不够丝滑，会有明显的棱边效果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-12.png?x-oss-process=image/resize,w_800" /></p><p>在布林-冯反射模型中，着色点的法向量是计算着色的关键变量。对于平面着色而言，我们可以通过三角形的任意两条边所构成的向量，计算叉积，即可得到法向量。</p><h2 id="顶点着色">顶点着色</h2><p>顶点着色会对三角形的三个顶点进行着色。对于三角形内部的点，则基于三个顶点的颜色，使用速度更快的插值法进行计算。相比平面着色，着色频率略高，性能开销略大，但是着色效果会好一点，会有细微的棱边效果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-13.png?x-oss-process=image/resize,w_800" /></p><p>对于顶点着色，我们需要计算三个顶点各自的法向量。通常有两种选择：</p><ul><li>当平面属于一个规则几何体的局部表面时，可以通过规则几何体的整体出发，计算对应平面的法向量。</li><li>其他情况时，可以基于周围平面的法向量，求解平均值，计算对应平面的法向量。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-15.png?x-oss-process=image/resize,w_800" /></p><h2 id="像素着色">像素着色</h2><p>像素着色，也称冯-着色，它会对每一个像素进行着色。这种方式着色频率很高，性能开销很大，但是着色效果非常丝滑。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-14.png?x-oss-process=image/resize,w_800" /></p><p>对于像素着色，我们首先以上述方式计算三角形顶点的法向量，对于三角形内部的点，则通过<strong>重心插值法</strong>（BarycentricInterpolation）来计算。关于重心插值法，我们稍后进行介绍。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-16.png?x-oss-process=image/resize,w_800" /></p><h1 id="实时渲染管线">实时渲染管线</h1><p>实时渲染管线（Real-time Rendering），也称图形管线（GraphicsPipeline），其描述了 3D 场景转换成 2D 图像的完整流程，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-17.png?x-oss-process=image/resize,w_800" /></p><p>实时渲染管线可以分为五个阶段，分别是：</p><ul><li><strong>顶点处理</strong>（Vertex Processing）</li><li><strong>三角形处理</strong>（Triangle Processing）</li><li><strong>光栅化</strong>（Rasterization）</li><li><strong>片段处理</strong>（Fragment Processing）</li><li><strong>帧缓冲操作</strong>（Framebuffer Operations）</li></ul><h2 id="顶点处理">顶点处理</h2><p>顶点处理的输入是 3D 空间中的顶点。为什么是顶点而不是 3D模型？这是因为 3D空间的所有模型都是以三角形为基本单元进行表示的，而三角形则可以通过顶点和连线来描述，3D模型的本质就是大量顶点和连线的定义。</p><p>在顶点处理阶段，我们会对顶点进行观测变换，即 <ahref="http://chuquan.me/2024/03/23/foundation-of-computer-graphic-03/">《计算机图形学基础（3）——观测变换》</a>中所介绍的 MVP 变换。最终输出经过观测变换的顶点。</p><h2 id="三角形处理">三角形处理</h2><p>在某些文章中，会将这个阶段定义成 <strong>图元处理</strong>（PrimitiveProcessing），三角形处理只是其中的一个子集，它还会处理点和线。这里我们为了突出重点，将其称为三角形处理。</p><p>由于顶点处理阶段只对顶点进行变换，而 3D模型还包括连线的定义，三角形处理阶段就是根据连线的定义，将顶点装配成三角形（也称图元）。</p><h2 id="光栅化">光栅化</h2><p>当顶点处理和三角形处理完成之后，我们得到了经过观测变换后的三角形。此时三角形仍然处于3D 空间中，不过我们可以通过正交投影快速获取它们在 2D 空间中的投影。</p><p>光栅化则是将连续的 2D 投影进行采样，转换成离散的 2D投影，这是因为屏幕由一个离散的二维像素矩阵所构成。关于光栅化具体要做的事情以及可能遇到的问题，我们在<ahref="http://chuquan.me/2024/03/30/foundation-of-computer-graphic-04/">《计算机图形学（4）——光栅化》</a>中进行了详细的介绍。</p><p>在实际的 GPU设计中，为了支持可编程、并行计算，实时渲染管线中的光栅化的主要任务是对连续的图形进行采样，使其离散化。</p><h2 id="片段处理">片段处理</h2><p>片段处理，也称像素处理，它会对每个片段的颜色、纹理坐标、深度值等进行计算，期间会大量应用插值法进行计算。严格意义上说，片段处理也属于光栅化的一部分。</p><h2 id="帧缓冲操作">帧缓冲操作</h2><p>帧缓冲操作包含了颜色混合、模板测试、深度测试、透明度检查等一系列操作，最终结果会保存在帧缓冲区，显示器会定时读取帧缓冲区，并将内容呈现在屏幕上。</p><h2 id="关于着色">关于着色</h2><p>整体而言，实时渲染管线包含观测变换、光栅化、着色三大部分。</p><p>然而，着色其实在顶点处理和片段处理阶段都可以存在，这取决于着色频率。如果我们采用顶点着色，那么着色可以发生在顶点处理阶段；如果我们采用像素着色，那么着色可以发生在片段处理阶段。</p><p>在现代 GPU中，实时渲染管线的部分阶段是支持可编程的，比如顶点处理阶段和片段处理阶段。在这些可编程阶段中，我们可以编写着色器（Shader）程序，从而生成自定义的着色结果。</p><h3 id="着色器">着色器</h3><p>在实时渲染领域，大部分从业者做的事情就是在写各种各样的着色器。如下所示，是OpenGL 中的一个片段着色器程序，其采用 GLSL着色语言编写。着色器程序最终由 GPU调用，对于每个像素都会执行并生成着色结果。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">uniform sampler2D myTexture;    <span class="comment">// program parameter</span></span><br><span class="line">uniform vec3 lightDir;          <span class="comment">// program parameter</span></span><br><span class="line">varying vec2 uv;                <span class="comment">// per fragment value (interp. by rasterizer)</span></span><br><span class="line">varying vec3 norm;              <span class="comment">// per fragment value (interp. by rasterizer)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">diffuseShader</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    vec3 kd;                                </span><br><span class="line">    kd = <span class="built_in">texture2d</span>(myTexture, uv);                  <span class="comment">// material color from texture</span></span><br><span class="line">    kd *= <span class="built_in">clamp</span>(<span class="built_in">dot</span>(–lightDir, norm), <span class="number">0.0</span>, <span class="number">1.0</span>);    <span class="comment">// Lambertian shading model</span></span><br><span class="line">    gl_FragColor = <span class="built_in">vec4</span>(kd, <span class="number">1.0</span>);                   <span class="comment">// output fragment color</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="纹理">纹理</h1><p>在介绍着色模型中，我们提到着色点的材质会影响最终的着色结果，比如各种反射系数<span class="math inline">\(k_d\)</span>、<spanclass="math inline">\(k_s\)</span>、<spanclass="math inline">\(k_a\)</span>等。除此之外，着色点的原始颜色、法线等属性也都会影响着色结果。</p><p>为了能够为着色点定义属性，提出了<strong>纹理</strong>（Texture）的概念，使用纹理来记录每个着色点的各种属性。通常情况下，我们会把纹理等同于贴图（图片），这是因为大多数情况下会使用纹理来定义颜色。不过从严格意义上说，贴图只是纹理的一种而已。</p><h2 id="纹理映射">纹理映射</h2><p>纹理映射的本质就是将纹理定义的属性映射到 3D 模型的各个着色点。</p><p>如下图所示，我们定义了一个模型和一个纹理，中间的模型经过纹理映射后渲染得到了我们期望的效果。在建模时，我们会将模型分割成一个个三角形。与模型所绑定的纹理，我们也会将其分割成一个个三角形。两者之间的三角形会一一对应。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-18.png?x-oss-process=image/resize,w_800" /></p><p>为了方便映射，我们会建立 <strong>纹理坐标系</strong>（TextureCoordinate），横坐标用 <span class="math inline">\(u\)</span>表示，纵坐标用 <span class="math inline">\(v\)</span> 表示。<spanclass="math inline">\(u\)</span> 和 <spanclass="math inline">\(v\)</span> 的值都在 <code>[0, 1]</code>之间，这是一个约定俗成的规定。模型中的每个顶点都会设定一个纹理坐标，通过这种方式可以实现纹理映射。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-19.png?x-oss-process=image/resize,w_800" /></p><h2 id="重心坐标">重心坐标</h2><p>虽然模型和纹理是绑定的，但是绑定是基于顶点实现的。因此在纹理映射中，对于模型三角形的顶点，我们可以直接使用绑定的纹理坐标找到纹理中对应坐标的属性。但是模型三角形内部的点该如何获取纹理属性呢？为了解决这个问题，提出了<strong>重心坐标</strong>（Barycentric Coordinate）的概念。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-20.png?x-oss-process=image/resize,w_800" /></p><p>以上图中的三角形为例，重心坐标定义了三角形内部任意一个点 <spanclass="math inline">\((x, y)\)</span> 具有以下几个特性。</p><span class="math display">\[\begin{aligned}\begin{cases}(x, y) = {\alpha}A + {\beta}B + {\gamma}C\\\alpha + \beta + \gamma = 1\\\alpha &gt;= 0;\beta &gt;= 0;\gamma &gt;= 0;\end{cases}\end{aligned}\]</span><p>最终，我们可以计算得到三角形内任意一个点的重心坐标 <spanclass="math inline">\((\alpha, \beta,\gamma)\)</span>。此时，我们可以使用重心坐标，结合顶点属性，计算得到该点的属性。这里的属性可以是位置、纹理坐标、颜色、法线、深度、材质等各种属性。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-21.png?x-oss-process=image/resize,w_800" /></p><p>需要注意的是，在投影时三角形的形状会发生变化，所以在着色时应该基于三维空间的坐标计算重心坐标，然后再做插值。</p><h2 id="纹理查询">纹理查询</h2><p>上面我们介绍了使用重心坐标表示三角形中的任意点。那么具体该如何应用重心坐标来查找对应的纹理属性呢？如下所示，我们使用伪代码描述了这个查找过程。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">for</span> each rasterized screen <span class="title">sample</span><span class="params">(x, y)</span> </span>&#123;</span><br><span class="line">    (u, v) = evaluate texture coordinate <span class="built_in">at</span> (x, y)</span><br><span class="line">    texcolor = texture.<span class="built_in">sample</span>(u, v)</span><br><span class="line">    set sample<span class="number">&#x27;</span>s color to texcolor</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们先说明一个前提：在光栅化阶段，即当三角形被转换为屏幕上的像素时，每个像素的纹理坐标会通过插值方式在三角形的顶点之间计算出来。此时，我们得到的是每个像素的屏幕坐标以及对应的纹理坐标。</p><p>上述伪代码所描述的流程是：</p><ul><li>遍历光栅化得到的屏幕采样点，比如一个三角形 <spanclass="math inline">\(ABC\)</span> 的区域内的某个像素点 <spanclass="math inline">\((x, y)\)</span>。</li><li>基于上述前提，有了像素的屏幕坐标 <span class="math inline">\((x,y)\)</span>，我们可以直接获取对应的纹理坐标。</li><li>当得到像素点的纹理坐标后，我们就可以在纹理中查找对应的属性，伪代码中查找的是颜色属性。</li><li>最后我们用纹理颜色来给像素着色。</li></ul><p>本质上，这是一个纹理采样过程。一旦涉及采样，就可能会出现走样问题。下面，我们来分情况讨论。</p><h3 id="纹理太小问题">纹理太小问题</h3><p>对于纹理太小的情况，那么会出现多个像素映射到一个<strong>纹素</strong>（Texel），即纹理中的一个点或像素。此时，就会出现锯齿问题。</p><p>为了解决锯齿问题，我们可以通过求均值的方式来解决。如下所示，为最近采样、双线性插值、双三次插值的对比结果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-22.png?x-oss-process=image/resize,w_800" /></p><p>双线性插值的原理非常简单，就是去临近的 4个像素，通过三次插值计算得到一个颜色平均值。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-23.png?x-oss-process=image/resize,w_800" /></p><p>双三次插值的原理与双线性插值类似，区别在于前者使用周围的 16个像素求插值，后者使用周围的 4 个像素求插值。</p><h3 id="纹理太大问题">纹理太大问题</h3><p>对于纹理太大的情况，会出现摩尔纹、锯齿等情况。本质上是采样频率低于信号频率，我们在<ahref="http://chuquan.me/2024/03/30/foundation-of-computer-graphic-04/">计算机图形学基础（4）——光栅化</a>中介绍过两种解决思路，一种是超采样，一种是过滤高频信号。</p><p>这两种思路，在这种场景下都存在开销过大的问题。于是，在图形学中提出了范围查询的方法，即<strong>Mipmap</strong>，从而避开了采样所带来的问题。</p><h4 id="点查询-范围查询">点查询 &amp; 范围查询</h4><p>本质上，采样就是点查询。当纹理太大时，屏幕上一个点对应到纹理上可能是一个很大的区域。然而，从这个区域中取一个点来代表整个区域的颜色，这显然是不合适的。对比而言，范围查询相当于提前计算出一个合适的值来代表这个区域。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-24.png?x-oss-process=image/resize,w_800" /></p><h4 id="mipmap">Mipmap</h4><p>Mipmap正是范围查询的一种实现方案，它会为一张纹理生成多个不同层级的纹理，如下图所示。Mipmap虽然生成了多个不同层级的纹理，但是整体的存储量只增加了不到 1/3。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-25.png?x-oss-process=image/resize,w_800" /></p><p>既然 Mipmap生成了多个不同层级的纹理，那么在纹理查询时，我们应该查询哪个层级的纹理呢？</p><p>如下图所示，对于屏幕上的一个像素点，考虑其相邻的两个点，获取它们的纹理坐标。根据纹理坐标计算相邻的距离，由此近似得到像素对应的矩形区域。我们获取矩形区域较大的边长<span class="math inline">\(L\)</span>。然后对 <spanclass="math inline">\(L\)</span>求对数，即可计算得出要查询的纹理的层级。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-26.png?x-oss-process=image/resize,w_800" /></p><span class="math display">\[\begin{aligned}L = &amp;max(\sqrt{(\frac{du}{dx})^2 + (\frac{du}{dx})^2},\sqrt{(\frac{du}{dy})^2 + (\frac{du}{dy})^2})\\D = &amp;log_2L\end{aligned}\]</span><h4 id="各向异性过滤">各向异性过滤</h4><p>事实上，Mipmap也并不是万能的。在有些场景下，也会出现过度模糊的问题，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-27.png?x-oss-process=image/resize,w_800" /></p><p>根本原因是，Mipmap的范围查询所覆盖的区域是正方形。如果屏幕像素点代表了纹理中的一个长方形区域，那么范围查询就无法准确代表长方形区域内的值，因此会出现走样，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-28.png?x-oss-process=image/resize,w_800" /></p><p>那么如何解决呢？方法是各项异性过滤（AnisotropicFiltering）。具体的技术是：除了生成针对正方形区域的范围查询的纹理外，还要生成其他形状（比如长方形）的范围查询的纹理。通过这种方式，纹理的存储量会增加3 倍，不过能够降低着色走样的概率。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-29.png?x-oss-process=image/resize,w_800" /></p><h2 id="纹理应用">纹理应用</h2><p>至此，我们基本了解了纹理及其工作原理，本质而言，纹理 = 内存存储 +范围查询。上述内容我们主要介绍了通过纹理记录颜色，事实上纹理还能记录其他很多属性，比如：环境光、微几何、法向量、高度偏移等等。</p><p>下面，我们来介绍纹理的其他几种应用。</p><h3 id="环境贴图">环境贴图</h3><p>纹理应用最多的就是 <strong>环境贴图</strong>（EnvironmentMap），这里又有非常多的类型。</p><p><strong>立方体环境贴图</strong>（Cube EnvironmentMap），它是将环境映射到一个立方体的六个面上，可以用于实现镜面反射和环境光照。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-30.png?x-oss-process=image/resize,w_800" /></p><p><strong>光照环境贴图</strong>（Light EnvironmentMap），它在渲染过程中预先计算和存储环境光照信息，以提高实时渲染效率和质量的技术</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-31.png?x-oss-process=image/resize,w_800" /></p><p>除此之外，还有很多环境贴图，比如：球谐环境贴图、镜面反射环境贴图、辐射度环境贴图、天空盒环境贴图等等。</p><h3 id="凹凸贴图">凹凸贴图</h3><p>假如我们希望渲染一个表面凹凸不同的球状体，如果使用三角形来表示，那么需要大量三角形，而且结构非常复杂。对于这种情况，我们可以凹凸贴图（BumpMap），它可以定义点的相对高度，从而改变法线，进而影响着色结果，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-32.png?x-oss-process=image/resize,w_800" /></p><h3 id="位移贴图">位移贴图</h3><p>凹凸贴图改变了着色时所使用的法向量，但并没有真正改变模型的形状。一种更现代化的<strong>位移贴图</strong>（DisplacementMapping），则定义了顶点高度的偏移量，使得真真正改变了模型的形状，从而实现更加逼真的效果。下图所示，为凹凸贴图和位移贴图的对比效果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/shading-33.png?x-oss-process=image/resize,w_800" /></p><h1 id="总结">总结</h1><p>本文我们主要介绍了着色相关的内容。</p><p>首先，我们介绍了着色模型，具体介绍了经典的布林-冯反射模型，其由漫反射、高光、环境光三部分组成。</p><p>其次，我们介绍了几种着色频率，包括平面着色、顶点着色、像素着色，简单对比了它们之间的差异。</p><p>然后，我们简单介绍了实时渲染管线的 5个阶段，包括顶点处理、三角形处理、光栅化、片段处理、帧缓冲操作等。</p><p>最后，我们详细介绍了着色中最重要的一部分——纹理。纹理查询是是如何通过重心坐标、纹理坐标查找对应的纹理属性。当然，纹理查询也属于采样，其中也会遇到走样的问题。于是，我们引入了线性插值、Mipmap、各向异性过滤等解决方案。除此之外，我们还介绍了纹理的几种应用，包括：环境贴图、凹凸贴图、位移贴图等。</p><h1 id="参考">参考</h1><ol type="1"><li>《GAMES 101》</li><li><a href="http://shadertop.com">Shadertoy</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;上一篇 &lt;a
href=&quot;http://chuquan.me/2024/03/30/foundation-of-computer-graphic-04/&quot;&gt;文章&lt;/a&gt;
我们介绍了光栅化所涉及的基本内容。通过光栅化，我们可以实现将 3D
空间模型的投影绘制到 2D
屏幕。然而，仅仅实现光栅化，还不足以让渲染结果具有真实感，如下图左部所示。我们希望能够模拟光线所带来的的明暗效果，如下图右部所示。&lt;/p&gt;</summary>
    
    
    
    <category term="图形学" scheme="http://chuquan.me/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="Mipmap" scheme="http://chuquan.me/tags/Mipmap/"/>
    
    <category term="各向异性过滤" scheme="http://chuquan.me/tags/%E5%90%84%E5%90%91%E5%BC%82%E6%80%A7%E8%BF%87%E6%BB%A4/"/>
    
    <category term="纹理" scheme="http://chuquan.me/tags/%E7%BA%B9%E7%90%86/"/>
    
    <category term="重心坐标" scheme="http://chuquan.me/tags/%E9%87%8D%E5%BF%83%E5%9D%90%E6%A0%87/"/>
    
    <category term="纹理坐标" scheme="http://chuquan.me/tags/%E7%BA%B9%E7%90%86%E5%9D%90%E6%A0%87/"/>
    
    <category term="布林-冯模型" scheme="http://chuquan.me/tags/%E5%B8%83%E6%9E%97-%E5%86%AF%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>计算机图形学基础（4）——光栅化</title>
    <link href="http://chuquan.me/2024/03/30/foundation-of-computer-graphic-04/"/>
    <id>http://chuquan.me/2024/03/30/foundation-of-computer-graphic-04/</id>
    <published>2024-03-30T01:20:55.000Z</published>
    <updated>2024-03-30T01:39:10.562Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇 <ahref="http://chuquan.me/2024/03/23/foundation-of-computer-graphic-03/">文章</a>我们介绍了矩阵变换在计算机图形学中的应用，包括：视图变换、模型变换、投影变换。此外，我们还详细介绍了投影变换中的正交投影和透视投影，以及屏幕映射过程中的视口变换。</p><span id="more"></span><p>本文，我们来介绍一下计算机图形学中最重要的内容之一——光栅化。</p><h1 id="栅格显示">栅格显示</h1><p>光栅化（Rasterization）中光栅（Raster）一词来源于德语，表示栅格的意思。我们现在用的显示设备基本上都是由像素点阵构成的栅格显示设备。因此，我们很容易理解光栅化的含义，即在栅格显示设备上绘制图形。</p><p>这里我们先介绍一下常见的栅格显示设备。</p><p>旧式的阴极射线管（Cathode Ray Tube，CRT）电视，它的基本原理是<strong>通过射线管将电子射到屏幕进行逐行扫描</strong>，如下图所示。在实际应用中，会借助视觉暂留效应，对屏幕进行<strong>隔行扫描</strong>，从而降低扫描的计算量。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-01.png?x-oss-process=image/resize,w_800" /></p><p>现代平板显示器（Flat Panel Displays）中最常用的是液晶显示器（LiquidCrystal Display，LCD），它的基本原理是<strong>通过扭转偏振来阻挡或传输光线</strong>，如下图所示。在实际应用中，会使用<strong>帧缓冲</strong>（FrameBuffer）来提前缓存画面的帧数据，从而提高显示流畅度。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-02.png?x-oss-process=image/resize,w_800" /></p><h1 id="光栅化">光栅化</h1><h2 id="基本单元">基本单元</h2><p>光栅化的基本单元是三角形，采用三角形作为基本单元的原因是：</p><ul><li>三角形是最基本的多边形。</li><li>三角形具有平面性。</li><li>三角形可以明确定义内部和外部。我们可以通过向量叉积来判断，详见 <ahref="http://chuquan.me/2024/01/15/foundation-of-computer-graphic-01/">计算机图形学基础（1）——线性代数</a>。</li><li>任意多边形可以拆分成 N 个三角形。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-07.png?x-oss-process=image/resize,w_800" /></p><h2 id="采样绘制">采样绘制</h2><p>2D屏幕是一个离散的像素阵列，空间中的三角形则是一个连续的函数。采样绘制的本质则是对一个函数进行离散化。具体的做法是：</p><ul><li>遍历像素阵列，判断每一个像素阵列是否位于三角形的投射区域内</li><li>如果是，进行绘制像素；否则，不绘制。</li></ul><p>如下所示为采样绘制的伪代码和示意图。注意，像素本身是一个矩形区域，因此判断像素是否在三角形内部时，采用的是像素点的中心作为参照。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">inside</span><span class="params">(t, x, y)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">point</span><span class="params">(x, y)</span> in <span class="title">triangle</span><span class="params">(t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> x = <span class="number">0</span>; x &lt; xmax; ++x) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> y = <span class="number">0</span>; y &lt; ymax; ++y) &#123;</span><br><span class="line">        image[x][y] = <span class="built_in">inside</span>(tri, x + <span class="number">0.5</span>, y + <span class="number">0.5</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-03.png?x-oss-process=image/resize,w_800" /></p><p>在绘制三角形时，一般不会对整个屏幕的像素点进行扫描，而是仅仅对三角形的<strong>包围盒</strong>（BoundingBox）区域内的像素进行扫描和绘制，从而有效降低算法复杂度。对于一些窄长三角形，甚至可以进一步优化算法，如下图右侧所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-04.png?x-oss-process=image/resize,w_800" /></p><h2 id="核心问题">核心问题</h2><p>我们观察上述这种简单的采样绘制方式，可以发现一个很明显的问题——<strong>锯齿</strong>（Jaggies）。这个问题根本上是采样导致的，对于这种现象我们称之为<strong>走样</strong>（Aliasing）。走样会带来很多奇怪的现象，比如：锯齿、摩尔纹（MoirePatterns）、车轮效应等，如下图所示。</p><p>光栅化要解决的核心问题就是走样问题，即<strong>反走样</strong>（Antialiasing）。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-05.png?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-06.png?x-oss-process=image/resize,w_800" /></p><h2 id="解决方法">解决方法</h2><p>计算机图形学中解决走样问题的最常用方法是：<strong>先模糊，后采样</strong>。模糊，从字面上理解就是将图片虚化，从数学上理解则是<strong>滤波</strong>，关于滤波，我们将在下一节中进行介绍。</p><p>下图所示，为「直接采样」和「先模糊，后采样」的流程对比图。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-08.png?x-oss-process=image/resize,w_800" /></p><p>在具体实践中，通过这种方式能够有效解决光栅化中的锯齿问题，如下所示为反走样前后的效果对比图。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-09.png?x-oss-process=image/resize,w_800" /></p><p>这里，我们可能会产生疑问：</p><ul><li>出现走样的根本原因是什么？</li><li>为什么先模糊（滤波）后采样能够实现反走样？</li></ul><p>要讲明白这些内容，我们必须要介绍一下采样理论。</p><h1 id="采样理论">采样理论</h1><p>采样理论是信号系统中非常重要的一个理论，它在数字信号处理、数字通信、图像处理等众多领域都有着广泛的应用。</p><p>在实际应用中，我们通过一定的采样率把连续信号转换为离散信号，然后再对离散信号进行处理。处理完后，我们又可以通过一定的重构方法把离散信号转换回连续信号，以便在实际系统中使用。</p><h2 id="傅里叶级数">傅里叶级数</h2><p>那么如何表示任意一种信号呢？法国数学家傅里叶认为，任何周期函数（信号）都可以用正弦函数和余弦函数构成的无穷级数来表示，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-10.png?x-oss-process=image/resize,w_800" /></p><p>对于上图中的信号，使用傅里叶级数展开的表示如下所示。其中，这里 <spanclass="math inline">\(t\)</span> 表示时间，<spanclass="math inline">\(A\)</span> 表示振幅，<spanclass="math inline">\(w\)</span> 表示角频率。</p><span class="math display">\[\begin{aligned}f(x) = \frac{2Acos(tw)}{\pi} - \frac{2Acos(3tw)}{3\pi} +\frac{2Acos(5tw)}{5\pi} - \frac{2Acos(7tw)}{7\pi} + ...\end{aligned}\]</span><h2 id="时域与频域">时域与频域</h2><p>基于傅里叶级数，我们可以对信号的时域（以时间为横坐标）和频域（以频率为横坐标）进行相互转换：</p><ul><li>时域转换成频域：采用 <strong>傅里叶变换</strong>（FourierTransform）</li><li>频域转换成时域：采用 <strong>逆傅里叶变换</strong>（Inverse FourierTransform）</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-11.png?x-oss-process=image/resize,w_800" /></p><h2 id="走样原理">走样原理</h2><p>了解了信号的时域和频域之后，我们再来介绍走样的原理。</p><p>理想情况下，对一个连续信号进行采样后得到的离散信号，应该能够近似重构原始信号。然而，当采样频率低于原始信号的频率时，就会很容易出现走样的问题。换句话说，就是采样得到的离散信号无法近似重构原始信号。</p><p>下图所示，我们列举了几种信号，信号频率依次从高到低，我们使用相同的频率对这些信号进行采样。很显然，我们对低频信号进行采样时，由于采样频率大于信号频率，得到的离散信号可以近似重构原始信号；但是，我们对高频信号采样时，由于采样频率小于信号频率，得到的离散信号则无法近似重构原始信号。</p><p>因此走样的根本原因就是 <strong>采样频率小于信号频率</strong>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-12.png?x-oss-process=image/resize,w_800" /></p><h1 id="滤波">滤波</h1><p>由于滤波在反走样中起到了重要作用，因此我们简单介绍一下图像处理中滤波。</p><p>如下图所示，通过傅里叶变换将左侧的像素空间（空间域）变为右侧的频谱（频域）。对于二维信号，其频谱的表示如下：</p><ul><li>高频部分代表细节、边缘、噪声</li><li>低频占据绝大多数能量，其中直流分量（零频）能量占比最大</li><li>频率分部具有中心对称性</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-13.png?x-oss-process=image/resize,w_800" /></p><p>下面，我们来介绍一下几种常见的滤波。</p><h2 id="高通滤波">高通滤波</h2><p>高通滤波（High-passfilter），保留高频信号。在图像中，轮廓的边缘会发生剧烈变化，属于高频信号。经过高通滤波后，图像只会保留一些轮廓信息，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-14.png?x-oss-process=image/resize,w_800" /></p><h2 id="低通滤波">低通滤波</h2><p>低通滤波（Low-passfilter），保留低频信号。在图像中，颜色变化平缓的区域属于低频信号。经过低通滤波后，图像会抹去轮廓信息，如下图所示。模糊处理基于低通滤波实现的。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-15.png?x-oss-process=image/resize,w_800" /></p><h2 id="带通滤波">带通滤波</h2><p>带通滤波（Band-passfilter），顾名思义，只保留一部分频率范围内的信号。对图像滤波后的效果取决于带通滤波所选择的频率范围。下图所示，为两种不同频率范围的带通滤波。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-16.png?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-17.png?x-oss-process=image/resize,w_800" /></p><h2 id="卷积">卷积</h2><p>那么如何实现滤波呢？卷积（Convolution）就是实现滤波的主要数学工具和底层原理。滤波器的基本原理是<strong>响应函数与输入信号进行卷积运算</strong>，因此滤波器也可以称为<strong>卷积核</strong>。</p><p>如下所示，是卷积的数学定义，两个函数的 <spanclass="math inline">\(f\)</span> 和 <spanclass="math inline">\(g\)</span> 卷积 <span class="math inline">\(f *g(n)\)</span>。</p><span class="math display">\[\begin{aligned}连续形式：&amp;(f * g)(n) = \int_{-\infty}^{\infty}f(\tau)g(n-\tau)d\tau\\离散形式：&amp;(f * g)(n) = \sum_{-\infty}^{\infty}f(\tau)g(n-\tau)\end{aligned}\]</span><p>观察 <span class="math inline">\(f(\tau)\)</span> 和 <spanclass="math inline">\(g(n-\tau)\)</span> 的关系，可以发现是对<code>g</code> 函数进行了「翻转」，这就是「卷」的来源。同时，对两个函数<code>f</code> 和 <code>g</code> 进行积分，这就是「积」的来源。</p><p>卷积本身是一个很难解释的数学定义，如果你想深入理解卷积，这里推荐一篇知乎高赞回答——<ahref="https://www.zhihu.com/question/22298352/answer/228543288">传送门</a>。简而言之，<strong>两个函数的卷积，会先将一个函数翻转，然后进行滑动叠加</strong>。本质上可以将卷积理解成加权平均。</p><p>下图所示，是对图像进行滤波（卷积）的过程，实现模糊处理。基于傅里叶变换，我们可以实现时域（空间域）与频域之间的相互转换。<strong>时域（空间域）上对两个信号进行卷积，等同于频域上对两个信号的频率进行乘积</strong>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-18.png?x-oss-process=image/resize,w_800" /></p><h1 id="反走样原理">反走样原理</h1><p>在「走样原理」一节中，我们提到了走样的根本原因是<strong>采样频率小于信号频率</strong>。在不提高采样频率的前提下，通过<strong>先滤波，后采样</strong>的方式可以实现反走样，这里的底层逻辑又是什么呢？</p><p>简单的理解就是，<strong>滤波（低通滤波，即模糊处理）会过滤掉信号中大于采样频率的信号分量</strong>。滤波后，剩余的信号分量的频率满足<strong>采样频率 &gt;= 信号频率</strong> 的条件，因此实现了反走样。</p><p>实现反走样的方法主要就是围绕两个角度来实现：</p><ul><li>提高采样频率。如：超采样技术（Supersampling）、多重采样抗锯齿（MSAA）、超分辨率</li><li>过滤高频信号。如：先模糊后采样（Pre-Filter）</li></ul><h1 id="遮挡与可见">遮挡与可见</h1><p>上述内容介绍了光栅化一个三角形的场景，以及其会遇到的问题——走样。下面，我们来介绍光栅化多个三角形会遇到的问题——遮挡与可见问题。</p><p>在 3D空间中，三角形之间存在着前后遮挡关系，那么三角形绘制的先后顺序应该是什么样的呢？</p><h2 id="画家算法">画家算法</h2><p>对此，我们先介绍一个经常被提到的算法：<strong>画家算法</strong>（Painter'sAlgorithm）。</p><p>画家算法，顾名思义，按照画家绘画时的先后顺序来执行，远的物体先绘制，进的物体后绘制，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-22.png?x-oss-process=image/resize,w_800" /></p><p>虽然画家算法适用于绝大多数场景，但是在某些场景下它仍然无法解决可见性问题。如下图所示，三个相互嵌套的三角形，使用画家算法则无法对三角形进行排序，因此无法准确实现光栅化。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-23.png?x-oss-process=image/resize,w_800" /></p><h2 id="深度缓冲算法">深度缓冲算法</h2><p>那么上述问题该如何解决呢？于是出现了<strong>深度缓冲算法</strong>（Z-Buffer Algorithm），其基本原理是：</p><ul><li>光栅化采用两个缓冲区<ul><li>原有的 <strong>帧缓冲区</strong>（FrameBuffer）存储每个像素颜色值</li><li>附加的<strong>深度缓冲区</strong>（Z-Buffer）存储每个像素深度值</li></ul></li><li>深度缓冲区存储每个像素当前的<strong>最小深度值</strong>（Z-Value）</li></ul><p>如下所示，为深度缓冲算法的伪代码实现。注意：我们会初始化深度值为无穷大。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (each triangle T) &#123;</span><br><span class="line">    <span class="keyword">for</span> (each <span class="built_in">sample</span> (x, y, z) in T) &#123;</span><br><span class="line">        <span class="keyword">if</span> (z &lt; zbuffer[x, y]) &#123;            <span class="comment">// 处理深度更小的采样点</span></span><br><span class="line">            framebuffer[x, y] = rgb;        <span class="comment">// 更新颜色值</span></span><br><span class="line">            zbuffer[x, y] = z;              <span class="comment">// 更新深度值</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下图所示，使用深度缓冲算法光栅化两个三角形的示意图。当光栅化红色三角形时，我们遍历红色三角形的每一个像素的深度值，并与当前深度值进行比较。由于当前深度值均为无穷大，所以红色三角形的每一个像素都可以绘制。当光栅化蓝色三角形时，同样会遍历蓝色三角形每一个像素的深度值，并与当前深度值变换，深度值大于当前深度值，则不绘制；否则，绘制并更新当前深度值。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/rasterization-24.png?x-oss-process=image/resize,w_800" /></p><p>注意，这里的深度值比较取决于坐标系是如何建立的。按照我们之前的介绍，相机是沿着-Z 轴方向观测，因此深度越大，则 Z 值越小。</p><h1 id="总结">总结</h1><p>本文，我们主要介绍了光栅化技术。首先介绍了光栅化的含义以及栅格设备。其次，我们介绍了光栅化的基本单元——三角形。</p><p>在绘制单个三角形时，我们会遇到走样问题。对此我们介绍了反走样的两种方法：提高采样频率、过滤信号频率。我们着重介绍了后者，先滤波（模糊）后采样，并介绍了其中涉及的原理。</p><p>在绘制多个三角形时，我们会遇到遮挡问题。对此我们介绍了两种算法：画家算法、深度缓冲算法。</p><p>后续，我们还会继续介绍计算机图形学的其他内容，敬请期待吧~</p><h1 id="参考">参考</h1><ol type="1"><li>《GAMES 101》</li><li><ahref="https://www.zhihu.com/question/22298352/answer/228543288">如何通俗易懂地解释卷积</a></li><li><ahref="https://www.zhihu.com/tardis/zm/art/54946461?source_id=1003">傅里叶变换与图像的频域处理</a></li><li><ahref="https://dezeming.top/wp-content/uploads/2022/04/%E9%87%87%E6%A0%B7%E5%AE%9A%E7%90%86.pdf">采样定理</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;上一篇 &lt;a
href=&quot;http://chuquan.me/2024/03/23/foundation-of-computer-graphic-03/&quot;&gt;文章&lt;/a&gt;
我们介绍了矩阵变换在计算机图形学中的应用，包括：视图变换、模型变换、投影变换。此外，我们还详细介绍了投影变换中的正交投影和透视投影，以及屏幕映射过程中的视口变换。&lt;/p&gt;</summary>
    
    
    
    <category term="图形学" scheme="http://chuquan.me/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="画家算法" scheme="http://chuquan.me/tags/%E7%94%BB%E5%AE%B6%E7%AE%97%E6%B3%95/"/>
    
    <category term="深度缓冲算法" scheme="http://chuquan.me/tags/%E6%B7%B1%E5%BA%A6%E7%BC%93%E5%86%B2%E7%AE%97%E6%B3%95/"/>
    
    <category term="采样" scheme="http://chuquan.me/tags/%E9%87%87%E6%A0%B7/"/>
    
    <category term="走样" scheme="http://chuquan.me/tags/%E8%B5%B0%E6%A0%B7/"/>
    
    <category term="滤波" scheme="http://chuquan.me/tags/%E6%BB%A4%E6%B3%A2/"/>
    
    <category term="傅里叶级数" scheme="http://chuquan.me/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E7%BA%A7%E6%95%B0/"/>
    
    <category term="卷积" scheme="http://chuquan.me/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>计算机图形学基础（3）——观测变换</title>
    <link href="http://chuquan.me/2024/03/23/foundation-of-computer-graphic-03/"/>
    <id>http://chuquan.me/2024/03/23/foundation-of-computer-graphic-03/</id>
    <published>2024-03-23T09:29:30.000Z</published>
    <updated>2024-04-07T14:31:53.927Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章我们介绍了计算机图形学中的数学基础，包括：2D 变换、3D变换、齐次坐标等。本文，我们则来介绍将三维模型投影到二维屏幕的数学原理。</p><span id="more"></span><h1 id="观测变换">观测变换</h1><p>我们将三维模型投影到二维屏幕的过程称之为<strong>观测变换</strong>（Viewing Transformation）。</p><p>事实上，观测变换和我们平时拍照一样，总体可以分成三个步骤：</p><ul><li>摆放物体。在图形学中称为 <strong>模型变换</strong>（ModelTransformation）</li><li>摆放相机。在图形学中称为 <strong>视图变换</strong>（ViewTransformation）</li><li>拍照。在图形学中称为 <strong>投影变换</strong>（ProjectTransformation）</li></ul><p>根据这三个步骤的英文缩写，观测变换也可以称为 <strong>MVP变换</strong>。不过在图形学中，并不是严格按照这个顺序来执行的，而是先进行视图变换，再进行模型变换。至于为什么，我们稍后再解释。</p><p>下面，我们来分别介绍这三种变换。</p><h1 id="视图变换">视图变换</h1><p>视图变换也称为相机变换（CameraTransformation），视图的内容本质上是由相机的位置决定的，因此这里我们真正要做的是相机变换。</p><p>首先，我们使用如下三个向量来描述相机的<strong>原始位置</strong>，从而唯一确定其位置、观测方向、画面方向。</p><ul><li>位置：<span class="math inline">\(\vec{e}\)</span></li><li>观测方向：<span class="math inline">\(\hat{g}\)</span></li><li>向上方向：<span class="math inline">\(\hat{t}\)</span></li></ul><p>为了方便后续的计算，我们将相机放置到空间坐标系的原点，具体如下：</p><ul><li>位置：原点坐标</li><li>观测方向：<code>-Z</code></li><li>向上方向：<code>Y</code></li></ul><p>这里我们将变换后的观测方向设置为<code>-Z</code>，而在有些渲染引擎中观测方向为<code>Z</code>。这主要取决于空间坐标系的定义，本文我们使用的是右手坐标系。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-01.png?x-oss-process=image/resize,w_800" /></p><h2 id="如何变换">如何变换？</h2><p>那么具体我们该如何进行变换呢？一种非常直观的方法，按照四个步骤进行变换：</p><ul><li>将 <span class="math inline">\(\vec{e}\)</span> 平移变换至原点</li><li>将 <span class="math inline">\(\hat{g}\)</span> 旋转变换至<code>-Z</code></li><li>将 <span class="math inline">\(\hat{t}\)</span> 旋转变换至<code>Y</code></li><li>将 <span class="math inline">\(\hat{g} \times \hat{t}\)</span>旋转变换至 <code>X</code></li></ul><p>很显然，变换矩阵为平移变换和旋转变换的组合，即 <spanclass="math inline">\(M_{view} =R_{view}T_{view}\)</span>。其中，我们很容易就能求解平移变换的变换矩阵，如下。</p><span class="math display">\[\begin{aligned}T_{view}=\left(\begin{matrix}1 &amp; 0 &amp; 0 &amp; -x_e \\0 &amp; 1 &amp; 0 &amp; -y_e \\0 &amp; 0 &amp; 1 &amp; -z_e \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><p>这里的难点在于求解几个旋转变换的变换矩阵 <spanclass="math inline">\(R_{view}\)</span>。那么，该如何求解呢？这里我们转换一下思路，考虑将位于原点的目标位置逆向转换至原始位置。通过这种方式我们可以得到<span class="math inline">\(R_{view}\)</span> 的逆矩阵 <spanclass="math inline">\(R_{view}^{-1}\)</span>。具体求解过程如下所示。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}? &amp; ? &amp; ? &amp; 0 \\? &amp; ? &amp; ? &amp; 0 \\? &amp; ? &amp; ? &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}1 \\0 \\0 \\0 \\\end{matrix}\right)= &amp;\left(\begin{matrix}x_{\hat{g} \times \hat{t}} \\y_{\hat{g} \times \hat{t}} \\z_{\hat{g} \times \hat{t}} \\0 \\\end{matrix}\right)\\\\\left(\begin{matrix}? &amp; ? &amp; ? &amp; 0 \\? &amp; ? &amp; ? &amp; 0 \\? &amp; ? &amp; ? &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}0 \\1 \\0 \\0 \\\end{matrix}\right)= &amp;\left(\begin{matrix}x_{t} \\y_{t} \\z_{t} \\0 \\\end{matrix}\right)\\\\\left(\begin{matrix}? &amp; ? &amp; ? &amp; 0 \\? &amp; ? &amp; ? &amp; 0 \\? &amp; ? &amp; ? &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}0 \\0 \\1 \\0 \\\end{matrix}\right)= &amp;\left(\begin{matrix}x_{-g} \\y_{-g} \\z_{-g} \\0 \\\end{matrix}\right)\\\\解得：R_{view}^{-1}= &amp;\left(\begin{matrix}x_{\hat{g} \times \hat{t}} &amp; x_{t} &amp; x_{-g} &amp; 0 \\y_{\hat{g} \times \hat{t}} &amp; y_{t} &amp; y_{-g} &amp; 0 \\z_{\hat{g} \times \hat{t}} &amp; z_{t} &amp; z_{-g} &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><p>由于旋转矩阵是正交矩阵，所以旋转矩阵的逆矩阵就是它的转置矩阵。由此得到：</p><span class="math display">\[\begin{aligned}R_{view}=\left(\begin{matrix}x_{\hat{g} \times \hat{t}} &amp; y_{\hat{g} \times \hat{t}} &amp;z_{\hat{g} \times \hat{t}} &amp; 0 \\x_{t} &amp; y_{t} &amp; z_{t} &amp; 0 \\x_{-g} &amp; y_{-g} &amp; z_{-g} &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><h1 id="模型变换">模型变换</h1><p>根据相对性原理，相机完成了特定的变换后，我们也需要对模型进行同样的变换，这样通过相机投影得到的画面才会相对不变。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-02.png?x-oss-process=image/resize,w_800" /></p><p>根据上述的相机变换，我们得到了对应的变换矩阵。根据此变换矩阵，我们再对空间中的所有模型进行变换，即完成了模型变换。之后，我们即可进行投影变换。</p><p>由模型和相机要进行相同的变换，因此也将模型变换和视图变换统称为<strong>模型视图变换</strong>（ModelView Transformation）。</p><h1 id="投影变换">投影变换</h1><p>投影变换本质上就是将 3D 模型投影到 2D画布的过程，具体可以分为两种：</p><ul><li>正交投影（OrthographicProjection）：一般用于工程制图软件，不具有近大远小的透视效果。</li><li>透视投影（PerspectiveProjection）：一般用于游戏引擎、渲染引擎，模拟真实的效果。</li></ul><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-03.png?x-oss-process=image/resize,w_800" /></p><p>事实上，正交投影可以认为是一种特殊的透视投影，即相机位于无限远的位置，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-04.png?x-oss-process=image/resize,w_800" /></p><h2 id="正交投影">正交投影</h2><p>下面，我们先来介绍一下正交投影的两种方法。</p><h3 id="方法一">方法一</h3><p>方法一非常直观，即丢弃 Z 坐标，直接转换成二维坐标系，然后再将其缩放至<span class="math inline">\([-1, 1]^2\)</span>的矩形区域，如下所示。为什么要缩放至 <span class="math inline">\([-1,1]^2\)</span>的矩形区域？事实上，这也是为了方便后续计算，是一种约定俗成的做法。当然，这种方式也存在一个问题，无法直接判断模型之间的远近关系，这个我们后续再讨论。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-05.png?x-oss-process=image/resize,w_800" /></p><h3 id="方法二">方法二</h3><p>不过，更普遍的做法是方法二，包括后续的透视投影也采用了这种方法。</p><p>方法二提出了一个 <strong>观测空间</strong>（ViewVolumne）的概念，这一点非常重要。对于正交投影，它的观测空间是一个无限长的长方体，其中以2D 画布为近面，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-09.png?x-oss-process=image/resize,w_800" /></p><p>由于 2D画布可能是任意比例的矩形，为了方便计算，我们将这个长方体的观测空间转换成成一个规范立方体（CanonicalCube），即 <span class="math inline">\([-1, 1]^3\)</span> 的空间。</p><p>在将观测空间转换成规范立方体的过程中，我们会组合平移、缩放等变换，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-06.png?x-oss-process=image/resize,w_800" /></p><p>很显然，要将模型转换成标准立方体，我们必须计算出变换矩阵 <spanclass="math inline">\(M_{ortho}\)</span>。由于投影变换不涉及旋转，因此变换矩阵相对而言比较容易求解，如下所示。</p><span class="math display">\[\begin{aligned}M_{ortho}=S_{ortho}T_{ortho}=\left(\begin{matrix}\frac{2}{r-l} &amp; 0 &amp; 0 &amp; 0 \\0 &amp; \frac{2}{t-b} &amp; 0 &amp; 0 \\0 &amp; 0 &amp; \frac{2}{n-f} &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}1 &amp; 0 &amp; 0 &amp; -\frac{r+l}{2} \\0 &amp; 1 &amp; 0 &amp; -\frac{t+b}{2} \\0 &amp; 0 &amp; 1  &amp; -\frac{n+f}{2} \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><p>在将观测空间转换成规范立方体的过程中，我们计算得到了变换矩阵 <spanclass="math inline">\(M_{ortho}\)</span>。根据相对不变性原理，我们要使用<span class="math inline">\(M_{ortho}\)</span>对空间中所有物体进行同样的变换。这个过程，这里我们不再赘述。</p><h2 id="透视投影">透视投影</h2><p>透视投影则借鉴了正交投影的做法，只不过相对而言，它多了一步压缩过程，也就是说，透视投影= 压缩 + 正交投影。</p><p>下面，我们重点介绍一下压缩。</p><h3 id="压缩">压缩</h3><p>透视投影不同于正交投影，它的观测空间是一个无限长的纺锤体，其中以 2D画布为近面，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-10.png?x-oss-process=image/resize,w_800" /></p><p>压缩的本质就是将透视投影的观测空间压缩成正交投影的观测空间，即将纺锥体转换成长方体。然后，透视投影就换转化成了正交投影了。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-11.png?x-oss-process=image/resize,w_800" /></p><!--模型压缩本质上并不是真正对模型进行压缩，而是一种透视压缩。如下图所示，透视投影将空间中的一个点投影到一个 2D 的画布上，坐标点 `(x, y, z)` 中 `x` 和 `y` 的值会产生压缩效应，变成 `x'` 和 `y'`。--><!--![](https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-07.png?x-oss-process=image/resize,w_800)--><!--因此，我们可以想像画布上的投影点和空间点构成一个纺椎体，然后对这个纺椎体进行压缩，如下图所示。`(x, y, z)` 经过压缩后会变成 `(x', y', z)`，再经过正交投影后会得到 `(x', y', z')`。--><p>那么，我们该如何求解压缩变换的变换矩阵 <spanclass="math inline">\(M_{persp-&gt;ortho}\)</span> 呢？</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-07.png?x-oss-process=image/resize,w_800" /></p><p>首先，由相似三角形定理，如上图所示，我们可以得出：</p><span class="math display">\[\begin{aligned}y^{&#39;}=\frac{n}{z}y;x^{&#39;}=\frac{n}{z}x\end{aligned}\]</span><p>然后，我们基于齐次坐标，结合三角形定理，计算得出投影点的坐标：</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x^{&#39;} \\y^{&#39;} \\z^{&#39;} \\1 \\\end{matrix}\right)=\left(\begin{matrix}nx/z \\ny/z \\? \\1 \\\end{matrix}\right)=\left(\begin{matrix}nx \\ny \\? \\z \\\end{matrix}\right)\end{aligned}\]</span><p>接下来，我们准备求解变换矩阵 <spanclass="math inline">\(M_{persp-&gt;ortho}^{4 \times4}\)</span>，得出一下关系式：</p><span class="math display">\[\begin{aligned}M_{persp-&gt;ortho}\left(\begin{matrix}x \\y \\z \\1 \\\end{matrix}\right)= &amp;\left(\begin{matrix}nx \\ny \\? \\z \\\end{matrix}\right)\\解得：M_{persp-&gt;ortho}= &amp;\left(\begin{matrix}n &amp; 0 &amp; 0 &amp; 0 \\0 &amp; n &amp; 0 &amp; 0 \\? &amp; ? &amp; ? &amp; ? \\0 &amp; 0 &amp; 1 &amp; 0 \\\end{matrix}\right)\end{aligned}\]</span><p>最后，我们来求解第三行的值。我们基于两个以下两个依据：</p><ul><li>近平面上的点的值不会变化，即 2D 画布上的值不变。</li><li>远平面上的在 Z 轴上的点不会变化。</li></ul><p>根据第一个依据，我们可以得出以下关系式。即将 <code>z</code> 替换成<code>n</code>。</p><span class="math display">\[\begin{aligned}M_{persp-&gt;ortho}\left(\begin{matrix}x \\y \\z \\1 \\\end{matrix}\right)=M_{persp-&gt;ortho}\left(\begin{matrix}x \\y \\n \\1 \\\end{matrix}\right)= &amp;\left(\begin{matrix}x \\y \\n \\1 \\\end{matrix}\right)=\left(\begin{matrix}nx \\ny \\n^2 \\n \\\end{matrix}\right)\\推导：\left(\begin{matrix}? &amp; ? &amp; ? &amp; ? \\\end{matrix}\right)\left(\begin{matrix}x \\y \\n \\1 \\\end{matrix}\right)= &amp;n^2\\解得：\left(\begin{matrix}? &amp; ? &amp; ? &amp; ? \\\end{matrix}\right)= &amp;\left(\begin{matrix}0 &amp; 0 &amp; ? &amp; ? \\\end{matrix}\right)\end{aligned}\]</span><p>我们使用 <code>(0, 0, A, B)</code> 抽象表示<code>(0, 0, ?, ?)</code>。根据两条依据，我们可以得到一个二元一次方程组，如下所示。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}0 &amp; 0 &amp; A &amp; B \\\end{matrix}\right)\left(\begin{matrix}x \\y \\n \\1 \\\end{matrix}\right)= &amp;n^2=&gt; &amp;An + B = n^2\\\left(\begin{matrix}0 &amp; 0 &amp; A &amp; B \\\end{matrix}\right)\left(\begin{matrix}0 \\0 \\f \\1 \\\end{matrix}\right)= &amp;\left(\begin{matrix}0 \\0 \\f^2 \\f \\\end{matrix}\right)=&gt; &amp;Af + B = f^2\\解得：A= &amp;n + f\\B= &amp;-nf\end{aligned}\]</span><p>综上述，求解得出压缩变换的变换矩阵如下所示，其中 f是一个动态值，即空间点 <code>(x, y, z)</code> 的 <code>z</code> 值。</p><span class="math display">\[\begin{aligned}M_{persp-&gt;ortho}=\left(\begin{matrix}n &amp; 0 &amp; 0 &amp; 0 \\0 &amp; n &amp; 0 &amp; 0 \\0 &amp; 0 &amp; n+f &amp; -nf \\0 &amp; 0 &amp; 1 &amp; 0 \\\end{matrix}\right)\end{aligned}\]</span><p>对于透视投影，我们首先求解观测空间的压缩变换的变换矩阵 <spanclass="math inline">\(M_{persp-&gt;ortho}\)</span>，然后再利用在将转换后的长方体观测空间转换成规范立方体，即上文正交投影中求解的<span class="math inline">\(M_{ortho}\)</span>。</p><p>当然，根据相对不变性原理，我们还要将这两个变换矩阵应用到空间中所有的物体上，对它们进行变换。</p><h1 id="屏幕映射">屏幕映射</h1><p>当 MVP 变换完成之后，我们则要开始将投影内容绘制到 2D画布中，其中包含了裁剪和视口变换两个步骤。</p><h2 id="裁剪">裁剪</h2><p>无论是正交投影还是透视投影，我们都将观测空间转换成了一个规范立方体，同时将转换矩阵应用到空间中的所有物体中。</p><p>之后，我们就可以通过规范立方体对空间进行裁剪，只保留规范立方体内的物体，如下所示。很显然，只有在规范立方体中的部分才是我们可以看见的部分。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-12.png?x-oss-process=image/resize,w_800" /></p><h2 id="视口变换">视口变换</h2><p>视口（Viewport）本质上就是我们所说的 2D画布，即屏幕。我们知道屏幕有各种各样的分辨率，宽高比。为了处理这种情况，我们将2D 画布抽象成一个 <span class="math inline">\([-1, 1]^2\)</span>的规范平面。然后通过视口变换将它映射到真正的视口中。</p><p>假设真实视口的宽度是 <spanclass="math inline">\(width\)</span>，高度是 <spanclass="math inline">\(height\)</span>，那么视口变换就是将 <spanclass="math inline">\([-1, 1]^2\)</span> 的平面转换成 <spanclass="math inline">\([0, width] \times [0, height]\)</span>的平面。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/projection-13.png?x-oss-process=image/resize,w_800" /></p><p>对此，我们很容易求解变换矩阵，如下所示。</p><span class="math display">\[\begin{aligned}M_{viewport}=\left(\begin{matrix}\frac{width}{2} &amp; 0 &amp; 0 &amp; \frac{width}{2} \\0 &amp; \frac{height}{2} &amp; 0 &amp; \frac{height}{2} \\0 &amp; 0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0 \\\end{matrix}\right)\end{aligned}\]</span><h1 id="总结">总结</h1><p>本文，我们主要介绍了观测变换的几个重点内容，包括视图变换、投影变换。其中，我们重点介绍了投影变换中的两种：正交投影和透视投影。</p><p>投影变换中提到了一个重要概念——观测空间。我们会将观测空间转换成一个规范立方体，根据相对不变性原理，对空间中所有物体做同样的变换。其中透视投影稍有复杂一点，我们会将纺锤体的观测空间转换成长方体的观测空间。</p><p>最后，我们将规范立方体以外的内容进行裁剪，并采用视口变换将内容映射到具体的屏幕上。</p><p>后面，我们将基于本章的内容继续介绍计算机图形学的相关基础。</p><h1 id="参考">参考</h1><ol type="1"><li>《GAMES 101》</li><li>Image Processing and Computer Graphics——Rendering Pipeline, MatthiasTeschner.</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;上一篇文章我们介绍了计算机图形学中的数学基础，包括：2D 变换、3D
变换、齐次坐标等。本文，我们则来介绍将三维模型投影到二维屏幕的数学原理。&lt;/p&gt;</summary>
    
    
    
    <category term="图形学" scheme="http://chuquan.me/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="观测变换" scheme="http://chuquan.me/tags/%E8%A7%82%E6%B5%8B%E5%8F%98%E6%8D%A2/"/>
    
    <category term="投影变换" scheme="http://chuquan.me/tags/%E6%8A%95%E5%BD%B1%E5%8F%98%E6%8D%A2/"/>
    
    <category term="正交投影" scheme="http://chuquan.me/tags/%E6%AD%A3%E4%BA%A4%E6%8A%95%E5%BD%B1/"/>
    
    <category term="透视投影" scheme="http://chuquan.me/tags/%E9%80%8F%E8%A7%86%E6%8A%95%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>我的独立产品集</title>
    <link href="http://chuquan.me/2024/01/18/my-indie-hacker-products/"/>
    <id>http://chuquan.me/2024/01/18/my-indie-hacker-products/</id>
    <published>2024-01-18T15:09:42.000Z</published>
    <updated>2024-04-01T15:03:03.353Z</updated>
    
    <content type="html"><![CDATA[<p>本文罗列了一些我业余时间开发的独立产品，如果我写的博客对你有些许帮助，那我也诚邀你下载体验一下我开发的独立产品。当然，如果能够针对产品提出一些建议或给个好评，那真的不胜感激！这也会是我继续完善产品的动力！</p><span id="more"></span><h1 id="macos-应用">MacOS 应用</h1><h2 id="莫负休息">莫负休息</h2><p>莫负休息（Morph Rest）是一款 MacOS休息提醒应用程序。通过定时休息，可以预防视力疲劳、腰间盘突出、颈椎疼痛等职业病，当然也可以辅助提醒喝水，避免尿酸过高，引发肾结石、痛风等疾病。</p><p>莫负休息的主要特性：</p><ul><li>最低支持 MacOS 10.15 系统</li><li>支持浅色模式、深色模式</li><li>支持简体中文、繁体中文、英语、日语、韩语等多种语言</li><li>支持用户自定义工作、休息间隔</li><li>提供多种主题用于在休息期间展示</li></ul><p>下载地址——<ahref="https://apps.apple.com/cn/app/%E8%8E%AB%E8%B4%9F%E4%BC%91%E6%81%AF-%E4%BC%91%E6%81%AF%E6%8F%90%E9%86%92/id6474056217?mt=12">传送门</a></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/morph-rest/0.9.0-common-market.png?x-oss-process=image/resize,w_800" /></p><h2 id="莫负时钟">莫负时钟</h2><p>莫负时钟（Morph Clock）是一款 MacOS屏幕保护程序。它采用了一种你从未见过的动态时钟效果，让你的 Mac成为办公室中最靓的仔~</p><p>莫负时钟的主要特性：</p><ul><li>最低支持 MacOS 10.14 系统。</li><li>动态渐变背景色，无时无刻都在变换颜色。</li></ul><p>下载地址——<ahref="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/morph-clock/MorphClock.saver.zip">传送门</a></p><div data-align="center"><video src="http://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/morph-clock/MorphClock.mov" type="video/mp4" controls="controls" width="60%" height="60%"></video></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文罗列了一些我业余时间开发的独立产品，如果我写的博客对你有些许帮助，那我也诚邀你下载体验一下我开发的独立产品。当然，如果能够针对产品提出一些建议或给个好评，那真的不胜感激！这也会是我继续完善产品的动力！&lt;/p&gt;</summary>
    
    
    
    <category term="作品集" scheme="http://chuquan.me/categories/%E4%BD%9C%E5%93%81%E9%9B%86/"/>
    
    
    <category term="Indie Hacker" scheme="http://chuquan.me/tags/Indie-Hacker/"/>
    
  </entry>
  
  <entry>
    <title>计算机图形学基础（2）——变换</title>
    <link href="http://chuquan.me/2024/01/18/foundation-of-computer-graphic-02/"/>
    <id>http://chuquan.me/2024/01/18/foundation-of-computer-graphic-02/</id>
    <published>2024-01-18T13:29:31.000Z</published>
    <updated>2024-03-18T01:46:47.645Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇<ahref="http://chuquan.me/2024/01/15/foundation-of-computer-graphic-01/">文章</a>我们介绍了计算机图形学中的线性代数基础，包括：点、向量、矩阵等。本文，我们将介绍向量和矩阵的进一步应用——变换。</p><span id="more"></span><h1 id="概述">概述</h1><p>计算机图形学中，我们可能会对图形进行各种变换（Transform），如：</p><ul><li>缩放（Scale）</li><li>平移（Transation）</li><li>旋转（Rotation）</li><li>切变（Shear）</li></ul><h1 id="d-变换">2D 变换</h1><p>首先，我们来介绍一下 2D变换，以便了解变换是如何通过矩阵变换来实现的。</p><h2 id="缩放变换">缩放变换</h2><p>对于缩放变换，它主要包含两种：等比例缩放、非等比缩放。</p><h3 id="等比例缩放">等比例缩放</h3><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-01.png?x-oss-process=image/resize,w_800" /></p><p>上图所示，为等比例缩放的示意图。根据等比例缩放的规则，我们可以根据缩放前<span class="math inline">\(x\)</span> 和 <spanclass="math inline">\(y\)</span> 的值，得到一组关系式，如下所示。</p><span class="math display">\[\begin{aligned}x&#39; =&amp; sx \\y&#39; =&amp; sy \\\end{aligned}\]</span><p>根据此关系式，我们可以进一步推导出缩放矩阵及关系式，如下所示。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39;\end{matrix}\right)=\left(\begin{matrix}s &amp; 0 \\0 &amp; s \\\end{matrix}\right)\left(\begin{matrix}x \\y\end{matrix}\right)\end{aligned}\]</span><h3 id="非等比缩放">非等比缩放</h3><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-02.png?x-oss-process=image/resize,w_800" /></p><p>上图所示，为非等比缩放的示意图。根据非比缩放的规则，我们可以根据缩放前<span class="math inline">\(x\)</span> 和 <spanclass="math inline">\(y\)</span> 的值，得到另一组关系式，如下所示。</p><span class="math display">\[\begin{aligned}x&#39; =&amp; s_xx \\y&#39; =&amp; s_yy \\\end{aligned}\]</span><p>根据此关系式，我们可以进一步推导出缩放矩阵及关系式，如下所示。对比一下，非等比缩放与等比例缩放的关系式非常相似。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\\end{matrix}\right)=\left(\begin{matrix}s_x &amp; 0 \\0 &amp; s_y \\\end{matrix}\right)\left(\begin{matrix}x \\y \\\end{matrix}\right)\end{aligned}\]</span><h2 id="镜像变换">镜像变换</h2><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-03.png?x-oss-process=image/resize,w_800" /></p><p>上图所示，为镜像变换的示意图。我们可以根据原始的 <spanclass="math inline">\(x\)</span> 和 <spanclass="math inline">\(y\)</span> 的值，得到一组关系式，如下所示。</p><span class="math display">\[\begin{aligned}x&#39; =&amp; -x \\y&#39; =&amp; y \\\end{aligned}\]</span><p>根据此关系式，我们可以进一步推导出镜像矩阵及关系式，如下所示。本质上，镜像变换是一种特殊的缩放变换。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\\end{matrix}\right)=\left(\begin{matrix}-1 &amp; 0 \\0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}x \\y\end{matrix}\right)\end{aligned}\]</span><h2 id="切变变换">切变变换</h2><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-04.png?x-oss-process=image/resize,w_800" /></p><p>上图所示，为切变变换的示意图。切变变换相对复杂一点，其 <spanclass="math inline">\(y\)</span> 坐标值与 <spanclass="math inline">\(x\)</span>坐标值成一个比例关系。不过，我们仍然可以根据原始的 <spanclass="math inline">\(x\)</span> 和 <spanclass="math inline">\(y\)</span> 的值，得到一组关系式，如下所示。</p><span class="math display">\[\begin{aligned}x&#39; =&amp; x + ay \\y&#39; =&amp; y \\\end{aligned}\]</span><p>根据此关系式，我们可以进一步推导出镜像矩阵及关系式，如下所示。本质上，镜像变换是一种特殊的缩放变换。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\\end{matrix}\right)=\left(\begin{matrix}1 &amp; a \\0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}x \\y \\\end{matrix}\right)\end{aligned}\]</span><h2 id="旋转变换">旋转变换</h2><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-05.png?x-oss-process=image/resize,w_800" /></p><p>上图所示，为旋转变换的示意图。旋转变换的坐标推导需要借助三角函数，最终可得到如下一组关系式。</p><span class="math display">\[\begin{aligned}x&#39; = cos{\theta}x - sin{\theta}y \\y&#39; = sin{\theta}x + cos{\theta}y \\\end{aligned}\]</span><p>根据此关系式，我们可以进一步推导出旋转矩阵及关系式，如下所示。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\\end{matrix}\right)=\left(\begin{matrix}cos\theta &amp; -sin\theta \\sin\theta &amp; cos\theta \\\end{matrix}\right)\left(\begin{matrix}x \\y \\\end{matrix}\right)\end{aligned}\]</span><h2 id="平移变换">平移变换</h2><p>截止目前位置，所有的的变换都可以通过推导得出一个变换矩阵，以此矩阵乘以任意点（以矩阵表示），都可以得到转换后的点（以矩阵表示），符合线性变换。</p><p>下面，我们来看一下比较特殊的平移变换。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-06.png?x-oss-process=image/resize,w_800" /></p><p>上图所示，为平移变换的示意图，同样，我们也可以可得到如下一组关系式。</p><span class="math display">\[\begin{aligned}x&#39; = x + t_x \\y&#39; = y + t_y \\\end{aligned}\]</span><p>但是，我们进一步推导，得到的关系式与之前的变换不同，它有额外的偏移量，不符合线性变换，如下所示。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\\end{matrix}\right)=\left(\begin{matrix}1 &amp; 0 \\0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}x \\y \\\end{matrix}\right)+\left(\begin{matrix}t_x \\t_y \\\end{matrix}\right)\end{aligned}\]</span><p>我们总是希望能使用一个统一的关系式来描述各种变换，然而，平移变换打破了我们的美好预期。那么该如何解决呢？为此，我们引入了齐次坐标。</p><h1 id="齐次坐标">齐次坐标</h1><p>为了能够统一表示所有变换，我们引入了<strong>齐次坐标</strong>（HomogenousCoordinates）。这里的核心思想是为每一个点或向量添加一个额外的 <spanclass="math inline">\(w\)</span> 坐标。</p><span class="math display">\[\begin{aligned}2D 点的齐次坐标表示：\left(\begin{matrix}x \\y \\1\end{matrix}\right)\\\\2D 向量的齐次坐标表示：\left(\begin{matrix}x \\y \\0 \\\end{matrix}\right)\end{aligned}\]</span><p>此时，我们再来尝试推导平移变换矩阵以及其关系式，可以得到如下所示内容。很显然，原来关系式中的偏移量没有了。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\w&#39; \\\end{matrix}\right)=\left(\begin{matrix}1 &amp; 0 &amp; t_x \\0 &amp; 1 &amp; t_y \\0 &amp; 0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}x \\y \\1 \\\end{matrix}\right)=\left(\begin{matrix}x+t_x \\y+t_y \\1 \\\end{matrix}\right)\end{aligned}\]</span><h2 id="仿射变换与线性变换">仿射变换与线性变换</h2><p>我们将线性变换和平移变换的组合，称为<strong>仿射变换</strong>（AffineTransform），如下所示。在未引入齐次坐标之前，我们推导出来的平移变换就是一种仿射变换。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\\end{matrix}\right)=\left(\begin{matrix}a &amp; b \\c &amp; d \\\end{matrix}\right)\left(\begin{matrix}x \\y \\\end{matrix}\right)+\left(\begin{matrix}t_x \\t_y \\\end{matrix}\right)\end{aligned}\]</span><p>当引入齐次坐标之后，所有的变换都可以统一使用线性变换来表示，如下所示。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\1 \\\end{matrix}\right)=\left(\begin{matrix}a &amp; b &amp; t_x \\c &amp; d &amp; t_y \\0 &amp; 0 &amp; 1 \\\end{matrix}\right)\left(\begin{matrix}x \\y \\1 \\\end{matrix}\right)\end{aligned}\]</span><p>如下所示，是引入齐次坐标后，缩放变换，旋转变换，平移变换所对应的变换矩阵。</p><span class="math display">\[\begin{aligned}缩放变换：&amp;S(s_x, s_y) =\left(\begin{matrix}s_x &amp; 0 &amp; 0 \\0 &amp; s_y &amp; 0 \\0 &amp; 0 &amp; 1 \\\end{matrix}\right)\\\\旋转变换：&amp;R(\alpha) =\left(\begin{matrix}cos\alpha &amp; -sin\alpha &amp; 0 \\sin\alpha &amp; cos\alpha &amp; 0 \\0 &amp; 0 &amp; 1 \\\end{matrix}\right)\\\\平移变换：&amp;T(t_x, t_y) =\left(\begin{matrix}1 &amp; 0 &amp; t_x \\0 &amp; 1 &amp; t_y \\0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><h1 id="逆变换">逆变换</h1><p>我们将所有的反向变换都称为 <strong>逆变换</strong>（InverseTransform），比如：我们将从 A 平移到 B 称为平移变换，那么从 B 平移到 A则可称为逆变换，其他的缩放变换、旋转变换同样如此。</p><p>上一节，我们引入了齐次坐标后，所有的变换都可以转换成线性变换，其中以<span class="math inline">\(M\)</span>为变换矩阵。而这些变换的逆变换，同样可以使用线性变换来表示，并以 <spanclass="math inline">\(M\)</span> 的逆矩阵 <spanclass="math inline">\(M^{-1}\)</span> 为变换矩阵。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-07.png?x-oss-process=image/resize,w_800" /></p><h1 id="组合变换">组合变换</h1><p>在真实情况下，我们遇到的变换大多数都是组合变换，也就是同时包含了缩放、旋转、平移等多种变换。</p><p>多种变换组合时，变换的顺序其实是非常重要的，我们以如下一个例子来进行介绍。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-08.png?x-oss-process=image/resize,w_800" /></p><p>对于上面这种变换，如果我们先平移，再旋转，那么最终会变成如下所示的。这里的根本原因在于旋转变换时，仍然是以坐标原点为锚点进行旋转。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-09.png?x-oss-process=image/resize,w_800" /></p><p>对此，正确的顺序应该是先旋转，后平移，这样才能达到预期的效果。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/transform-10.png?x-oss-process=image/resize,w_800" /></p><p>不同的顺序，矩阵变换的结果完全不同。前一篇文章我们提到过矩阵乘法不符合交换律，从这一点其实也能够解释这个现象。</p><p>在实际开发中，遇到这种类似的情况，我们一般都会先将目标平移至原点，然后进行各种其他变换，然后再通过逆变换平移回去。</p><h1 id="d-变换-1">3D 变换</h1><p>关于 3D 变换，本质上与 2D变换一样，只不过在矩阵表示上多了一个维度而已。</p><p>当我们引入齐次坐标之后，3D 的点和向量可以采用如下方式表示。</p><span class="math display">\[\begin{aligned}3D点的齐次坐标表示：&amp;\left(\begin{matrix}x \\y \\z \\1 \\\end{matrix}\right)\\\\3D向量的齐次坐标表示：&amp;\left(\begin{matrix}x \\y \\z \\0 \\\end{matrix}\right)\end{aligned}\]</span><p>与此对应，3D 变换的矩阵变换关系式为如下所示。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}x&#39; \\y&#39; \\z&#39; \\1 \\\end{matrix}\right)=\left(\begin{matrix}a &amp; b &amp; c &amp; t_x \\d &amp; e &amp; f &amp; t_y \\g &amp; h &amp; i &amp; t_z \\0 &amp; 0 &amp; 0 &amp;1 \\\end{matrix}\right)\left(\begin{matrix}x \\y \\z \\1 \\\end{matrix}\right)\end{aligned}\]</span><h2 id="缩放变换-1">缩放变换</h2><p>如下所示，为 3D 空间中的缩放变换的变换矩阵的定义。</p><span class="math display">\[\begin{aligned}S(s_x, s_y, s_z)=\left(\begin{matrix}s_x &amp; 0 &amp; 0 &amp; 0 \\0 &amp; s_y &amp; 0 &amp; 0 \\0 &amp; 0 &amp; s_z &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><h2 id="平移变换-1">平移变换</h2><p>如下所示，为 3D 空间中的平移变换的变换矩阵的定义。</p><span class="math display">\[\begin{aligned}T(t_x, t_y, t_z)=\left(\begin{matrix}1 &amp; 0 &amp; 0 &amp; t_x \\0 &amp; 1 &amp; 0 &amp; t_y \\0 &amp; 0 &amp; 1 &amp; t_z \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><h2 id="旋转变换-1">旋转变换</h2><p>如下所示，为 3D空间中的旋转变换的变换矩阵的定义，沿着不同的轴旋转，变换矩阵的定义也有所不同。</p><span class="math display">\[\begin{aligned}R_x(\alpha)=\left(\begin{matrix}1 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; cos\alpha &amp; -sin\alpha &amp; 0 \\0 &amp; sin\alpha &amp; cos\alpha &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\\\\R_y(\alpha)=\left(\begin{matrix}cos\alpha &amp; 0 &amp; sin\alpha &amp; 0 \\0 &amp; 1 &amp; 0 &amp; 0 \\-sin\alpha &amp; 0 &amp; cos\alpha &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\\\\R_z(\alpha)=\left(\begin{matrix}cos\alpha &amp; -sin\alpha &amp; 0 &amp; 0 \\sin\alpha &amp; cos\alpha &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><h1 id="总结">总结</h1><p>本文我们简单梳理了一下缩放、旋转、平移几种变换对应的矩阵关系式。其中，平移变换比较特殊，为了能够统一关系式，我们引入了齐次坐标，在点、向量的矩阵表示中增加了一个维度。然后，我们介绍了一下在组合变换中变换顺序的重要性。最后，我们简单总结了3D 变换的矩阵关系式。</p><h1 id="参考">参考</h1><ol type="1"><li>《GAMES 101》</li><li>《计算机图形学入门：3D渲染指南》</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;上一篇&lt;a
href=&quot;http://chuquan.me/2024/01/15/foundation-of-computer-graphic-01/&quot;&gt;文章&lt;/a&gt;我们介绍了计算机图形学中的线性代数基础，包括：点、向量、矩阵等。本文，我们将介绍向量和矩阵的进一步应用——变换。&lt;/p&gt;</summary>
    
    
    
    <category term="图形学" scheme="http://chuquan.me/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="缩放变换" scheme="http://chuquan.me/tags/%E7%BC%A9%E6%94%BE%E5%8F%98%E6%8D%A2/"/>
    
    <category term="平移变换" scheme="http://chuquan.me/tags/%E5%B9%B3%E7%A7%BB%E5%8F%98%E6%8D%A2/"/>
    
    <category term="旋转变换" scheme="http://chuquan.me/tags/%E6%97%8B%E8%BD%AC%E5%8F%98%E6%8D%A2/"/>
    
    <category term="切变变换" scheme="http://chuquan.me/tags/%E5%88%87%E5%8F%98%E5%8F%98%E6%8D%A2/"/>
    
    <category term="仿射变换" scheme="http://chuquan.me/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/"/>
    
  </entry>
  
  <entry>
    <title>计算机图形学基础（1）——线性代数</title>
    <link href="http://chuquan.me/2024/01/15/foundation-of-computer-graphic-01/"/>
    <id>http://chuquan.me/2024/01/15/foundation-of-computer-graphic-01/</id>
    <published>2024-01-15T12:09:48.000Z</published>
    <updated>2024-03-18T01:23:03.865Z</updated>
    
    <content type="html"><![CDATA[<p>最近对计算机图形学比较感兴趣，刷了一遍《计算机图形学入门：3D渲染指南》，看了一遍《GAMES101》。本文对计算机图形学的线性代数相关基础进行了梳理和总结，以便后续进行复习和回顾。</p><span id="more"></span><h1 id="点">点</h1><p><strong>点</strong>（Point）表示坐标系中的一个特定位置，其具体表示和抽象表示分别如下。</p><ul><li>使用 <strong>大写字母</strong> 的方式来抽象表示一个点，如：<spanclass="math inline">\(P\)</span>。</li><li>使用 <strong>圆括号 + 数字序列</strong>的方式来具体表示一个点，如：<span class="math inline">\((4,3)\)</span>。</li></ul><p>在具体表示中，数字序列的顺序很重要。按照惯例，在 2D 平面中依次表示<span class="math inline">\(x\)</span>、<spanclass="math inline">\(y\)</span> 轴的值；在 3D 空间中依次表示 <spanclass="math inline">\(x\)</span>、<spanclass="math inline">\(y\)</span>、<span class="math inline">\(z\)</span>轴的值。对此，我们也可以结合两种方式来表示一个抽象的点，比如：<spanclass="math inline">\((P_x, P_y)\)</span>。</p><h1 id="向量">向量</h1><p><strong>向量</strong>（Vector）表示两个点所构成线段的长度和方向，其具体表示和抽象表示分别如下。</p><ul><li>向量的抽象表示一般有三种，分别是：<ul><li>使用 <strong>小写字母 + 箭头上标</strong> 的方式，如：<spanclass="math inline">\(\vec{a}\)</span>。</li><li>使用 <strong>加粗小写字母</strong> 的方式，如：<spanclass="math inline">\(\textbf{a}\)</span>。</li><li>使用 <strong>两个点 + 箭头上标</strong> 的方式，如：<spanclass="math inline">\(\overrightarrow{AB} = B -A\)</span>。注：大写字母表示点。</li></ul></li><li>向量的具体表示中，默认以坐标原点作为起点，因此只需要描述终点即可。向量一般使用矩阵来表示，包含两种方式，分别是：</li></ul><span class="math display">\[\begin{aligned}列向量表示法：&amp;\left(\begin{matrix}x \\y\end{matrix}\right)\\行向量表示法：&amp;\left(\begin{matrix}x &amp; y\end{matrix}\right)\end{aligned}\]</span><p>在定义中我们提到向量包含了两个点之间的长度和方向两种信息。对此，我们可以各自使用一种方式来表示这两种信息。</p><ul><li>使用 <strong>小写字母 + <code>^</code> 上标</strong> 的方式表示<strong>单位向量</strong>（Unit Vector），即长度等于 1 的向量，如：<spanclass="math inline">\(\widehat{a}\)</span>。一般用来表示方向。</li><li>使用 <strong>向量 + 双竖线</strong> 的方式表示<strong>向量长度</strong>（Vector Length），如：<spanclass="math inline">\(|\vec{a}|\)</span>。</li></ul><p>单位向量可以通过向量除以向量长度的方式计算得到，如下所示。</p><span class="math display">\[\begin{aligned}\widehat{a} = \vec{a} / |\vec{a}|\end{aligned}\]</span><p>在计算机图形学中，单位向量的应用非常多，比如：法线向量。在计算光线的折射和反射时，法线必不可少。</p><h2 id="向量的加减运算">向量的加减运算</h2><p>向量的加减运算可以使用 <strong>平行四边形法则</strong> 或<strong>三角形法则</strong> 进行计算，如下图所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/vector-01.png?x-oss-process=image/resize,w_800" /></p><p>向量的加减运算非常简单，只需要把两个向量的对应坐标的值进行加减运算即可，如下所示。</p><span class="math display">\[\begin{aligned}\vec{a} + \vec{b}=\left(\begin{matrix}a_x &amp; a_y\end{matrix}\right)+\left(\begin{matrix}b_x &amp; b_y\end{matrix}\right)=\left(\begin{matrix}a_x + b_x &amp; a_y + b_y\end{matrix}\right)\\\vec{a} - \vec{b}=\left(\begin{matrix}a_x &amp; a_y\end{matrix}\right)-\left(\begin{matrix}b_x &amp; b_y\end{matrix}\right)=\left(\begin{matrix}a_x - b_x &amp; a_y - b_y\end{matrix}\right)\end{aligned}\]</span><h2 id="向量的乘法运算">向量的乘法运算</h2><p>向量的乘法运算比较特殊，它有两种乘法运算，分别是：</p><ul><li><strong>点积</strong>（Dot Product），或称<strong>点乘</strong></li><li><strong>叉积</strong>（Cross Product），或称<strong>叉乘</strong></li></ul><h3 id="点积">点积</h3><p>两个向量之间的点积是一个数值，一般使用 <strong>点运算符</strong>表示。</p><p>点积的运算非常简单，只要将每个向量对应的坐标值相乘并求和即可，如下所示为一个点积的示例。</p><span class="math display">\[\begin{aligned}\vec{a} \cdot \vec{b}=\left(\begin{matrix}a_x &amp; a_y &amp; a_z\end{matrix}\right)\left(\begin{matrix}b_x \\b_y \\b_z \\\end{matrix}\right)=a_x \cdot b_x + a_y \cdot b_y + a_z \cdot b_z\end{aligned}\]</span><p>向量点积的特性</p><ul><li>符合交换律，即 <span class="math inline">\(\vec{a} \cdot \vec{b} =\vec{b} \cdot \vec{a}\)</span></li><li>符合分配律，即 <span class="math inline">\(\vec{a} \cdot (\vec{b} +\vec{c}) = \vec{a} \cdot \vec{b} + \vec{a} \cdot \vec{c}\)</span></li><li>不符合结合律。点乘的结果是一个标量，不是向量，因此无法与另一个向量继续进行点乘运算。</li></ul><p>在计算机图形学中，点积的应用非常广泛，主要包括：</p><ul><li><strong>计算两个向量之间的夹角</strong></li><li><strong>计算一个向量在另一个向量上的投影</strong></li><li><strong>计算一个向量正交分解后的两个向量</strong></li><li><strong>判断一个向量相对于另一个向量是正向还是反向</strong></li></ul><p>下面，我们来看一下这几种应用是如果通过计算实现的。</p><p>首先，如何计算两个向量之间的夹角？在几何上，两个向量的点积与它们的长度以及它们之间的夹角<span class="math inline">\(a\)</span>有关，确切的公式巧妙地将线性代数和三角函数联系在了一起，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/vector-02.png?x-oss-process=image/resize,w_800" /></p><span class="math display">\[\begin{aligned}\vec{a} \cdot \vec{b} =&amp; |\vec{a}| \cdot |\vec{b}| \cdot cos\theta\\cos\theta =&amp; \frac{\vec{a} \cdot \vec{b}}{|\vec{a}| \cdot |\vec{b}|}\\cos\theta =&amp; \widehat{a} \cdot \widehat{b}\end{aligned}\]</span><p>其次，如何计算一个向量在另一个向量上的投影？如下所示，求向量 <spanclass="math inline">\(\vec{b}\)</span> 在向量 <spanclass="math inline">\(\vec{a}\)</span> 上的投影 <spanclass="math inline">\(\vec{b}_\bot\)</span>，很显然，<spanclass="math inline">\(\vec{b}_\bot\)</span> 与 <spanclass="math inline">\(\vec{a}\)</span>的方向是一致的，只是长度可能不同。因此，我们可以通过前面提到的方式计算两者之间的夹角，然后计算投影长度，并使用该长度乘以单位向量<span class="math inline">\(\widehat{a}\)</span> 即可。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/vector-03.png?x-oss-process=image/resize,w_800" /></p><span class="math display">\[\begin{aligned}\vec{b}_\bot =&amp; k \cdot \widehat{a}\\k =&amp; |\vec{b_\bot}| = |\vec{b}| \cdot cos\theta = |\vec{b}| \cdot\widehat{a} \cdot \widehat{b}\\\end{aligned}\]</span><p>接着，如何计算一个向量正交分解后的两个向量？上面我们在计算一个向量在另一个向量上的投影时，已经计算得到了一个方向的分解向量，另一个方向的分解向量我们只需通过向量减法即可得到，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/vector-04.png?x-oss-process=image/resize,w_800" /></p><p>最后，如何判断一个向量相对于另一个向量是正向还是反向？判断两个向量的方向关系，本质上是看两者之间的夹角，如果是锐角，则认为是正向，如果是钝角，则认为是反向，如下所示。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/vector-05.png?x-oss-process=image/resize,w_800" /></p><h3 id="叉积">叉积</h3><p>两个向量之间的叉积是一个向量，一般使用 <strong>叉乘符号</strong>表示。</p><p>叉积是一个垂直于两个向量的向量，其方向可以通过<strong>右手螺旋定则</strong> 确定。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/vector-06.png?x-oss-process=image/resize,w_800" /></p><span class="math display">\[\begin{aligned}\vec{c}= &amp;\vec{a} \times \vec{b}\\= &amp;\left(\begin{matrix}a_x &amp; a_y &amp; a_z\end{matrix}\right)\left(\begin{matrix}b_x \\b_y \\b_z \\\end{matrix}\right)\\= &amp;\left(\begin{matrix}a_y \cdot b_z - a_z \cdot b_y \\a_z \cdot b_x - a_x \cdot b_z \\a_x \cdot b_y - a_y \cdot b_x \\\end{matrix}\right)\\即：\\c_x =&amp; a_y \cdot b_z - a_z \cdot b_y\\c_y =&amp; a_z \cdot b_x - a_x \cdot b_z\\c_z =&amp; a_x \cdot b_y - a_y \cdot b_x\end{aligned}\]</span><p>向量叉积的特性</p><ul><li>符合分配律，即 <span class="math inline">\(\vec{a} \times (\vec{b} +\vec{c}) = \vec{a} \times \vec{b} + \vec{a} \times \vec{c}\)</span></li><li>不符合结合律，即 <span class="math inline">\((\vec{a} \times\vec{b}) \times \vec{c} \neq \vec{a} \times (\vec{b} \times\vec{c})\)</span></li><li>不符合交换律，即 <span class="math inline">\(\vec{a} \times \vec{b}\neq \vec{b} \times \vec{a}\)</span></li></ul><p>在计算机图形学中，乘积的应用主要包括一下这些：</p><ul><li><strong>判断一个向量相对于另一个向量的左右关系</strong></li><li><strong>判断一个向量相对于一个三角形的内外关系</strong></li></ul><p>那么，如何判断一个向量相对于另一个向量的左右关系？可以直接判断两个向量叉积的正负值。如下所示，在一个3D 坐标中，<span class="math inline">\(\vec{a}\)</span> 和 <spanclass="math inline">\(\vec{b}\)</span>的叉乘符合右手螺旋定则的方向（图中与 Y 轴方向相同），则表示 <spanclass="math inline">\(\vec{A}\)</span> 在 <spanclass="math inline">\(\vec{B}\)</span> 的右边，反之则表示 <spanclass="math inline">\(\vec{A}\)</span> 在 <spanclass="math inline">\(\vec{B}\)</span> 的左边。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/vector-07.png?x-oss-process=image/resize,w_800" /></p><p>以及，如何判断一个向量相对于一个三角形的内外关系？事实上，我们可以利用上面这种左右关系判断的方法来组合判断。如下所示，我们可以分别判断<span class="math inline">\(\overrightarrow{AP}\)</span> 和 <spanclass="math inline">\(\overrightarrow{AB}\)</span> 的左右关系，<spanclass="math inline">\(\overrightarrow{BP}\)</span> 和 <spanclass="math inline">\(\overrightarrow{BC}\)</span> 的左右关系，<spanclass="math inline">\(\overrightarrow{CP}\)</span> 和 <spanclass="math inline">\(\overrightarrow{CA}\)</span>的左右关系，如果前者都在后者的一边（左边或右边），那么 <spanclass="math inline">\(P\)</span> 就在三角形内；否则，在三角形外。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/vector-08.png?x-oss-process=image/resize,w_800" /></p><h1 id="矩阵">矩阵</h1><p>矩阵是一个 <span class="math inline">\(m\)</span> 行 <spanclass="math inline">\(n\)</span> 列的数字阵列，通常我们使用一个<strong>加粗的大写字母</strong>抽象表示一个矩阵。在计算机图形学中，它被广泛应用于各种变换计算中，比如：平移、旋转、缩放等。</p><h2 id="矩阵的乘法运算">矩阵的乘法运算</h2><p>矩阵的乘法必须满足一个前提：矩阵 <spanclass="math inline">\(A\)</span> 的列数等于矩阵 <spanclass="math inline">\(B\)</span> 的行数，即<code>(M x N)(N x P) = (M x P)</code>。</p><p>矩阵 <span class="math inline">\(A\)</span> 乘以矩阵 <spanclass="math inline">\(B\)</span> 得到矩阵 <spanclass="math inline">\(C\)</span>，其中矩阵 <spanclass="math inline">\(C\)</span> 中的任意元素 <code>(i, j)</code>的值等于 <span class="math inline">\(A\)</span> 中第 <code>i</code> 行与<span class="math inline">\(B\)</span> 中第 <code>j</code>列的点积，如下所示是一个矩阵乘法的示例。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}1 &amp; 3 \\5 &amp; 2 \\0 &amp; 4 \\\end{matrix}\right)\left(\begin{matrix}3 &amp; 6 &amp; 9 &amp; 4 \\2 &amp; 7 &amp; 8 &amp; 3 \\\end{matrix}\right)=\left(\begin{matrix}9 &amp; 27 &amp; 33 &amp; 13 \\19 &amp; 44 &amp; 61 &amp; 26 \\8 &amp; 28 &amp; 32 &amp; 12 \\\end{matrix}\right)\end{aligned}\]</span><p>矩阵乘法的特性</p><ul><li>不符合交换律，即 <span class="math inline">\(AB \neqBA\)</span></li><li>符合分配律，即 <span class="math inline">\(A(B + C) = AB +BA\)</span></li><li>符合结合律，即 <span class="math inline">\((AB)C =A(BC)\)</span></li></ul><p>在计算机图形学中，向量也会使用矩阵（行矩阵或列矩阵）来表示，向量之间的乘法以及向量与矩阵的乘法都符合矩阵乘法的基本规则。</p><h2 id="矩阵的转置运算">矩阵的转置运算</h2><p>矩阵的转置本质上就是沿着主对角线（从左上角至右下角）的对角线将 i x j的矩阵翻转成 j x i 的矩阵。一般我们使用一个 <strong>上标 T</strong>表示一个矩阵的转置，如：<span class="math inline">\(A^T\)</span>。</p><p>如下所示，是一个矩阵转置运算的示例。</p><span class="math display">\[\begin{aligned}\left(\begin{matrix}1 &amp; 2 \\3 &amp; 4 \\5 &amp; 6 \\\end{matrix}\right)^T=\left(\begin{matrix}1 &amp; 3 &amp; 5 \\2 &amp; 4 &amp; 6 \\\end{matrix}\right)\end{aligned}\]</span><p>矩阵转置的特性</p><ul><li><span class="math inline">\((AB)^T = B^TA^T\)</span></li></ul><h2 id="矩阵的类型">矩阵的类型</h2><p>下面，我们来介绍各种不同类型的矩阵。</p><h3 id="对角矩阵">对角矩阵</h3><p>对角矩阵，其主对角线（从左上角到右下角）上的元素都是非0，其他元素都为 0。</p><h3 id="单位矩阵">单位矩阵</h3><p>单位矩阵，其主对角线（从左上角到右下角）上的元素都为 1，其余元素都为0，一般使用大写字母 <span class="math inline">\(I\)</span>来表示。单位矩阵是一个特殊的对角矩阵。如下所示，是一个单位矩阵实例。</p><span class="math display">\[\begin{aligned}I_{3 \times 3}=\left(\begin{matrix}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\\end{matrix}\right)\end{aligned}\]</span><h3 id="逆矩阵">逆矩阵</h3><p>给定一个矩阵 <spanclass="math inline">\(A\)</span>，如果存在一个与之维度相同的矩阵，使得这两个矩阵相乘的结果是一个单位矩阵，那么我们称这个矩阵为矩阵<span class="math inline">\(A\)</span> 的逆矩阵，使用 <spanclass="math inline">\(A^{-1}\)</span>表示。如下所示，为逆矩阵的示例。</p><span class="math display">\[\begin{aligned}A A^{-1} =&amp; A^{-1}A = I\\(AB)^{-1} =&amp; B^{-1}A^{-1}\end{aligned}\]</span><h2 id="矩阵的应用">矩阵的应用</h2><p>上述我们介绍的向量的两种运算，其实完全可以使用矩阵的乘法来实现。</p><p>关于向量的点积，我们可以使用如下矩阵乘法来表示。</p><span class="math display">\[\begin{aligned}\vec{a} \cdot \vec{b}=\vec{a}^T \cdot \vec{b}=\left(\begin{matrix}a_x &amp; a_y &amp; a_z\end{matrix}\right)\left(\begin{matrix}b_x \\b_y \\b_z \\\end{matrix}\right)=a_x \cdot b_x + a_y \cdot b_y + a_z \cdot b_z\end{aligned}\]</span><p>关于向量的叉积，我们可以使用如下矩阵乘法来表示，如下所示。其中 <spanclass="math inline">\(A^*\)</span> 为矩阵 <spanclass="math inline">\(A\)</span> 的对偶矩阵。</p><span class="math display">\[\begin{aligned}\vec{a} \times \vec{b}=A^* b=\left(\begin{matrix}0    &amp; -a_z &amp; a_y \\a_z  &amp; 0    &amp; -a_x \\-a_y &amp; a_x  &amp; 0 \\\end{matrix}\right)\left(\begin{matrix}b_x \\b_y \\b_z \\\end{matrix}\right)=\left(\begin{matrix}a_y \cdot b_z - a_z \cdot b_y \\a_z \cdot b_x - a_x \cdot b_z \\a_x \cdot b_y - a_y \cdot b_x \\\end{matrix}\right)\end{aligned}\]</span><h1 id="总结">总结</h1><p>本文介绍了点、向量、矩阵的基本定义和运算方法。向量的乘法包含两种：点积和叉积，两者被广泛应用在了在计算机图形学中。</p><p>点积和叉积的具体运算可以通过矩阵运算来实现，这也是为什么我们常说计算机图形学中包含了大量矩阵运算。</p><p>下文，我们将探讨矩阵在图形的变换中的应用，敬请期待吧~</p><h1 id="参考">参考</h1><ol type="1"><li>《GAMES 101》</li><li>《计算机图形学入门：3D渲染指南》</li><li><ahref="https://blog.csdn.net/weixin_42782150/article/details/104878759">史上最全Markdown公式、符号总结</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近对计算机图形学比较感兴趣，刷了一遍《计算机图形学入门：3D渲染指南》，看了一遍《GAMES
101》。本文对计算机图形学的线性代数相关基础进行了梳理和总结，以便后续进行复习和回顾。&lt;/p&gt;</summary>
    
    
    
    <category term="图形学" scheme="http://chuquan.me/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="向量" scheme="http://chuquan.me/tags/%E5%90%91%E9%87%8F/"/>
    
    <category term="矩阵" scheme="http://chuquan.me/tags/%E7%9F%A9%E9%98%B5/"/>
    
    <category term="点积" scheme="http://chuquan.me/tags/%E7%82%B9%E7%A7%AF/"/>
    
    <category term="叉积" scheme="http://chuquan.me/tags/%E5%8F%89%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>2023，31岁客三消从业者的年度回顾</title>
    <link href="http://chuquan.me/2024/01/05/2023-summary/"/>
    <id>http://chuquan.me/2024/01/05/2023-summary/</id>
    <published>2024-01-05T14:55:30.000Z</published>
    <updated>2024-06-05T10:43:46.522Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-01.jpg?x-oss-process=image/resize,w_600" /></p><span id="more"></span><p>每年总是要例行回顾一下过去一年，看看自己做了什么，收获了什么。</p><h1 id="角色适应">角色适应</h1><p>今年是作为移动客户端负责人的第 2年，自己基本已经适应了这个角色。一开始，我和团队中很多成员都是一线的研发，后面被提拔到这个位置。那时候，在技术决策、任务分配、会议沟通时，经常会想自己的决策和做法是否被认可？是否被信服？团队成员是否认可自己？总之，心理负担一直都是有的。</p><p>在业务的迭代和发展过程中，我会回顾自己做的决策。从结果看来，整体都是符合预期的，比如：</p><ul><li><strong>关于兄弟团队借人的决策</strong>。部门内有两条业务线，另一条业务线今年年初在快速发展，缺iOS 开发，于是经过部门老大的同意向我们借人，希望能够支援 3个月。，最终我决定让组内的几个同学轮流支援，每人支援一个月。一方面，可以避免外派同学产生边缘心理；另一方面，也可以避免借人还人的情况。</li><li><strong>关于提测质量竞赛的决策</strong>。年初在与测试同学的沟通中了解到：在服务端、前端、客户端的测试过程中，测试体验最差的往往都是客户端。当然，这里面是存在客观原因的。关于服务端，其主要是开发业务逻辑，不包含UI 逻辑，一旦联调完毕，在测试阶段 BUG其实是很少的；关于前端，其与客户端非常相似，区别在于前端在测试阶段可以热修复BUG，前一分钟还存在的BUG，后一分钟可能就解决了。关于客户端，相比服务端多了 UIBUG，相比前端修复周期比较长（修复、编译、验证、打包、提测）。最终产生客户端测试体验差的感觉。为了提高提测质量，提出了提测质量竞赛的机制，对于同一个需求，Android/iOSBUG 数量出现大于等于 5 的情况时，BUG数量少的一方和对应的测试同学将获得一杯喜茶，并记录在 Score Board中。一年下来，组内成员的自测意识确实提高了不少，产生的 BUG基本上都是测试用例之外的 BUG。</li></ul><p>正确的决策会带来正向的激励，从而产生正反馈效应。于是，之前心理负担开始慢慢的消失，自己对于这个角色也开始逐步适应，慢慢开始变得得心应手起来。</p><h1 id="工作产出">工作产出</h1><p>在工作产出方面，今年主要做了一些工程能力和技术调研等工作，比如：</p><ul><li>构建客户端 NodeJS 服务，支持 Sentry 崩溃告警、GitLab CodeReview、包体积分析等能力。</li><li>Cocos引擎定制的工程化，解决底层引擎替换问题，增加引擎日志，独立引擎打包等。</li><li>推进并落地 Cocos 资源代码隔离能力，从而让 App 内的 Cocos互动题具备全局的热修复能力。</li><li>直播自建、局部录屏、恢复购买、家庭共享、Deferred Deep Link等技术方案调研。</li></ul><p>今年，在 iOS同学外派支援期间，我做了一些业务需求，其他时间基本都没有参与复杂业务和模块的具体开发。因为团队内Android 和 iOS的研发人员数量对等，所以不需要我来承担额外的开发任务。只有当出现临时需求或者排期时没有分配的需求时，为了不打乱既定的排期，一般会由我来兜底做这些需求，一个人同时写Android 和 iOS。</p><p>整体而言，今年开始逐步退居二线，做一些技术决策和工程能力等相关工作。不过，在日常中我仍然坚持写代码，因为我始终觉得一旦自己脱离一线太久，很容易会作出一些不符合现实的决策和排期。</p><h1 id="身体是革命的本钱">身体是革命的本钱</h1><h2 id="眼睛疲劳">眼睛疲劳</h2><p>从 2021 年下半年到 2022年上半年，我一直有着眼睛疲劳的症状，具体表现就是<strong>眼睛无法准确对焦</strong>。当我在观察 3米以外的物体时，大脑中呈现的视觉效果是有两个物体（两个眼镜各自成像的物体），两个物体无法合成到一个画面中，需要非常努力的弄眉挤眼才能对焦上，但是过不了多久又会失焦。</p><p>这个问题我一直都没有发现，因为在工作生活中，眼睛对焦基本都在 3米以内。最后是在电影院观影时才发现的，画面有重影，观影感极差。从那时起，我才开始重视视力问题。</p><p>在 4月份，我预约了同仁医院的眼科挂号。在医院里，我看到了各种饱受眼科疾病困扰的患者，青光眼、视网膜脱落、近视手术后遗症等等，这让我视力恢复之前都非常焦虑。在经历一系列眼科诊断之后，医生得出的结论是眼睛疲劳+近视度数上涨。于是，在同仁医院配了一副眼镜，自己额外配了一副隔蓝光的镜片，配合着服用叶黄素，进行修养。总体来说，是有效果的，但是效果还是有点慢。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-02.jpg?x-oss-process=image/resize,w_600" /></p><p>在 5月份，五一长假休假在家，我尝试尽量不使用电脑和手机。即使使用电脑，也是投屏到电视上，然后坐在沙发上观看电视屏幕，尽量保持远距离观看。经过一个多星期的调养。眼睛疲劳改善非常明显。假期结束后，我期望着眼睛能完全恢复，可惜大概一个月作用的时间，眼睛又开始疲劳。特别是中午遇到强光时，症状会更加严重。</p><p>在 8月份，我开始意识到眼睛疲劳可能是因为睡前和醒后躺在床上刷手机导致的。每次睡觉前我都会不由自主地刷一个多小时的手机，早上醒来也是躺在床上刷一个多小时手机，加上姿势不正确，导致视力疲劳。于是，我开始强制自己在床上玩手机不超过20 分钟。坚持了半年了，现在视力明显恢复了。</p><p>在视力恢复之前，我一度非常焦虑，经常思考程序员的职业给我带来了什么？如果视力无法治疗该怎么办？...好在现在恢复了，这次经历让我明白了身体健康的重要性。一定要注意身体，不要让打工挣的钱成为身体的医疗费！从而言之，身体是革命的本钱。</p><h2 id="运动健身">运动健身</h2><h3 id="健身">健身</h3><p>8 月份，在 <span class="citation" data-cites="昱总">@昱总</span>的安利下，我办了天奥的健身卡，怕自己坚持不下来，先办了一年的年卡。由于 8月份期间参加各种篮球赛，所以真正开始规律健身应该是从 9月份开始，周一练背，周四练肩，周五练手臂，偶尔练练卧推。目前卧推能 60KG做组，左右手力量也均衡了很多。除此之外，双十一配了肌酸和蛋白粉，喝的不算多，佛系健身。于我而言，健身的目的是为了自己变得壮一点，而不是看起来像细狗，仅此而已，什么健体、健美并不是我的目标。</p><h3 id="篮球">篮球</h3><p>今年算是工作以来打篮球最多的一年了，首先是固定每周二中午打 2小时篮球。另外就是篮球赛，8 月参加了 CBD 篮球联赛，9 月参加了 CYBA篮球联赛，这两个月周末总有一天是在打篮球。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-03.jpg?x-oss-process=image/resize,w_600" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-04.jpg?x-oss-process=image/resize,w_600" /></p><h3 id="跑步">跑步</h3><p>2022年终总结时给自己定了一个目标——<strong>参加一次半程马拉松</strong>。因此，我计划参加4月份的北京半程马拉松。结果，等到报名时发现要求必须三年内参加过其他马拉松，并提供相关证明。没办法，没有资格参加，只能选择参加奥森马拉松。</p><p>我从 2.25 开始备战，从 5 公里开始，每周跑一次，每次比上一次增加 2.5公里左右，最终达到 21 公里。练了一次 21公里后，参加比赛。最终成绩还不错，用时 <code>2:01:52</code>。定一个 2024年的小目标——<strong>半马破 2 小时</strong>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-05.jpg?x-oss-process=image/resize,w_320" /></p><p>参加半马之后，我开始坚持每周末都跑一次 10公里，偶尔还会参加一下线上马拉松，收集了不少奖牌。最终坚持到了 10月底，11 月份室外跑步属实太冷了，打算 2 月份重新开始。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-06.jpg?x-oss-process=image/resize,w_800" /></p><h3 id="作息">作息</h3><p>正是办了健身卡之后，我和媳妇开始调整生活作息，拒绝熬夜，晚上 11:20之前睡觉。早上差不多能 6、7点起床，起来后去四得公园，媳妇跑步，我则散步。在公园大概 40分钟，期间能呼吸一下新鲜空气，放空一下大脑。当然在一个人散步的时候会思考很多，比如：职业规划、业余项目、技术问题等。散步结束回来大概8 点左右，还能有两个小时看会儿书或写会儿代码。</p><p>调整作息之后，感觉自己的精神状态好了很多，下班时间的使用效率也变得更高了。当然，周末也不再是没有上午的周末，时间也变得更加充足。作息调整是今年个人转变的最大成就，为了健康和效率，未来一直要继续保持下去。</p><h1 id="学习收获">学习收获</h1><p>上半年因为眼睛问题，有意减少电脑使用时间，下半年业余时间主要在项目，因此整体而言，2023年在学习上投入的时间并不是很多。关于学习方面的成就主要有以下几部分。</p><h2 id="书籍">书籍</h2><p>因为尽量不过度用眼，今年看的书并不多，只有以下几本：</p><ul><li>《第一行代码——Android（第3版）》</li><li>《架构师的自我修炼》</li><li>《程序员修炼之道》</li><li>《On Java 基础版》</li><li>《重构》</li><li>《程序员的自我修养》三刷</li><li>《计算机图形学入门：3D渲染指南》</li></ul><h2 id="博客">博客</h2><p>今年写的博客也不多，年初的时候产出了几篇编程语言相关的博客：<ahref="http://chuquan.me/2023/01/15/actor/">《浅谈 Actor 模型》</a>、<ahref="http://chuquan.me/2023/03/11/structured-concurrency/">《结构化并发》</a>、<ahref="http://chuquan.me/2023/04/22/prototype-based-inheritance/">《基于原型的继承模式》</a>。</p><p>年中的时候研究 Homebrew 和 fishhook 产出了两篇原理分析博客：<ahref="http://chuquan.me/2023/06/24/understand-fishhook-design/">《如何从链接原理的角度理解fishhook 的设计思想？》</a>、<ahref="http://chuquan.me/2023/08/27/understand-the-design-of-homebrew/">《Homebrew的设计哲学》</a>。</p><p>最后就是十一那会儿写了两篇关于差分算法的博客：<ahref="http://chuquan.me/2023/09/13/myers-difference-algorithm/">《Myers差分算法 》</a>、<ahref="http://chuquan.me/2023/10/06/paul-heckel-difference-algorithm/">《PaulHeckel 差分算法》</a>。</p><h2 id="项目">项目</h2><p>今年业余时间总共做了三个半项目，相比之前几年，产出高出了不少，希望明年继续保持。</p><p>第一个项目是 <ahref="https://github.com/baochuquan/taskloop">Taskloop</a>。这是一款基于crontab的定时任务管理器，支持语义化的配置规则，并且支持环境变量导入、日持查询等功能。具体介绍详见<ahref="http://chuquan.me/2023/07/30/introduction-to-taskloop/">《如何优雅地管理你的定时任务？》</a>。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/taskloop-logo-white.png?x-oss-process=image/resize,w_800" /></p><p>第二个项目是 Morph Clock（中文名：莫负时钟）屏幕保护程序。这是一款MacOS 屏幕保护程序，采用一种数字变形的艺术效果实现。</p><p>第三个项目是 <ahref="https://apps.apple.com/us/app/morph-rest-break-reminder/id6474056217">MorphRest</a>（中文名：莫负休息）。这是一款 MacOS休息提醒应用程序，预防眼睛疲劳、腰间盘突出、劲椎疼痛等职业病，也可辅助提醒喝水，避免尿酸过高，引发痛风等疾病。为什么做这个项目？主要有两方面原因：一方面，我经历了眼睛疲劳，迫切需要一款软件能够经常提醒我站起来活动活动，让视线远离屏幕，顺带提醒自己多喝水。另一方面，我希望打造一款独立产品，尝试利用业务时间成为一位IndieHacker。于是，差不多花了一个半月的业余时间，完成了项目，并最终上架。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-07.png?x-oss-process=image/resize,w_800" /></p><p>Morph Clock 和 Morph Rest 则是面向普通 Mac用户的独立产品。项目代码并没有开源，因为我希望能够通过它们创造收入，这里定一个小目标：<strong>在未来2024 年内通过独立产品创造 99美元的收入，回收的开通苹果开发者账号的成本</strong>。如果有用户支持，欢迎下载使用。</p><h2 id="其他">其他</h2><p>其他方面的学习收获也是有的，首先是 Android开发，春节假期期间，在家学习了一下 Android 开发，重写了海豚 AI学中的一个 Flutter 页面，算是入门了 Android。鉴于此，下半年能够做一些Android 小需求。但是没有深入研究 Android开发，也没有做过一些复杂业务开发，这一方面希望 2024 年能够有所改善。</p><p>其次，在下半年做独立产品期间，系统性地学习了 Sketch相关技巧和理论。Morph Rest 和 Morph Clock 相关的 UI设计和切图也都是自己完成的，算是额外掌握了一个 Indie Hacker必备的技能吧。</p><p>最后，系统性地学习了一下 MacOS 开发，它与 iOS开发在整体上一致的，在一些实现细节上有所不同。如果按照自己所认知的 iOS原理来开发 MacOS 应用会遇到很多奇怪的 BUG。在系统性学习之后，再来开发MacOS 应用会简单很多，这一点我深有体会。</p><h1 id="生活不只有工作">生活不只有工作</h1><p>今年是工作以来第一个没有债务的年份，因此不再考虑紧巴巴地生活了，该吃吃，该喝喝，该玩玩。不过因为疫情三年养成了一种「宅」感，所以还需要继续调整和适应。</p><h2 id="假期">假期</h2><h3 id="合肥">合肥</h3><p>五一假期回合肥休假，为了调养眼睛疲劳，没怎么学习，主打的就是休假。期间主要在滨湖转悠，骑上共享电驴，环游了一些景点和公园，渡江战役纪念馆、岸上草原、安徽名人馆、塘西河公园、金斗公园等。比较可惜的是，没约上安徽美术馆，不过以后有的是机会。</p><p>在家期间，用闲置的 Mac Mini配上电视，效果很不错，也很护眼。用这一套装置在家看完了《漫长的季节》！《漫长的季节》成为了我心中国产剧的No.1，墙裂推荐！</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-09.jpg?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-10.jpg?x-oss-process=image/resize,w_800" /></p><h3 id="廊坊">廊坊</h3><p>7月份，我在朋友圈看到有同事去了廊坊的只有红楼梦·梦幻戏剧城，感觉很不错，加上自己很喜欢《红楼梦》，所以抽了一个周末去了一趟廊坊。园区非常大，网上的评价大多是一天的游玩时间不够，于是我们就订了2 日通票。不得不说，里面的建筑和剧场都非常惊艳！绝对值得去玩一次！</p><p>不过很可惜，我们去的那个周末天气不太好。周六阴天，周日暴雨。因为暴雨园区闭园，给我们退了一半的票，算下来也就是玩了一天时间，差不多玩了大半个园区吧，只不过话剧和情景剧没看够。</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-08.jpg?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-11.jpg?x-oss-process=image/resize,w_800" /></p><h3 id="哈尔滨">哈尔滨</h3><p>今年 10月份原本打算去哈尔滨，结果跟我弟了解了一下情况后，决定等到冰雪大世界开放之后再去。最终在元旦前请了几天假提前出发，主要是为了避开假期旅游高峰。好巧不巧，哈尔滨旅游今年出圈了，游客非常多，几个热门项目排队时间都超长，几乎每个都要排队3个小时起步，比如：大滑梯、摩天轮、哈冰秀。我们在冰雪大世界整一天就是佛系游玩，毕竟在零下十度的室外排队几个小时的体验可不是那么好。不过有一说一，冰雪大世界里的冰雕、雪雕确实都非常精美、壮观，绝对值得去参观一次！</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-12.jpg?x-oss-process=image/resize,w_800" /></p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-13.jpg?x-oss-process=image/resize,w_600" /></p><h2 id="搬家">搬家</h2><p>今年 10 月份搬了一次家，离开了住了 6年的高家园。高家园附近环境其实很不错，小区门口很多街边商店，很繁华；马路对面就是丽都，是一个相对比较高端的街区；500米远处是四得公园，疫情期间翻修了一次，环境非常不错。因为生活很方便，所以在这里住了6年。搬家期间，特别是对面的室友搬走的时候，内心非常感慨：岁月匆匆，人生匆匆，北漂生活何时终了？</p><p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-14.jpg?x-oss-process=image/resize,w_600" /></p><h1 id="思维转变">思维转变</h1><p>2023 年，我感觉自己最大的变化是思维的转换，主要是两点：</p><ul><li>身体是最重要的，其他的一切都是身外之物。</li><li>打工是没有出路的，提前计划自己的未来。</li></ul><p>第一点不用多说，是眼睛疲劳期间非常焦虑，那会儿才真正体会和理解这一点。第二点是因为今年8月开始早起散步，散步期间开始思考未来的打算。这两年各种裁员消息层出不穷，即使你学历再好，技术再厉害，当公司不需要你时，无外乎其他任何因素，随时都可能裁你。一旦失业，你再就业的难度会与你的年龄正比，这是非常现实的问题。</p><p>于是，我开始逛一下独立开发者相关的网站，比如：Indie Hacker，ProductHunt，w2solo。在这些论坛中，我看到了很多独立开发者的成功案例，这也激励了我尝试使用业余时间来走这条道路。11月份，我开始着手做一款 iOSApp，期间自己做产品调研，画设计稿，代码实现。期间感觉自己对于产品的最终效果还是有点不确定，而且担心战线太长，所以果断暂停了项目，转而开发形态更加确定的一款MacOS App——<ahref="https://apps.apple.com/cn/app/morph-rest-break-reminder/id6474056217?mt=12">MorphRest</a>。期间，还写了一个MacOS 屏幕保护程序——MorphClock。这些产品未来不一定能成功，但是我不迈出这一步，那么永远都不会成功。</p><h1 id="新年愿景">新年愿景</h1><p>未来一年，我应该还会继续尝试做一些独立产品，努力成为 IndieHacker。当然，技术博客也会被不定期更新，毕竟这是热爱，而不是生活。</p><p>最后，祝新年快乐~</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/blog-images/2023-summary-01.jpg?x-oss-process=image/resize,w_600&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="生活" scheme="http://chuquan.me/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
    <category term="年度总结" scheme="http://chuquan.me/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Sketch Cheat Sheet</title>
    <link href="http://chuquan.me/2023/10/21/sketch-cheat-sheet/"/>
    <id>http://chuquan.me/2023/10/21/sketch-cheat-sheet/</id>
    <published>2023-10-21T14:01:50.000Z</published>
    <updated>2023-10-21T14:05:47.410Z</updated>
    
    <content type="html"><![CDATA[<p><imgsrc="https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/sketch-exercise01.png?x-oss-process=image/resize,w_800" /></p><span id="more"></span><p>我的博客文章配图基本上都是使用 Sketch绘制的，但是绘制方法仅限我的自我认知而已。由于没有系统性地学习过Sketch，因此在遇到一些复杂场景时，绘制的效率非常低。于是最近业余时间在 B站上学习了一套 Sketch 教程——<ahref="https://www.bilibili.com/video/BV1hd4y1z7pZ/?spm_id_from=333.337.search-card.all.click&amp;vd_source=247f8a5d677ea1cff0161b9afe62e41f">《Sketch零基础速学教程（矢量绘图设计工具）》</a>。</p><p>这里记录一下教程中提到的快捷键技巧，便于后续参考。经过实测，这些技巧确实能够提升效率，文章封面图就是学完教程结合技巧绘制的图标。</p><h1 id="快捷键">快捷键</h1><h2 id="基础快捷键">基础快捷键</h2><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th style="text-align: left;">功能</th><th style="text-align: left;">快捷键</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">新建画板（New Artboard）</td><td style="text-align: left;">A</td></tr><tr class="even"><td style="text-align: left;">插入矩形（Rectangle）</td><td style="text-align: left;">R</td></tr><tr class="odd"><td style="text-align: left;">插入原型（Oval）</td><td style="text-align: left;">O</td></tr><tr class="even"><td style="text-align: left;">插入文本（Text）</td><td style="text-align: left;">T</td></tr><tr class="odd"><td style="text-align: left;">钢笔工具（Vector Point）</td><td style="text-align: left;">V</td></tr><tr class="even"><td style="text-align: left;">放大（Zoom</td><td style="text-align: left;">按住 Z，然后框选想放大的区域</td></tr><tr class="odd"><td style="text-align: left;">查看间距（Guides）</td><td style="text-align: left;">Alt（选中一个图层，按住Alt，鼠标移动移动到另一个图层，可查看选中图层到指向图层的间距）</td></tr><tr class="even"><td style="text-align: left;">创建分组（Group）</td><td style="text-align: left;">⌘ G</td></tr><tr class="odd"><td style="text-align: left;">取消分组（Ungroup）</td><td style="text-align: left;">⌘ ⇧ G</td></tr><tr class="even"><td style="text-align: left;">复制上一步操作（Duplicate</td><td style="text-align: left;">⌘ D</td></tr><tr class="odd"><td style="text-align: left;">编辑（Edit）</td><td style="text-align: left;">Enter</td></tr></tbody></table><h2 id="吸管工具">吸管工具</h2><table><thead><tr class="header"><th style="text-align: left;">功能</th><th style="text-align: left;">快捷键</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">吸取颜色（Color Picker）</td><td style="text-align: left;">⌃ C</td></tr></tbody></table><h2 id="复制样式">复制样式</h2><table><thead><tr class="header"><th style="text-align: left;">功能</th><th style="text-align: left;">快捷键</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">复制图层样式（Copy Style）</td><td style="text-align: left;">⌘ ⌥ C</td></tr><tr class="even"><td style="text-align: left;">粘贴图层样式（Paste Style）</td><td style="text-align: left;">⌘ ⌥ V</td></tr></tbody></table><h2 id="视图模式">视图模式</h2><table><thead><tr class="header"><th style="text-align: left;">功能</th><th style="text-align: left;">快捷键</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">演示模式（Presentation Mode）</td><td style="text-align: left;">⌘ .</td></tr><tr class="even"><td style="text-align: left;">以画布为中心放大（Center Canvas）</td><td style="text-align: left;">⌘ 1</td></tr><tr class="odd"><td style="text-align: left;">以选择的图层为中心放大（ZoomSelection）</td><td style="text-align: left;">⌘ 2</td></tr><tr class="even"><td style="text-align: left;">视图放大</td><td style="text-align: left;">⌘ +</td></tr><tr class="odd"><td style="text-align: left;">视图缩小</td><td style="text-align: left;">⌘ -</td></tr><tr class="even"><td style="text-align: left;">恢复到画布实际大小</td><td style="text-align: left;">⌘ 0</td></tr></tbody></table><h2 id="图层">图层</h2><table><thead><tr class="header"><th style="text-align: left;">功能</th><th style="text-align: left;">快捷键</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">在图层面板从上往下选择图层（SelectingLayer Below）</td><td style="text-align: left;">tab</td></tr><tr class="even"><td style="text-align: left;">在图层面板从下往上选择图层（SelectingLayer Below）</td><td style="text-align: left;">⇧ tab</td></tr><tr class="odd"><td style="text-align: left;">上移图层（Bring Forward）</td><td style="text-align: left;">⌘ ]</td></tr><tr class="even"><td style="text-align: left;">下移图层（Bring Backward）</td><td style="text-align: left;">⌘ [</td></tr><tr class="odd"><td style="text-align: left;">置顶图层（Bring to Front）</td><td style="text-align: left;">⌘ ⌥ ]</td></tr><tr class="even"><td style="text-align: left;">置底图层（Bring to End）</td><td style="text-align: left;">⌘ ⌥ [</td></tr></tbody></table><h2 id="相关补充">相关补充</h2><table><thead><tr class="header"><th style="text-align: left;">功能</th><th style="text-align: left;">快捷键</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">隐藏图层</td><td style="text-align: left;">⌘ ⇧ H</td></tr><tr class="even"><td style="text-align: left;">锁定图层</td><td style="text-align: left;">⌘ ⇧ L</td></tr><tr class="odd"><td style="text-align: left;">查找图层</td><td style="text-align: left;">⌘ F</td></tr><tr class="even"><td style="text-align: left;">变换工具</td><td style="text-align: left;">⌘ ⇧ T</td></tr><tr class="odd"><td style="text-align: left;">旋转工具</td><td style="text-align: left;">⌘ ⇧ R</td></tr><tr class="even"><td style="text-align: left;">将字体转换成轮廓</td><td style="text-align: left;">⌘ ⌥ O</td></tr><tr class="odd"><td style="text-align: left;">显示/取消填充</td><td style="text-align: left;">F</td></tr><tr class="even"><td style="text-align: left;">显示/取消描边</td><td style="text-align: left;">B</td></tr><tr class="odd"><td style="text-align: left;">将当前图层用作蒙版</td><td style="text-align: left;">⌘ ⌃ M</td></tr><tr class="even"><td style="text-align: left;">改变形状尺寸</td><td style="text-align: left;">⌘ 键盘上/下/左/右</td></tr><tr class="odd"><td style="text-align: left;">切换不同的 Sketch 文件</td><td style="text-align: left;">⌘ ~</td></tr></tbody></table><h2 id="设置">设置</h2><table><thead><tr class="header"><th style="text-align: left;">功能</th><th style="text-align: left;">快捷键</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">打开设置</td><td style="text-align: left;">⌘ ,</td></tr></tbody></table><h1 id="屏幕分辨率尺寸">屏幕分辨率尺寸</h1><table><thead><tr class="header"><th style="text-align: left;">设备</th><th style="text-align: left;">屏幕尺寸</th><th style="text-align: left;">屏幕分辨率（px）</th><th style="text-align: left;">逻辑分辨率（pt）</th><th style="text-align: left;">PPI</th><th style="text-align: left;">倍率</th><th style="text-align: left;">换算</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">iPhone14</td><td style="text-align: left;">6.1寸</td><td style="text-align: left;">1170x2532</td><td style="text-align: left;">390x844</td><td style="text-align: left;">460</td><td style="text-align: left;"><span class="citation"data-cites="3x">@3x</span></td><td style="text-align: left;">1pt=3px</td></tr><tr class="even"><td style="text-align: left;">iPhone12 Pro Max</td><td style="text-align: left;">6.7寸</td><td style="text-align: left;">1284x2778</td><td style="text-align: left;">428x926</td><td style="text-align: left;">458</td><td style="text-align: left;"><span class="citation"data-cites="3x">@3x</span></td><td style="text-align: left;">1pt=3px</td></tr><tr class="odd"><td style="text-align: left;">iPhone12 Pro</td><td style="text-align: left;">6.1寸</td><td style="text-align: left;">1170x2532</td><td style="text-align: left;">390x844</td><td style="text-align: left;">460</td><td style="text-align: left;"><span class="citation"data-cites="3x">@3x</span></td><td style="text-align: left;">1pt=3px</td></tr><tr class="even"><td style="text-align: left;">iPhone11 Pro Max</td><td style="text-align: left;">6.5寸</td><td style="text-align: left;">1242x2688</td><td style="text-align: left;">414x896</td><td style="text-align: left;">458</td><td style="text-align: left;"><span class="citation"data-cites="3x">@3x</span></td><td style="text-align: left;">1pt=3px</td></tr><tr class="odd"><td style="text-align: left;">iPhone11 Pro</td><td style="text-align: left;">6.1寸</td><td style="text-align: left;">1125x2436</td><td style="text-align: left;">375x812</td><td style="text-align: left;">458</td><td style="text-align: left;"><span class="citation"data-cites="3x">@3x</span></td><td style="text-align: left;">1pt=3px</td></tr><tr class="even"><td style="text-align: left;">iPhone11</td><td style="text-align: left;">5.8寸</td><td style="text-align: left;">828x1792</td><td style="text-align: left;">414x896</td><td style="text-align: left;">326</td><td style="text-align: left;"><span class="citation"data-cites="2x">@2x</span></td><td style="text-align: left;">1pt=2px</td></tr><tr class="odd"><td style="text-align: left;">iPhone8 Plus</td><td style="text-align: left;">5.5寸</td><td style="text-align: left;">1242x2208</td><td style="text-align: left;">414x736</td><td style="text-align: left;">401</td><td style="text-align: left;"><span class="citation"data-cites="3x">@3x</span></td><td style="text-align: left;">1pt=3px</td></tr><tr class="even"><td style="text-align: left;">iPhone8</td><td style="text-align: left;">4.7寸</td><td style="text-align: left;">750x1334</td><td style="text-align: left;">375x667</td><td style="text-align: left;">326</td><td style="text-align: left;"><span class="citation"data-cites="2x">@2x</span></td><td style="text-align: left;">1pt=2px</td></tr><tr class="odd"><td style="text-align: left;">iPhoneSE</td><td style="text-align: left;">4.0寸</td><td style="text-align: left;">640x1136</td><td style="text-align: left;">320x568</td><td style="text-align: left;">326</td><td style="text-align: left;"><span class="citation"data-cites="2x">@2x</span></td><td style="text-align: left;">1pt=2px</td></tr><tr class="even"><td style="text-align: left;">iPhone3GS</td><td style="text-align: left;">3.5寸</td><td style="text-align: left;">320x480</td><td style="text-align: left;">320x480</td><td style="text-align: left;">163</td><td style="text-align: left;"><span class="citation"data-cites="1x">@1x</span></td><td style="text-align: left;">1pt=1px</td></tr></tbody></table><p>UI 设计一般以 390x844 或 375x812 为尺寸进行绘制。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img
src=&quot;https://chuquan-public-r-001.oss-cn-shanghai.aliyuncs.com/sketch-images/sketch-exercise01.png?x-oss-process=image/resize,w_800&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="设计" scheme="http://chuquan.me/categories/%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="Sketch" scheme="http://chuquan.me/tags/Sketch/"/>
    
  </entry>
  
</feed>
